% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{Li2019}{article}{}
    \name{author}{3}{}{%
      {{hash=LY}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Yuanzhi},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=WC}{%
         family={Wei},
         familyi={W\bibinitperiod},
         given={Colin},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MT}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Tengyu},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{LYWCMT1}
    \strng{fullhash}{LYWCMT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Stochastic gradient descent with a large initial learning rate is widely
  used for training modern neural net architectures. Although a small initial
  learning rate allows for faster training and better test performance
  initially, the large learning rate achieves better generalization soon after
  the learning rate is annealed. Towards explaining this phenomenon, we devise
  a setting in which we can prove that a two layer network trained with large
  initial learning rate and annealing provably generalizes better than the same
  network trained with a small learning rate from the start. The key insight in
  our analysis is that the order of learning different types of patterns is
  crucial: because the small learning rate model first memorizes
  easy-to-generalize, hard-to-fit patterns, it generalizes worse on
  hard-to-generalize, easier-to-fit patterns than its large learning rate
  counterpart. This concept translates to a larger-scale setting: we
  demonstrate that one can add a small patch to CIFAR-10 images that is
  immediately memorizable by a model with small initial learning rate, but
  ignored by the model with large learning rate until after annealing. Our
  experiments show that this causes the small learning rate model's accuracy on
  unmodified images to suffer, as it relies too much on the patch early on.%
    }
    \verb{eprint}
    \verb 1907.04595
    \endverb
    \field{issn}{10495258}
    \field{pages}{1\bibrangedash 49}
    \field{title}{{Towards explaining the regularization effect of initial
  large learning rate in training neural networks}}
    \field{volume}{32}
    \verb{file}
    \verb :home/taylanot/Dropbox/PhD/Papers/Library/1907.04595.pdf:pdf
    \endverb
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{eprinttype}{arXiv}
    \field{year}{2019}
  \endentry

  \entry{Nakkiran2020b}{article}{}
    \name{author}{1}{}{%
      {{hash=NP}{%
         family={Nakkiran},
         familyi={N\bibinitperiod},
         given={Preetum},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{NP1}
    \strng{fullhash}{NP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2020}
    \field{labeldatesource}{}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{abstract}{%
    Learning rate schedule can significantly affect generalization performance
  in modern neural networks, but the reasons for this are not yet understood.
  Li-Wei-Ma (2019) recently proved this behavior can exist in a simplified
  non-convex neural-network setting. In this note, we show that this phenomenon
  can exist even for convex learning problems -- in particular, linear
  regression in 2 dimensions. We give a toy convex problem where learning rate
  annealing (large initial learning rate, followed by small learning rate) can
  lead gradient descent to minima with provably better generalization than
  using a small learning rate throughout. In our case, this occurs due to a
  combination of the mismatch between the test and train loss landscapes, and
  early-stopping.%
    }
    \verb{eprint}
    \verb 2005.07360
    \endverb
    \field{pages}{1\bibrangedash 9}
    \field{title}{{Learning Rate Annealing Can Provably Help Generalization,
  Even for Convex Problems}}
    \verb{url}
    \verb http://arxiv.org/abs/2005.07360
    \endverb
    \verb{file}
    \verb :home/taylanot/Dropbox/PhD/Papers/Library/2005.07360.pdf:pdf
    \endverb
    \field{eprinttype}{arXiv}
    \field{year}{2020}
  \endentry
\enddatalist
\endinput
