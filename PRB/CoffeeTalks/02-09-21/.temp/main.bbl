% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{Belkin2019}{article}{}
    \name{author}{4}{}{%
      {{hash=BM}{%
         family={Belkin},
         familyi={B\bibinitperiod},
         given={Mikhail},
         giveni={M\bibinitperiod},
      }}%
      {{hash=HD}{%
         family={Hsu},
         familyi={H\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Siyuan},
         giveni={S\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Mandal},
         familyi={M\bibinitperiod},
         given={Soumik},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Bias–variance trade-off,Machine learning,Neural networks}
    \strng{namehash}{BMHDMSMS1}
    \strng{fullhash}{BMHDMSMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Breakthroughs in machine learning are rapidly changing science and society,
  yet our fundamental understanding of this technology has lagged far behind.
  Indeed, one of the central tenets of the field, the bias–variance
  trade-off, appears to be at odds with the observed behavior of methods used
  in modern machine-learning practice. The bias–variance trade-off implies
  that a model should balance underfitting and overfitting: Rich enough to
  express underlying structure in data and simple enough to avoid fitting
  spurious patterns. However, in modern practice, very rich models such as
  neural networks are trained to exactly fit (i.e., interpolate) the data.
  Classically, such models would be considered overfitted, and yet they often
  obtain high accuracy on test data. This apparent contradiction has raised
  questions about the mathematical foundations of machine learning and their
  relevance to practitioners. In this paper, we reconcile the classical
  understanding and the modern practice within a unified performance curve.
  This “double-descent” curve subsumes the textbook U-shaped
  bias–variance trade-off curve by showing how increasing model capacity
  beyond the point of interpolation results in improved performance. We provide
  evidence for the existence and ubiquity of double descent for a wide spectrum
  of models and datasets, and we posit a mechanism for its emergence. This
  connection between the performance and the structure of machine-learning
  models delineates the limits of classical analyses and has implications for
  both the theory and the practice of machine learning.%
    }
    \verb{doi}
    \verb 10.1073/pnas.1903070116
    \endverb
    \verb{eprint}
    \verb arXiv:1812.11118v2
    \endverb
    \field{issn}{10916490}
    \field{number}{32}
    \field{pages}{15849\bibrangedash 15854}
    \field{title}{{Reconciling modern machine-learning practice and the
  classical bias–variance trade-off}}
    \field{volume}{116}
    \verb{file}
    \verb :home/taylanot/Dropbox/PhD/CoffeeTalks/02-09-21/1812.11118.pdf:pdf
    \endverb
    \field{journaltitle}{Proceedings of the National Academy of Sciences of the
  United States of America}
    \field{eprinttype}{arXiv}
    \field{year}{2019}
  \endentry

  \entry{Loog2019}{article}{}
    \name{author}{3}{}{%
      {{hash=LM}{%
         family={Loog},
         familyi={L\bibinitperiod},
         given={Marco},
         giveni={M\bibinitperiod},
      }}%
      {{hash=VT}{%
         family={Viering},
         familyi={V\bibinitperiod},
         given={Tom},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mey},
         familyi={M\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{LMVTMA1}
    \strng{fullhash}{LMVTMA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Plotting a learner's average performance against the number of training
  samples results in a learning curve. Studying such curves on one or more data
  sets is a way to get to a better understanding of the generalization
  properties of this learner. The behavior of learning curves is, however, not
  very well understood and can display (for most researchers) quite unexpected
  behavior. Our work introduces the formal notion of risk monotonicity, which
  asks the risk to not deteriorate with increasing training set sizes in
  expectation over the training samples. We then present the surprising result
  that various standard learners, specifically those that minimize the
  empirical risk, can act nonmonotonically irrespective of the training sample
  size. We provide a theoretical underpinning for specific instantiations from
  classification, regression, and density estimation. Altogether, the proposed
  monotonicity notion opens up a whole new direction of research.%
    }
    \verb{eprint}
    \verb 1907.05476
    \endverb
    \field{issn}{10495258}
    \field{number}{NeurIPS}
    \field{title}{{Minimizers of the empirical risk and risk monotonicity}}
    \field{volume}{32}
    \verb{file}
    \verb :home/taylanot/Dropbox/PhD/Papers/MLoog/NeurIPS-2019-minimizers-of-th
    \verb e-empirical-risk-and-risk-monotonicity-Paper.pdf:pdf
    \endverb
    \field{journaltitle}{Advances in Neural Information Processing Systems}
    \field{eprinttype}{arXiv}
    \field{year}{2019}
  \endentry

  \entry{Loog2020}{article}{}
    \name{author}{5}{}{%
      {{hash=LM}{%
         family={Loog},
         familyi={L\bibinitperiod},
         given={Marco},
         giveni={M\bibinitperiod},
      }}%
      {{hash=VT}{%
         family={Viering},
         familyi={V\bibinitperiod},
         given={Tom},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mey},
         familyi={M\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
      {{hash=KJH}{%
         family={Krijthe},
         familyi={K\bibinitperiod},
         given={Jesse\bibnamedelima H.},
         giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=TDM}{%
         family={Tax},
         familyi={T\bibinitperiod},
         given={David\bibnamedelima M.J.},
         giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \strng{namehash}{LMVTMAKJHTDM1}
    \strng{fullhash}{LMVTMAKJHTDM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2020}
    \field{labeldatesource}{}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \verb{doi}
    \verb 10.1073/pnas.2001875117
    \endverb
    \verb{eprint}
    \verb 2004.04328
    \endverb
    \field{issn}{10916490}
    \field{number}{20}
    \field{pages}{10625\bibrangedash 10626}
    \field{title}{{A brief prehistory of double descent}}
    \field{volume}{117}
    \verb{file}
    \verb :home/taylanot/Dropbox/PhD/CoffeeTalks/02-09-21/2004.04328.pdf:pdf
    \endverb
    \field{journaltitle}{Proceedings of the National Academy of Sciences of the
  United States of America}
    \field{eprinttype}{arXiv}
    \field{year}{2020}
  \endentry

  \entry{Nakkiran2020a}{article}{}
    \name{author}{4}{}{%
      {{hash=NP}{%
         family={Nakkiran},
         familyi={N\bibinitperiod},
         given={Preetum},
         giveni={P\bibinitperiod},
      }}%
      {{hash=VP}{%
         family={Venkat},
         familyi={V\bibinitperiod},
         given={Prayaag},
         giveni={P\bibinitperiod},
      }}%
      {{hash=KS}{%
         family={Kakade},
         familyi={K\bibinitperiod},
         given={Sham},
         giveni={S\bibinitperiod},
      }}%
      {{hash=MT}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Tengyu},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{NPVPKSMT1}
    \strng{fullhash}{NPVPKSMT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2020}
    \field{labeldatesource}{}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{abstract}{%
    Recent empirical and theoretical studies have shown that many learning
  algorithms -- from linear regression to neural networks -- can have test
  performance that is non-monotonic in quantities such the sample size and
  model size. This striking phenomenon, often referred to as "double descent",
  has raised questions of if we need to re-think our current understanding of
  generalization. In this work, we study whether the double-descent phenomenon
  can be avoided by using optimal regularization. Theoretically, we prove that
  for certain linear regression models with isotropic data distribution,
  optimally-tuned $\ell_2$ regularization achieves monotonic test performance
  as we grow either the sample size or the model size. We also demonstrate
  empirically that optimally-tuned $\ell_2$ regularization can mitigate double
  descent for more general models, including neural networks. Our results
  suggest that it may also be informative to study the test risk scalings of
  various algorithms in the context of appropriately tuned regularization.%
    }
    \verb{eprint}
    \verb 2003.01897
    \endverb
    \field{issn}{2331-8422}
    \field{title}{{Optimal Regularization Can Mitigate Double Descent}}
    \verb{url}
    \verb http://arxiv.org/abs/2003.01897
    \endverb
    \verb{file}
    \verb :home/taylanot/Dropbox/PhD/CoffeeTalks/02-09-21/2003.01897.pdf:pdf
    \endverb
    \field{eprinttype}{arXiv}
    \field{year}{2020}
  \endentry
\enddatalist
\endinput
