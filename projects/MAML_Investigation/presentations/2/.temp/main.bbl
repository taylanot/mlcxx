% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{Denevi2018}{article}{}
    \name{author}{4}{}{%
      {{hash=DG}{%
         family={Denevi},
         familyi={D\bibinitperiod},
         given={Giulia},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CC}{%
         family={Ciliberto},
         familyi={C\bibinitperiod},
         given={Carlo},
         giveni={C\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Stamos},
         familyi={S\bibinitperiod},
         given={Dimitris},
         giveni={D\bibinitperiod},
      }}%
      {{hash=PM}{%
         family={Pontil},
         familyi={P\bibinitperiod},
         given={Massimiliano},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{DGCCSDPM1}
    \strng{fullhash}{DGCCSDPM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2018}
    \field{labeldatesource}{}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    In learning-to-learn the goal is to infer a learning algorithm that works
  well on a class of tasks sampled from an unknown metadistribution. In
  contrast to previous work on batch learning-to-learn, we consider a scenario
  where tasks are presented sequentially and the algorithm needs to adapt
  incrementally to improve its performance on future tasks. Key to this setting
  is for the algorithm to rapidly incorporate new observations into the model
  as they arrive, without keeping them in memory. We focus on the case where
  the underlying algorithm is Ridge Regression parametrised by a symmetric
  positive semidefinite matrix. We propose to learn this matrix by applying a
  stochastic strategy to minimize the empirical error incurred by Ridge
  Regression on future tasks sampled from the meta-distribution. We study the
  statistical properties of the proposed algorithm and prove non-asymptotic
  bounds on its excess transfer risk, that is, the generalization performance
  on new tasks from the same meta-distribution. We compare our online
  learning-to-learn approach with a state-of-the-art batch method, both
  theoretically and empirically.%
    }
    \verb{eprint}
    \verb 1803.08089
    \endverb
    \field{isbn}{9781510871601}
    \field{pages}{457\bibrangedash 466}
    \field{title}{{Incremental learning-to-learn with statistical guarantees}}
    \field{volume}{1}
    \verb{file}
    \verb :home/taylanot/Dropbox/PhD/Papers/subjects/lit_rev_miguel/1803.08089.
    \verb pdf:pdf
    \endverb
    \field{journaltitle}{34th Conference on Uncertainty in Artificial
  Intelligence 2018, UAI 2018}
    \field{eprinttype}{arXiv}
    \field{year}{2018}
  \endentry

  \entry{Scholkopf2002}{article}{}
    \name{author}{1}{}{%
      {{hash=SB}{%
         family={Sch{\"{o}}lkopf},
         familyi={S\bibinitperiod},
         given={Bernhard},
         giveni={B\bibinitperiod},
      }}%
    }
    \strng{namehash}{SB1}
    \strng{fullhash}{SB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2002}
    \field{labeldatesource}{}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    In the 90s, a new type of learning algorithm was developed, based on
  results from statistical learning theory: the Support Vector Machine (SVM).
  This gave rise to the development of a new class of theoretically elegant
  learning machines which use a central concept of SVMs - \emph {kernels} - for
  a number of different learning tasks. Kernel machines now provide a modular
  and simple to use framework that can be adapted to different tasks and
  domains by the choice of the kernel function and the base algorithm. They
  have been shown to perform very well in problems ranging from computer vision
  to text categorization and applications in computational biology. The talk
  will start with an outline of the basic ideas of kernel machines and
  statistical learning theory. Following this, new approaches and directions in
  kernel machine research will be discussed, including a justification of the
  large margin principle using compression, and an algorithm for a big class of
  new learning tasks.%
    }
    \verb{doi}
    \verb 10.7551/mitpress/4175.001.0001
    \endverb
    \field{isbn}{0780375084}
    \field{title}{{Learning with kernels}}
    \field{volume}{1}
    \verb{file}
    \verb :home/taylanot/Dropbox/Books/ML/0262194759_TOC.pdf:pdf
    \endverb
    \field{journaltitle}{Proceedings of 2002 International Conference on
  Machine Learning and Cybernetics}
    \field{year}{2002}
  \endentry
\enddatalist
\endinput
