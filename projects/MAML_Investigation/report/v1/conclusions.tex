Upon our investigation, it is found empirically that meta-information about the task-space can help the generalization performance in linear and nonlinear problem settings. It is found out that the MAML variant methods are superior under certain conditions. It is observed that the gradient-based meta-learning method MAML is only capable performing better only when the task distribution variance is small, compared to regularization-based meta-learning models. Moreover, it is shown that limiting gradient descent steps taken can be beneficial due to the regularizing effect in both linear and nonlinear problem setting. 

Comparing the meta-regularization information utilization via biased regularization provided in \cite{Denevi2018a} has clear advantages in linear problem setting, when compared to meta-information inclusion via training an intermediate model presented in \cite{Finn2017}. Looking at the performance of the biased regularization (\eg General Ridge model) observation including the single task learning performance of the Kernel Ridge regression for the selected investigations, begs the question; "If we can, somehow, incorporate information regarding the task distribution, can we come up with a simpler kernelized and convex model to achieve a more competing method?". Investigation of this type of method is especially interesting for the reasons of explainability and the ease of theoretical understanding compared to models which have non-convex loss spaces.

