Upon our investigation we it is found empirically that meta-information about the task-space can help the generalization performance in linear and nonlinear problem settings. This is observed to be true for both gradient-based meta-learning methods and regularization based meta-learning models. It is observed that limiting gradient descent can provide a regularizing effect in nonlinear problem setting by looking at the expected error of the model for the tasks. The benefit of utilizing task information can prove to be useful in any shape or form.

Comparing the meta-regularization information utilization via biased regularization provided in \cite{Denevi2018a} has clear advantages in linear problem setting, when compared to meta-information inclusion via training an intermediate model presented in \cite{Finn2017}. Looking at the performance of the biased regularization (\eg General Ridge model) observation including the single task learning performance of the Kernel Ridge regression for the selected investigations, begs the question; "If we can, somehow, incorporate information regarding the task distribution, can we come up with a simpler kernelized and convex model to achieve a more competing method?". Investigation of this type of method is especially interesting for the reasons of explainability and for the ease of theoretical understanding of compared to models which have non-convex loss spaces.



