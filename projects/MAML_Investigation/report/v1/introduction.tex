Learning-to-learn(meta-learning) is a hot research field which treats the learning of the traditional machine learning task as the learning problem. The utility that comes with this specific learning enhances the capability of the machine related prediction tasks by means of increased efficiency of data utilization resulting from the context detection capability of the algorithms presented under this paradigm. 

Model-Agnostic Machine Learning (MAML)\cite{Finn2017} is arguably one of the most impactful meta-learning algorithm that is proposed recently. The reason for this method to gain this much traction can be associated with it being non-parametric in the meta level. In a broad sense, this algorithm seeks to find an intermediate model in the environment of tasks. The way to obtain this model is to look at the future loss of a possible gradient steps from the tasks observed. This makes MAML highly attractive for the gradient based methods as the implementation effort that goes into any model that rely on gradients are minimal. Given the fact that most deep learning frameworks rely on the gradient descent the research avenue that \cite{Finn2017} opens up is quite wide.

Although, there are multiples of works that built upon the ideology presented in \cite{Finn2017}, the claimed generalization aspect is investigated in a limited extended only with convex problem assumptions. In a similar fashion this paper aims to investigate the generalization of MAML with the aim to find out to what extend the claims regarding generalization hold. The main research questions investigated in this paper are:

\textit{
\begin{itemize}
  \item What is the extend of MAML's generalization capabilities?
  \item Is MAML algorithm really model agnostic?
  \item Is the generalization performance of MAML, merely and artifact of non-convex problem setting?
\end{itemize}
}

Authors' of this paper acknowledges the fact that in the definition of meta-learning given in \cite{Finn2017} includes the term "quickly learn" which indicates that there is computational budget limitation. That is why the biggest chunk of interest lies in the small sample and limited gradient steps cases in both linear and non-linear  problem setting. However, the effect of number of gradient steps is investigated as well.



