Learning-to-learn(meta-learning) is a hot research field which treats the learning of the traditional machine learning task as the learning problem. The utility that comes with this specific learning enhances the capability of the machine related prediction tasks by means of increased efficiency of data utilization resulting from the context detection capability of the algorithms presented under this paradigm. 

Model-Agnostic Machine Learning (MAML)\cite{Finn2017} is arguably one of the most impactful meta-learning algorithm that is proposed recently. The reason for this method to gain this much traction can be associated with it being non-parametric in the meta level. In a broad sense, this algorithm seeks to find an intermediate model in the environment of tasks. The way to obtain this model is to look at the future loss of a possible gradient steps from the tasks observed. This makes MAML highly attractive for the gradient based methods as the implementation effort that goes into any model that rely on gradients are minimal. Given the fact that most deep learning frameworks rely on the gradient descent the research avenue that \cite{Finn2017} opens up is quite wide.

Although, there are multiples of works that built upon the ideology presented in \cite{Finn2017} (\eg \cite{Flennerhag2019,Nichol2018,Rajasegaran2020,Collins2020,Guiroy2019a, Antoniou2019}) to improve upons its defficiencies. However, the claimed optimization for generalization \cite{Nichol2018, Finn2017} aspect for the adapted task is investigated to a limited extent. Espeically compared to models simple models without meta-learning on any given task. In a similar fashion this paper aims to investigate the generalization of MAML for a given task with the aim to find out to what extend the claims regarding generalization are valid, to what extent a gradient based meta-learner that tries to find a warm starting point for a given task distribution. Hence, the main of this paper is to provide an emprical study of a gradient based meta-learning algortihm. The main research questions investigated in this paper are:

\begin{itemize}
  \item \textit{What is the extend of MAML's generalization capabilities compared to Bayes Error and other methods that has no information regarding the other tasks?} We will compare the simple models (\eg Least-Squares and Ridge Regression) with the Bayes Error as the baseline to see how much benefit does MAML provide in terms of generalization compared to other methods in various problems.

  \item \textit{Is MAML algorithm, model agnostic?} By means of considering simple linear models performance of MAML is investigated in a simple settings. 

  \item \textit{Is the generalization performance of MAML, merely an artifact of non-convex problem setting?} Comparision of the convex and non-convex problem settings, we will try to investigate the generalization performance of MAML algorithm.

\end{itemize}

%In \cite{Finn2017} the term meta-learning is constrained to "quickly learn" which indicates that there is computational budget limitation. That is why the biggest chunk of interest lies in the small sample and limited gradient steps cases. 


