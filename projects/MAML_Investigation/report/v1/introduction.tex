Learning-to-learn(meta-learning) is a hot research field that treats the learning of the traditional machine learning task as the learning problem. The utility that comes with this specific learning paradigm enhances the capability of the machine-related prediction tasks employing increased efficiency of data utilization resulting from the context detection capability of the algorithms presented under this paradigm. 

Model-Agnostic Machine Learning (MAML)\cite{Finn2017} is arguably one of the most impactful meta-learning algorithms that is proposed recently. The reason for this method to gain this much traction can be associated with it being non-parametric in the meta-level. In a broad sense, this algorithm seeks to find an intermediate model in the environment of tasks. The way to obtain this model is to look at the future loss of possible gradient steps from the tasks observed. This makes MAML highly attractive for the gradient-based methods as the implementation effort that goes into any model that relies on gradients is minimal. Given the wide-spread use of gradient descent in machine learning applications, the research avenue that \cite{Finn2017} opens up is quite wide.

Although, there are multiples of works that built upon the ideology presented in \cite{Finn2017} (\eg \cite{Flennerhag2019,Nichol2018,Rajasegaran2020,Collins2020,Guiroy2019a, Antoniou2019}) to improve upon its deficiencies. The claimed optimization for generalization \cite {Nichol2018, Finn2017} aspect for the adapted task is investigated to a limited extent. Especially compared to models single task learning models without meta-learning on any given task. Similarly, this paper aims to investigate the generalization of MAML for a given task to find out to what extent the claims regarding generalization are valid, to what extent a gradient-based meta-learner that tries to find a warm starting point for given task distribution. Hence, the main of this paper is to provide an empirical study of a gradient-based meta-learning algorithm. The main research questions investigated in this paper are:

\begin{itemize}
  \item \textit{What is the extent of MAML's generalization capabilities?} We will compare the single task learning models (\eg Least-Squares and Ridge Regression) with the Bayes Error as the baseline to see how much benefit does MAML provides in terms of generalization compared to other methods in various problems.

  \item \textit{Is MAML algorithm, model agnostic?} Through considering linear models performance of MAML is investigated in a convex problem setting. We try to investigate if there is performance gain for linear problem setting, where less parameters are involved.

%  \item \textit{Is the generalization performance of MAML, merely an artifact of non-convex problem setting?} Comparision of the convex and non-convex problem settings, we will try to investigate the generalization performance of the MAML algorithm.

\end{itemize}

%In \cite{Finn2017} the term meta-learning is constrained to "quickly learn" which indicates that there is a computational budget limitation. That is why the biggest chunk of interest lies in the small sample and limited gradient steps cases. 

