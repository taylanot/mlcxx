This works aims to investigate the generalization performance via the expected loss investigation of, arguably the most impactful meta-learning algorithm Model-Agnostic Meta-Learning (MAML) \cite{Finn2017}, which claims to have "good" generalization capabilities for a given task in few-shot image classification, few-shot regression, and reinforcement learning scenarios. However, the claimed generalization properties of this method remain rather elusive due to the non-convex problem settings that it is being utilized. This work aim illustrates first in a linear and convex, then in a non-linear and non-convex supervised learning setting.  Throughout the work compared with the Bayes error is used as a baseline and models with convex losses are used as benchmarks with or without meta-information for the problems being considered. %We aim to showcase the gain of performance that comes with the MAML if there is any.
