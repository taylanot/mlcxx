This works aims to investigate the generalization via the expected loss calculation of, arguably the most impactful meta-learning algorithm Model-Agnostic Meta Learning (MAML) \cite{Finn2017}, which claims to have "good" generalization capabilities for a given task in few-shot image classification, few-shot regression and reinforcement learning scenarios. However, the claimed generalization properties of this method remains rather illusive due to non-convex problem settings that it is being utilized. This work aims illustrates first in a linear and convex, then in a non-linear and non-convex supervised learning setting.  Throughout the work comparison with the Bayes error is used as a baseline and models with convex losses are used as benchmarks with or without meta-information for the problems being considered. We aim to showcase the gain of performance that comes with the MAML, if there is any.


