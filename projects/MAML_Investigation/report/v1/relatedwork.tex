
Since its publications MAML \cite{Finn2017} opened a wide range of research path that tries to understand gradient based meta learning which aims to find an intermediate model to be used for adaptation at a later stage. There are both empirical and theoretical work dedicated to the understanding of MAML.

The, empirical works try to understand and improve the MAML algorithm. For example, the sensitivity of MAML to architectural details and the difficulty in training of the MAML algorithm is addressed and tackled in \cite{Antoniou2019} under the name of MAML++. This method tries to provide a much stable algorithm by overcoming the gradient instability, the absence of batch statistics accumulation, the shared bias per layer, constant learning rates utilized in the algorithm. Although, its training difficulties the generalization capabilities are empirically investigated in \cite{Guiroy2019a} and concluded that the generalization to new tasks can be related to the similarity between the trajectories of encountered during meta-training and the adaptation procedures. Moreover, in the same work $L_2$ regularization like regularization for the inner loop is proposed for improved generalization performance. 

Aside from empirical studies some theoretical work trying to understand the convergence and the generalization of MAML over the past years. However, since theoretical investigation of over-parametrized neural network are quite cumbersome, most of the effort goes in to understanding this algorithm in much milder settings with strong assumptions. For instance, in \cite{Khodak2019} various gradient-based meta-learning algorithms including MAML and its derivatives are investigated for convex optimization problem and their usefulness is shown compared to single task learning. In addition, a negative learning rates in the inner loop is shown to be optimal during meta-training stage, theoretically in \cite{Bernacchia2021} for mixed linear regression problem. Moreover, in \cite{Collins2020b} the effect of task distribution is is investigated for linear regression problems and shown that when tasks are similar to each other although the task hardness (determined by the curvature of the loss) MAML is able to achieve success if the tasks seen in test time are similar to the ones seen at adaptation step. Finally, in \cite{Fallah2021} the generalization of MAML is investigated from the algorithmic stability and generalization bounds perspective and concluded that the MAML generalizes well even to an unseen task if the training and test task distributions are sufficiently close. 


Another line of work, tries to come up with other meta learning scenarios involving convex settings by construction of the model like least-squares regression. In \cite{Denevi2018a, Denevi2019} meta-learning models inspired by the biased regularization works \cite{Kuzborskij2017, Kuzborskij2017a} where bias is tried to be learned from the task environment by minimizing the \textit{Transfer Risk} (Expected Loss), is proposed. Due to the nature of the problem the theoretical foundation is also provided for the proposed models. For  learning-to-learn and continual learning settings. On top of this work \cite{Bai2020} investigate the need for \textit{train-test} split for the same models and concludes that the \textit{train-train} model achieves strictly better generalization performance for structured tasks in the setting of learning around a common mean problem presented in \cite{Denevi2018a}. 


\paragraph{Our Contribution:} On one hand, the loss MAML \cite{Finn2017} is trying to optimize is the loss that the model would make given a batch of tasks obtained from the environment of certain task distribution. The model parameters update are done by looking at the possible loss for each task if the models parameters are changed for a certain task. On the other hand, the methods proposed by \cite{Denevi2018a} is trying to optimize for the so called "Transfer Risk" in other words the expected loss over for the task distribution. The main contribution of this paper is to investigate in linear and nonlinear settings the average performance of the MAML algorithm by looking at its expected loss, which will then be compared to individual learning task performance where there is no information coming from the task environment. The reason that the expected loss of MAML is not investigated can speculated to be the computational burden that it bestows upon the problem. Here, for the linear problem case this problem is tried to be elevated by means of NUMBA \cite{Lam2015}, which is able to create compiled code for Python. However, due to the inflexibility of the NUMBA, pure PyTorch \cite{Paszke2019} implementation, which is slower, is used for nonlinear problem with a lesser degree of fidelity. 

