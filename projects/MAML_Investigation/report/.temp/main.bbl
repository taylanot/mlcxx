\begin{thebibliography}{10}

\bibitem{Finn2017}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock {Model-agnostic meta-learning for fast adaptation of deep networks}.
\newblock {\em 34th International Conference on Machine Learning, ICML 2017},
  3:1856--1868, 2017.

\bibitem{Flennerhag2019}
Sebastian Flennerhag, Andrei~A. Rusu, Razvan Pascanu, Francesco Visin, Hujun
  Yin, and Raia Hadsell.
\newblock {Meta-Learning with Warped Gradient Descent}.
\newblock pages 1--28, 2019.

\bibitem{Nichol2018}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock {On first-order meta-learning algorithms}.
\newblock {\em arXiv}, pages 1--15, 2018.

\bibitem{Rajasegaran2020}
Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad~Shahbaz Khan, and
  Mubarak Shah.
\newblock {ITAML: An Incremental Task-Agnostic Meta-learning Approach}.
\newblock {\em Proceedings of the IEEE Computer Society Conference on Computer
  Vision and Pattern Recognition}, pages 13585--13594, 2020.

\bibitem{Collins2020}
Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai.
\newblock {Task-robust model-agnostic meta-learning}.
\newblock {\em Advances in Neural Information Processing Systems},
  2020-Decem:1--30, 2020.

\bibitem{Guiroy2019a}
Simon Guiroy, Vikas Verma, and Christopher Pal.
\newblock {Towards Understanding Generalization in Gradient-Based
  Meta-Learning}.
\newblock 2019.

\bibitem{Antoniou2019}
Antreas Antoniou, Amos Storkey, and Harrison Edwards.
\newblock {How to train your MAML}.
\newblock {\em 7th International Conference on Learning Representations, ICLR
  2019}, pages 1--11, 2019.

\bibitem{Khodak2019}
Mikhail Khodak, Maria~Fiorina Balcan, and Ameet Talwalkar.
\newblock {Provable guarantees for gradient-based meta-learning}.
\newblock {\em 36th International Conference on Machine Learning, ICML 2019},
  2019-June:651--675, 2019.

\bibitem{Bernacchia2021}
Alberto Bernacchia.
\newblock {Meta-learning with Negative Learning Rates}.
\newblock (2020), 2021.

\bibitem{Collins2020b}
Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai.
\newblock {How Does the Task Landscape Affect MAML Performance?}
\newblock pages 1--47, 2020.

\bibitem{Fallah2021}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock {Generalization of Model-Agnostic Meta-Learning Algorithms: Recurring
  and Unseen Tasks}.
\newblock pages 1--23, 2021.

\bibitem{Denevi2018a}
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil.
\newblock {Learning to learn around a common mean}.
\newblock {\em Advances in Neural Information Processing Systems},
  2018-Decem(NeurIPS):10169--10179, 2018.

\bibitem{Denevi2019}
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil.
\newblock {Learning-to-learn stochastic gradient descent with biased
  regularization}.
\newblock {\em 36th International Conference on Machine Learning, ICML 2019},
  2019-June:2814--2843, 2019.

\bibitem{Kuzborskij2017}
Ilja Kuzborskij, Francesco Orabona, and Barbara Caputo.
\newblock {Scalable greedy algorithms for transfer learning}.
\newblock {\em Computer Vision and Image Understanding}, 156:174--185, 2017.

\bibitem{Kuzborskij2017a}
Ilja Kuzborskij and Nicol{\`{o}} Cesa-Bianchi.
\newblock {Nonparametric online regression while learning the metric}.
\newblock {\em Advances in Neural Information Processing Systems},
  2017-Decem(2):668--677, 2017.

\bibitem{Bai2020}
Yu~Bai, Minshuo Chen, Pan Zhou, Tuo Zhao, Jason~D. Lee, Sham Kakade, Huan Wang,
  and Caiming Xiong.
\newblock {How Important is the Train-Validation Split in Meta-Learning?}
\newblock 2020.

\bibitem{Lam2015}
Siu~Kwan Lam, Antoine Pitrou, and Stanley Seibert.
\newblock {Numba}.
\newblock pages 1--6, 2015.

\bibitem{Paszke2019}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas K{\"{o}}pf, Edward Yang, Zach DeVito, Martin Raison,
  Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and
  Soumith Chintala.
\newblock {PyTorch: An imperative style, high-performance deep learning
  library}.
\newblock {\em Advances in Neural Information Processing Systems}, 32(NeurIPS),
  2019.

\bibitem{Nakkiran2020b}
Preetum Nakkiran.
\newblock {Learning Rate Annealing Can Provably Help Generalization, Even for
  Convex Problems}.
\newblock pages 1--9, 2020.

\bibitem{Denevi2018}
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil.
\newblock {Incremental learning-to-learn with statistical guarantees}.
\newblock {\em 34th Conference on Uncertainty in Artificial Intelligence 2018,
  UAI 2018}, 1:457--466, 2018.

\end{thebibliography}
