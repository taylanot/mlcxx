Throughout this work uppercase bold letters (\eg $\mathbf{X}$), lowercase bold letters (\eg $\mathbf{x}$), and lowercase letters (\eg ${x}$) are used for matrices, vectors and scalars respectively. Moreover, the vectors are assumed to be stored in columns. Finally, the $\I_{D}$ represents a $D\times D$ identity matrix, the $\ones_{D}$ and $\zeros_{D}$ represents $D\times 1$ vector of ones and zeros respectively.

\subsection{Learning Problems}

In this work one linear and one nonlinear problem constitute the family of tasks. In the linear case a realization of slope of a linear function will be used for the definition of a single task, and for the nonlinear case a single task is represented by a realization of amplitude and phase for the sine function. 

\subsubsection{Linear Problem}\label{sec:Linear}

Consider the conventional linear regression problem in $\R^D$ is given by
\begin{equation}\label{eq:linearreg}
  \lab = \trans{\inp}\scaletask+\noise, 
\end{equation}
where $\lab\in\R$, $\inp\in\R^D$, $\scaletask\in\R^D$ and $\noise\sim\normal{0}{\var}$. Each realization of $\scaletask$ corresponds to a task $\task$ and collection of $N$ observations is represented by $\dataset_{j}:=(\inp, \lab)_{i=1}^{N}$. 

\subsubsection{Nonlinear Problem}\label{sec:Nonlinear}

Consider a nonlinear problem in $\R^D$ is given by
\begin{equation}\label{eq:nonlinearreg}
  \lab = \trans{\sine(\inp+\phasetask)}\scaletask+\noise, 
\end{equation}
where $\lab\in\R$, $\inp\in\R^D$, $\scaletask\in\R^D$ and $\noise\sim\normal{0}{\var}$. Assuming that the each realization of scale term $\scaletask$ and $\phasetask$ corresponds to a task observed in the environment $\task$ and each set of observed $N$ input ($\inp$) and its corresponding label ($\lab$) is represented by a dataset $\dataset_{j}:=(\inp_i, \lab_i)_{i=1}^{N}$.

\subsubsection{General Problem Setting}
For both problems presented in Sections \ref{sec:Linear} and \ref{sec:Nonlinear} sample distribution is given by $\prob_\dataset$ for a given $\task$ and the task distribution is represented by $\prob_\task$. A model parameterized by $\param$ is represented by $\model(\inp, \param):\inp\to\lab$. An estimator that is trained with $\dataset$ that is obtained from the $\task$ is represented by $\estim(\inp)$. Noting that $\estim(\inp)$ for a base learner is only exposed to a single task $\task$ and a single dataset $\dataset$, whereas  a meta learner, in this case MAML, is exposed to multiple tasks $T$'s and multiple datasets $\dataset$ in the meta-learning stage and then the adaptation is done as in the case of a base learner with just a single single task $\task$ and a single dataset $\dataset$.  The discrepancy between the prediction of the estimator $\estim$ and $\lab$ is measured in terms of squared loss $\loss:=(\estim(x)-\lab)^2$. The main loss that this paper tries to investigate is the \textit{Expected Squared Loss} of an estimator $\estim$ over the $\prob_{\task}$. Then the expected squared loss can be represented as

\begin{equation}\label{eq:ee}
  \EE:= \iiint(\estim(x) - y)^2\prob(\inp, y)\prob_{\dataset}\prob_{\task} d\inp d\lab d\dataset d\task.
\end{equation}

For the defined expected error and the problem definitions, the \textit{Bayes Error} is given by $\sigma^2$ that is coming from the noise term, which represents a model that is the perfect estimator, which is referred as oracle in some of the meta-learning literature.

\subsubsection{Experimental Assumptions}
For all the problems the input distribution is given by $\prob_\inp\sim\normal{0,k\I}$ where $k$ is a parameter for the variance of the inputs. For the linear problem the $\prob_\task:=\prob(\scaletask)\sim\normal{{m\ones_{D}},{c\I_{D}}}$ and for nonlinear problem the task distribution takes the form of a joint distribution $\prob_\task:=\prob(\scaletask, \phasetask)$ where $\prob_{\scaletask}\sim\normal{\ones_{D},c_1\I_{D}}$ and $\prob_{\phasetask}\sim\normal{\zeros_{D},c_2\I_{D}}$

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includetikz{\textwidth}{Figures/methods/lin_eg.tikz}
    \caption{$\lab = \trans{\inp}\scaletask$}
    \label{fig:lintasks}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includetikz{\textwidth}{Figures/methods/nonlin_eg.tikz}
    \caption{$\lab = \trans{\sine(\inp+\phasetask)}\scaletask$}
    \label{fig:nonlintasks}
  \end{subfigure}
  \caption{100 sample tasks drawn from $\prob_\task$ for both linear ($m=0$ and $c=1$) and nonlinear ($c_1=1$ and $c_2=1$) problems.}
\end{figure}

\subsection{Models} 

\subsubsection{Single Task Learning Models}

Assuming a linear model in the form of $\model(\inp,\slope,\bias):=\inp\slope+\bias$ or $\model(\inp, \param):=\inpbias\param$ with $\inp\in\R^{1\times D}$, $\slope\in\R^{D\times 1}$, $\inpbias\in\R^{1\times D+1}$ and $\param\in\R^{D+1\times}$ where $\param:=\trans{[\slope, \bias]}$ and $\inpbias:=[\inp, 1]$. The optimum parameters ($\opt$) for different models are obtained as follow:

\paragraph{Linear Estimator} is given by the least-squares solution, $\opt:=(\inv{\trans{\design}\design)}\trans{\design}\labs$, where $\design\in\R^{N\times D}$ is the design matrix where the observed input data is stored in rows.

\paragraph{Ridge Estimator} is given by $\opt:=(\inv{\trans{\design}\design+\ridge \I_{D})}\trans{\design}\labs$ which is obtained by minimizing the squared loss with the additional term of $\ridge\norm{\param}{2}^2$. Thus, overall loss takes the form $\loss+\ridge\norm{\param}{2}^2$.


\paragraph{Kernel Ridge Estimator} is given by $\opt= \trans{\design}\weight$ where $\weight:=\inv{(\gram+\ridge\I_{N})}\lab$ where $\gram\in\R^{N\times N}$ is the  \textit{Gram Matrix} obtained by replacing $\trans{\design}\design$ inner product by a kernel $\kernel(\design, \design)$. Then, the prediction of the estimator takes the form $\estim(\pred,\param)=\trans{\weight}\kernel(\pred,\design)$ where $\pred\in\R^{D\times 1}$.

\paragraph{Gradient Descent Estimator for the Linear Model} for a given number of iterations $\iter$ the gradient descent estimator is given by $\{\param_{j+1}=\slope_{j} - \lr\sum_i^{N}\inp_i(\model(\inp,\param_j)-\lab_i)\}_{j=0}^{\iter}$. In other words for any given value of $\param$ one gradient update is given by the gradient with respect to $\param$ with a scaling parameter $\nu$. 

\subsubsection{Meta Learning Models}

\paragraph{Model-Agnostic Meta-Learning (MAML)} aims to obtain an intermediate model that can generalize well after adaptation to a dataset $\dataset$ observed from a new and unseen task $\task_i$ drawn from $\prob_\task$ where the number of training points $K$ and the number of iterations $\iter$ is limited. The general procedure for supervised learning problems is given in Algorithm \ref{alg:MAML}.

\begin{algorithm}
  \caption{MAML\cite{finn2017} Algorithm}\label{alg:MAML}
  \KwData{$\prob_{\task}$, $\alpha$, $\beta$}
  \KwResult{Intermediate Model $\model(\param_{meta})$}
  initialize $\param$ randomly; \\
  \While{not done}
  {
    sample a batch of tasks $\task_i$ from $\prob_{\task}$\\
    \ForAll{$\task_i$}
    {
      Obtain future gradients: $\grad{\param}\loss_{\task_i}(\model(\param))$ wrt. $\dataset_K$ \\
      Possible future parameters: $\param_i^\prime = \param_i -\alpha\grad{\param}\loss_{\task_i}(\model(\param))$
    }
    Update: $\param \leftarrow \param- \beta\grad{\param}\sum\loss_{\task_i}(\model(\param_i^\prime))$
  }
\end{algorithm}

As stated before after employing this algorithm the model $\model(\inp, \param)$ is left at a place that is in optimal position for quick adaptation. At the time for inference for a particular dataset $\dataset$ obtained from a particular task $\task$, simple gradient descent as mentioned above is utilized for the quick adaptation.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includetikz{\textwidth}{Figures/methods/lin_maml.tikz}
    \caption{Linear Problem}
    \label{fig:lin_maml}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includetikz{\textwidth}{Figures/methods/nonlin_maml.tikz}
    \caption{Nonlinear Problem}
    \label{fig:nonlin_maml}
  \end{subfigure}
  \caption{Visualizing the MAML intermediate model. 100 sample tasks drawn from $\prob_\task$ for both linear ($m=0$ and $c=1$) and nonlinear ($c_1=2$ and $c_2=2$) problems shown transparent and intermediate model trained (obtained from MAML algorithm) for 1D cases of the experimentation given with solid dark orange line.}
\end{figure}


