To formalize our supervised learning problem setting let us start by introducing generic input and output spaces as $\ispace$ and $\ospace$. A learning algorithm $\algo$ takes i.i.d samples $\samp:=(\inp,\out)_{i=1}^{N}$ (also represented by$\algo(\samp)$), from an unknown distribution $\prob_{\samp}$ over $\ispace \times \ospace$ and produces a hypothesis $\hyp$ from hypothesis class $\hypspace$ defined by the learner $\algo$. Then, the prediction of a learner can be represented as $\pred=h(\inp)\in\ospace$. Moreover, the performance of the learner is measured by a loss function $\loss(\out,\pred)$. Thus, the expected loss or risk $\risk$ of a hypothesis generated by a learning algorithm over the true distribution $\prob_{\samp}$ is given as:

\begin{equation}
 \risk(\hyp) = \int\loss(\out,\pred)d\prob_{\samp}.
\end{equation}

%Noting that the loss used to calculate the risk does not need to be the same loss utilized in the training of the learner. 

A reliable learning curve of a learner is obtained by averaging over many $\samp$ for varying $\nsamp$. Then, the average risk of a learner is given as:
\begin{equation}
 \avgrisk(\algo, \nsamp) = \underset{{\samp}}\expect\risk(\algo(\samp))).
\end{equation}

An individual learning curve $\lc:\R\to\R$ depends on $\algo$ and $\prob_{\samp}$. This allows us to represent this as a supervised learning problem. Let us assume that predicting a learning curve is the task $\task_i$ that we are interested in and we are given training size and corresponding risk values from this curve $\samplr:=(\nsamp_i,\risk_i)_{i=1}^{\nsamplr}$ and learning curves $(\lc_i)_{i=1}^\numlc$. Note that the curves are not analytical but in a discretized point-wise available. As mentioned before the cost of obtaining a learning curve increases as the $\nsamp$ increases. Thus, we assume that $\nsampi\subset[0,\nlim]$ where $\nlim\in\Zpos$, but limited to be small, so that the cost of obtaining $\risk(\algo(\sampi))$ is not large. In other words, we are trying to model $\lc$ by training a learner $\model:\R\to\R$ with samples $\samplr$ and $(\lc_i)_{i=1}^\numlc$.

It should be noted that in most cases the true distribution of the data is unknown, thus the empirical version of the risk is obtained by creating one hold-out set and several cross-validation splits. Moreover, additional splits might be necessary if the hyper-parameters of the learner are to be tuned as well. 


