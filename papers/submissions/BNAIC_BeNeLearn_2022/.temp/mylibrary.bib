
@article{achille2019,
  title = {{{Task2Vec}}: {{Task Embedding}} for {{Meta-Learning}}},
  shorttitle = {{{Task2Vec}}},
  author = {Achille, Alessandro and Lam, Michael and Tewari, Rahul and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless and Soatto, Stefano and Perona, Pietro},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.03545 [cs, stat]},
  eprint = {1902.03545},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a ``probe network'' and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar). We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/9GTMRFYF/Achille et al. - 2019 - Task2Vec Task Embedding for Meta-Learning.pdf}
}

@article{adeli,
  title = {Knowledge Engineering},
  author = {Adeli, H},
  journal = {Knowledge Engineering},
  pages = {18},
  abstract = {Several new and emerging computational paradigms for engineering problem solving automation and processing of various types of knowledge are reviewed. They are artificial intelligence/sybmolic processing, object-oriented programming paradigm, machine learning, evolutionary computing/genetic algorithms, and neurocomputing. Examples of recent research efforts in each area are briefly described.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/X83SV5BW/Adeli - Knowledge engineering.pdf}
}

@article{aiolli,
  title = {Transfer {{Learning}} by {{Kernel Meta-Learning}}},
  author = {Aiolli, Fabio},
  pages = {15},
  abstract = {A crucial issue in machine learning is how to learn appropriate representations for data. Recently, much work has been devoted to kernel learning, that is, the problem of finding a good kernel matrix for a given task. This can be done in a semi-supervised learning setting by using a large set of unlabeled data and a (typically small) set of i.i.d. labeled data. Another, even more challenging problem, is how one can exploit partially labeled data of a source task to learn good representations for a different, but related, target task. This is the main subject of transfer learning.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/ERZH8PJK/Aiolli - Transfer Learning by Kernel Meta-Learning.pdf}
}

@article{alexander2021,
  title = {The {{Autodidactic Universe}}},
  author = {Alexander, Stephon and Cunningham, William J. and Lanier, Jaron and Smolin, Lee and Stanojevic, Stefan and Toomey, Michael W. and Wecker, Dave},
  year = {2021},
  month = sep,
  journal = {arXiv:2104.03902 [gr-qc, physics:hep-th, physics:physics, physics:quant-ph]},
  eprint = {2104.03902},
  eprinttype = {arxiv},
  primaryclass = {gr-qc, physics:hep-th, physics:physics, physics:quant-ph},
  abstract = {We present an approach to cosmology in which the Universe learns its own physical laws. It does so by exploring a landscape of possible laws, which we express as a certain class of matrix models. We discover maps that put each of these matrix models in correspondence with both a gauge/gravity theory and a mathematical model of a learning machine, such as a deep recurrent, cyclic neural network. This establishes a correspondence between each solution of the physical theory and a run of a neural network.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,General Relativity and Quantum Cosmology,High Energy Physics - Theory,Physics - History and Philosophy of Physics,Quantum Physics},
  file = {/home/taylanot/Zotero/storage/S5XZN9ED/Alexander et al. - 2021 - The Autodidactic Universe.pdf}
}

@article{aln2015,
  title = {The {{FEniCS Project Version}} 1.5},
  author = {Aln, Martin S and Kehlet, Benjamin and Logg, Anders and Richardson, Chris and Ring, Johannes and Rognes, E and Wells, Garth N},
  year = {2015},
  pages = {15},
  abstract = {The FEniCS Project is a collaborative project for the development of innovative concepts and tools for automated scientific computing, with a particular focus on the solution of differential equations by finite element methods. The FEniCS Project consists of a collection of interoperable software components, including DOLFIN, FFC, FIAT, Instant, mshr, and UFL. This note describes the new features and changes introduced in the release of FEniCS version 1.5.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/YIASEI7C/Aln et al. - 2015 - The FEniCS Project Version 1.5.pdf}
}

@article{andrychowicz2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {\noopsort{freitas}}{de Freitas}, Nando},
  year = {2016},
  month = nov,
  journal = {arXiv:1606.04474 [cs]},
  eprint = {1606.04474},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/taylanot/Zotero/storage/4LHY89F4/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf;/home/taylanot/Zotero/storage/JR654ZN6/Andrychowicz et al. - Learning to learn by gradient descent by gradient .pdf}
}

@misc{antoniou2019,
  title = {How to Train Your {{MAML}}},
  author = {Antoniou, Antreas and Edwards, Harrison and Storkey, Amos},
  year = {2019},
  month = mar,
  number = {arXiv:1810.09502},
  eprint = {1810.09502},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem. Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/T3JFWBN9/Antoniou et al. - 2019 - How to train your MAML.pdf}
}

@article{arenas2021,
  title = {The Breakdown of {{Moore}}'s Law Induced by Weak {{Anderson}} Localization and by Size Effects in Nano-Scale Metallic Connectors},
  author = {Arenas, Claudio and Herrera, Guillermo and Mu{\~n}oz, Enrique and Munoz, Raul C},
  year = {2021},
  month = jan,
  journal = {Materials Research Express},
  volume = {8},
  number = {1},
  pages = {015026},
  issn = {2053-1591},
  doi = {10.1088/2053-1591/abd422},
  abstract = {Abstract                            We report the resistivity measured at temperatures between 5 K and 300 K of a Cu film 63 nm thick with grains that have a diameter d~=~10.5 nm on the average. The resistivity of this film is described by the first quantum theory of resistivity of nano-scale metallic connectors [R C Munoz               et al               , App. Phys. Rev.               4               (2017) 011102]. We also report an improved version of this theory that includes a new analytical description of the effect of grain boundary disorder on electron transport. We employ the surface roughness and grain size distribution measured on this Cu film as input data to compute, using our heory, the room temperature resistivity of Cu wires of rectangular cross section, and compare with the resistivity of these wires reported in the literature [M H Van der Veen               et al               , 2018 IEEE International Interconnect Technology Conference (IITC) (2018)], that are used for designing Integrated Circuits (IC) for the 14 nm, 10 nm, 7 nm, 5 nm, 3 nm and 2 nm nodes, respectively. The quantum theory predicts an increase in resistivity with diminishing wire dimensions that accurately agrees with the room temperature resistivity measured on these Cu wires. The resistivity induced by electron-rough surface scattering accounts for about half of the increase over the bulk observed in the 3 nm and 2 nm tech node; scattering by non-uniform grain boundaries contributes the remaining increase in resistivity\textemdash the latter is responsible for the weak Anderson localization. According to the description of electron motion furnished by this improved quantum theory, the break down of Moore's law with shrinking wire dimensions is to be expected, since it               originates from size effects triggered by electron scattering with rough surfaces and scattering by non-equally spaced grain boundaries, which become dominant as the dimensions of the metallic wire shrinks.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/2MJU7RP8/Arenas et al. - 2021 - The breakdown of Mooreâ€™s law induced by weak Ander.pdf}
}

@article{argyriou,
  title = {A {{Unifying View}} of {{Representer Theorems}}},
  author = {Argyriou, Andreas and Dinuzzo, Francesco},
  pages = {9},
  abstract = {It is known that the solution of regularization and interpolation problems with Hilbertian penalties can be expressed as a linear combination of the data. This very useful property, called the representer theorem, has been widely studied and applied to machine learning problems. Analogous optimality conditions have appeared in other contexts, notably in matrix regularization. In this paper we propose a unified view, which generalizes the concept of representer theorems and extends necessary and sufficient conditions for such theorems to hold. Our main result shows a close connection between representer theorems and certain classes of regularization penalties, which we call orthomonotone functions. This result not only subsumes previous representer theorems as special cases but also yields a new class of optimality conditions, which goes beyond the classical linear combination of the data. Moreover, orthomonotonicity provides a useful criterion for testing whether a representer theorem holds for a specific regularization problem.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/S7VUI7FQ/Argyriou and Dinuzzo - A Unifying View of Representer Theorems.pdf}
}

@article{argyrioua,
  title = {Convex {{Multi-Task Feature Learning}}},
  author = {Argyriou, Andreas and Evgeniou, Theodoros and Pontil, Massimiliano},
  pages = {40},
  abstract = {We present a method for learning sparse representations shared across multiple tasks. This method is a generalization of the well-known singletask 1-norm regularization. It is based on a novel non-convex regularizer which controls the number of learned features common across the tasks. We prove that the method is equivalent to solving a convex optimization problem for which there is an iterative algorithm which converges to an optimal solution. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the former step it learns task-specific functions and in the latter step it learns common-across-tasks sparse representations for these functions. We also provide an extension of the algorithm which learns sparse nonlinear representations using kernels. We report experiments on simulated and real data sets which demonstrate that the proposed method can both improve the performance relative to learning each task independently and lead to a few learned features common across related tasks. Our algorithm can also be used, as a special case, to simply select \textendash{} not learn \textendash{} a few common variables across the tasks3.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/XYWA8NZD/Argyriou et al. - Convex Multi-Task Feature Learning.pdf}
}

@article{arnold2021,
  title = {When {{MAML Can Adapt Fast}} and {{How}} to {{Assist When It Cannot}}},
  author = {Arnold, S{\'e}bastien M. R. and Iqbal, Shariq and Sha, Fei},
  year = {2021},
  month = jan,
  journal = {arXiv:1910.13603 [cs, stat]},
  eprint = {1910.13603},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Model-Agnostic Meta-Learning (MAML) and its variants have achieved success in metalearning tasks on many datasets and settings. Nonetheless, we have just started to understand and analyze how they are able to adapt fast to new tasks. In this work, we contribute by conducting a series of empirical and theoretical studies, and discover several interesting, previously unknown properties of the algorithm. First, we find MAML adapts better with a deep architecture even if the tasks need only a shallow one. Secondly, linear layers can be added to the output layers of a shallower model to increase the depth without altering the modelling capacity, leading to improved performance in adaptation. Alternatively, an external and separate neural network metaoptimizer can also be used to transform the gradient updates of a smaller model so as to obtain improved performances in adaptation. Drawing from these evidences, we theorize that for a deep neural network to meta-learn well, the upper layers must transform the gradients of the bottom layers as if the upper layers were an external meta-optimizer, operating on a smaller network that is composed of the bottom layers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/JAKZRIKF/Arnold et al. - 2021 - When MAML Can Adapt Fast and How to Assist When It.pdf}
}

@article{aydin2019,
  title = {General {{Multi-Fidelity Framework}} for {{Training Artificial Neural Networks With Computational Models}}},
  author = {Aydin, Roland Can and Braeu, Fabian Albert and Cyron, Christian Johannes},
  year = {2019},
  month = apr,
  journal = {Frontiers in Materials},
  volume = {6},
  pages = {61},
  issn = {2296-8016},
  doi = {10.3389/fmats.2019.00061},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/WVY3GDRR/Aydin et al. - 2019 - General Multi-Fidelity Framework for Training Arti.pdf}
}

@article{azriel2021,
  title = {Semi-{{Supervised}} Linear Regression},
  author = {Azriel, David and Brown, Lawrence D. and Sklar, Michael and Berk, Richard and Buja, Andreas and Zhao, Linda},
  year = {2021},
  month = apr,
  journal = {arXiv:1612.02391 [math, stat]},
  eprint = {1612.02391},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {We study a regression problem where for some part of the data we observe both the label variable (Y ) and the predictors (X), while for other part of the data only the predictors are given. Such a problem arises, for example, when observations of the label variable are costly and may require a skilled human agent. When the conditional expectation ErY |Xs is not exactly linear, one can consider the best linear approximation to the conditional expectation, which can be estimated consistently by the least squares estimates (LSE). The latter depends only on the labeled data. We suggest improved alternative estimates to the LSE that use also the unlabeled data. Our estimation method can be easily implemented and has simply described asymptotic properties. The new estimates asymptotically dominate the usual standard procedures under certain non-linearity condition of ErY |Xs; otherwise, they are asymptotically equivalent. The performance of the new estimator for small sample size is investigated in an extensive simulation study. A real data example of inferring homeless population is used to illustrate the new methodology.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/taylanot/Zotero/storage/GDQF8F7D/Azriel et al. - 2021 - Semi-Supervised linear regression.pdf}
}

@article{bai2021,
  title = {How {{Important}} Is the {{Train-Validation Split}} in {{Meta-Learning}}?},
  author = {Bai, Yu and Chen, Minshuo and Zhou, Pan and Zhao, Tuo and Lee, Jason D. and Kakade, Sham and Wang, Huan and Xiong, Caiming},
  year = {2021},
  month = feb,
  journal = {arXiv:2010.05843 [cs, stat]},
  eprint = {2010.05843},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Meta-learning aims to perform fast adaptation on a new task through learning a ``prior'' from multiple existing tasks. A common practice in meta-learning is to perform a train-validation split (train-val method ) where the prior adapts to the task on one split of the data, and the resulting predictor is evaluated on another split. Despite its prevalence, the importance of the train-validation split is not well understood either in theory or in practice, particularly in comparison to the more direct train-train method, which uses all the per-task data for both training and evaluation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/5I85GCRK/Bai et al. - 2021 - How Important is the Train-Validation Split in Met.pdf}
}

@article{bao2022,
  title = {An {{Operator Learning Approach}} via {{Function-valued Reproducing Kernel Hilbert Space}} for {{Differential Equations}}},
  author = {Bao, Kaijun and Qian, Xu and Liu, Ziyuan and Song, Songhe},
  year = {2022},
  month = apr,
  journal = {arXiv:2202.09488 [cs, math]},
  eprint = {2202.09488},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Much recent work has addressed the solution of a family of partial differential equations (PDEs) by computing the inverse operator map between function spaces. Toward this end, we incorporate function-valued reproducing kernel Hilbert spaces (function-valued RKHS) in our operator learning model. Motivated by recently successful neural operator: Deep operator networks (DeepONets), we use neural networks to parameterize the Hilbert-Schmidt integral operator and propose an architecture based on the representer theorem in function-valued RKHS. Experiments including the advection, KdV, burgers', and poisson equations show that the proposed architecture has better accuracy on nonlinear PDEs and linear PDEs with a small amount of data than DeepONets. We also show that by learning the mappings between function spaces, the proposed method can find the solution of a high-resolution input after learning from lower-resolution data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Numerical Analysis},
  file = {/home/taylanot/Zotero/storage/8WVUHIVB/Bao et al. - 2022 - An Operator Learning Approach via Function-valued .pdf}
}

@article{baxter1998,
  title = {Theoretical {{Models}} of {{Learning}} to {{Learn}}},
  author = {Baxter, Jonathan},
  year = {1998},
  journal = {arXiv:2002.12364 [cs, stat]},
  eprint = {2002.12364},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.1007/978-1-4615-5529-2},
  abstract = {A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an environment of related tasks, then it can learn its own bias by learning sufficiently many tasks from the environment [4, 6]. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/3EMXBHKX/Baxter - 1998 - Theoretical Models of Learning to Learn.pdf}
}

@article{baxter2000,
  title = {A {{Model}} of {{Inductive Bias Learning}}},
  author = {Baxter, J.},
  year = {2000},
  month = mar,
  journal = {Journal of Artificial Intelligence Research},
  volume = {12},
  pages = {149--198},
  issn = {1076-9757},
  doi = {10.1613/jair.731},
  abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/IDN5NZW6/Baxter - 2000 - A Model of Inductive Bias Learning.pdf}
}

@inproceedings{bechtle2021,
  title = {Meta {{Learning}} via {{Learned Loss}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Bechtle, Sarah and Molchanov, Artem and Chebotar, Yevgen and Grefenstette, Edward and Righetti, Ludovic and Sukhatme, Gaurav and Meier, Franziska},
  year = {2021},
  month = jan,
  pages = {4161--4168},
  publisher = {{IEEE}},
  address = {{Milan, Italy}},
  doi = {10.1109/ICPR48806.2021.9412010},
  isbn = {978-1-72818-808-9},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MCT5ECB3/Bechtle et al. - 2021 - Meta Learning via Learned Loss.pdf}
}

@book{beer2011,
  title = {Mechanics of Materials},
  editor = {Beer, Ferdinand P.},
  year = {2011},
  edition = {6th ed},
  publisher = {{McGraw-Hill}},
  address = {{New York}},
  isbn = {978-0-07-338028-5},
  langid = {english},
  lccn = {TA405 .B39 2011},
  keywords = {Strength of materials,Textbooks},
  file = {/home/taylanot/Zotero/storage/FHSECTZD/Beer - 2011 - Mechanics of materials.pdf}
}

@misc{behl2019,
  title = {Alpha {{MAML}}: {{Adaptive Model-Agnostic Meta-Learning}}},
  shorttitle = {Alpha {{MAML}}},
  author = {Behl, Harkirat Singh and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Torr, Philip H. S.},
  year = {2019},
  month = may,
  number = {arXiv:1905.07435},
  eprint = {1905.07435},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/BG5RZ6LW/Behl et al. - 2019 - Alpha MAML Adaptive Model-Agnostic Meta-Learning.pdf}
}

@misc{behl2019a,
  title = {Alpha {{MAML}}: {{Adaptive Model-Agnostic Meta-Learning}}},
  shorttitle = {Alpha {{MAML}}},
  author = {Behl, Harkirat Singh and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Torr, Philip H. S.},
  year = {2019},
  month = may,
  number = {arXiv:1905.07435},
  eprint = {1905.07435},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/ILZADZMS/Behl et al. - 2019 - Alpha MAML Adaptive Model-Agnostic Meta-Learning.pdf}
}

@misc{behl2019b,
  title = {Alpha {{MAML}}: {{Adaptive Model-Agnostic Meta-Learning}}},
  shorttitle = {Alpha {{MAML}}},
  author = {Behl, Harkirat Singh and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Torr, Philip H. S.},
  year = {2019},
  month = may,
  number = {arXiv:1905.07435},
  eprint = {1905.07435},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{bengio2012,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  author = {Bengio, Yoshua},
  year = {2012},
  month = sep,
  journal = {arXiv:1206.5533 [cs]},
  eprint = {1206.5533},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyperparameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on backpropagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/DEEJZWQL/Bengio - 2012 - Practical recommendations for gradient-based train.pdf}
}

@article{bernacchia2021,
  title = {{{META-LEARNING WITH NEGATIVE LEARNING RATES}}},
  author = {Bernacchia, Alberto},
  year = {2021},
  pages = {23},
  abstract = {Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or learning to learn a distribution of tasks, where learning is represented by an outer loop, and to learn by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative values. These results help clarify under what circumstances meta-learning performs best.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/2UMY7FLT/Bernacchia - 2021 - META-LEARNING WITH NEGATIVE LEARNING RATES.pdf}
}

@article{bertinetto2019,
  title = {Meta-Learning with Differentiable Closed-Form Solvers},
  author = {Bertinetto, Luca and Henriques, Jo{\~a}o F. and Torr, Philip H. S. and Vedaldi, Andrea},
  year = {2019},
  month = jul,
  journal = {arXiv:1805.08136 [cs, stat]},
  eprint = {1805.08136},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures. Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/FI9TKN3T/Bertinetto et al. - 2019 - Meta-learning with differentiable closed-form solv.pdf}
}

@article{bessa2017,
  title = {A Framework for Data-Driven Analysis of Materials under Uncertainty: {{Countering}} the Curse of Dimensionality},
  shorttitle = {A Framework for Data-Driven Analysis of Materials under Uncertainty},
  author = {Bessa, M.A. and Bostanabad, R. and Liu, Z. and Hu, A. and Apley, Daniel W. and Brinson, C. and Chen, W. and Liu, Wing~Kam},
  year = {2017},
  month = jun,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {320},
  pages = {633--667},
  issn = {00457825},
  doi = {10.1016/j.cma.2017.03.037},
  abstract = {A new data-driven computational framework is developed to assist in the design and modeling of new material systems and structures. The proposed framework integrates three general steps: (1) design of experiments, where the input variables describing material geometry (microstructure), phase properties and external conditions are sampled; (2) efficient computational analyses of each design sample, leading to the creation of a material response database; and (3) machine learning applied to this database to obtain a new design or response model.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/EUE4H6V7/Bessa et al. - 2017 - A framework for data-driven analysis of materials .pdf}
}

@article{bessa2019,
  title = {Bayesian {{Machine Learning}} in {{Metamaterial Design}}: {{Fragile Becomes Supercompressible}}},
  shorttitle = {Bayesian {{Machine Learning}} in {{Metamaterial Design}}},
  author = {Bessa, Miguel A. and Glowacki, Piotr and Houlder, Michael},
  year = {2019},
  month = nov,
  journal = {Advanced Materials},
  volume = {31},
  number = {48},
  pages = {1904845},
  issn = {0935-9648, 1521-4095},
  doi = {10.1002/adma.201904845},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/GEAX733C/Bessa et al. - 2019 - Bayesian Machine Learning in Metamaterial Design .pdf}
}

@article{bhouri2021,
  title = {Gaussian Processes Meet {{NeuralODEs}}: {{A Bayesian}} Framework for Learning the Dynamics of Partially Observed Systems from Scarce and Noisy Data},
  shorttitle = {Gaussian Processes Meet {{NeuralODEs}}},
  author = {Bhouri, Mohamed Aziz and Perdikaris, Paris},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.03385 [physics, stat]},
  eprint = {2103.03385},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {This paper presents a machine learning framework (GP-NODE) for Bayesian systems identification from partial, noisy and irregular observations of nonlinear dynamical systems. The proposed method takes advantage of recent developments in differentiable programming to propagate gradient information through ordinary differential equation solvers and perform Bayesian inference with respect to unknown model parameters using Hamiltonian Monte Carlo sampling and Gaussian Process priors over the observed system states. This allows us to exploit temporal correlations in the observed data, and efficiently infer posterior distributions over plausible models with quantified uncertainty. Moreover, the use of sparsity-promoting priors such as the Finnish Horseshoe for free model parameters enables the discovery of interpretable and parsimonious representations for the underlying latent dynamics. A series of numerical studies is presented to demonstrate the effectiveness of the proposed GP-NODE method including predatorprey systems, systems biology, and a 50-dimensional human motion dynamical system. Taken together, our findings put forth a novel, flexible and robust workflow for data-driven model discovery under uncertainty. All code and data accompanying this manuscript are available online at https://github.com/PredictiveIntelligenceLab/GP-NODEs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/B9S9D5XM/Bhouri and Perdikaris - 2021 - Gaussian processes meet NeuralODEs A Bayesian fra.pdf}
}

@article{bishop,
  title = {Neural {{Networks}} for {{Pattern Recognition}}},
  author = {Bishop, Christopher M},
  pages = {498},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/U422ZZ9Y/Bishop - Neural Networks for Pattern Recognition.pdf}
}

@article{bishop1993,
  title = {Analysis of Multiphase Flows Using Dual-Energy Gamma Densitometry and Neural Networks},
  author = {Bishop, C.M. and James, G.D.},
  year = {1993},
  month = apr,
  journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  volume = {327},
  number = {2-3},
  pages = {580--593},
  issn = {01689002},
  doi = {10.1016/0168-9002(93)90728-Z},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/XZRI3V2J/Bishop and James - 1993 - Analysis of multiphase flows using dual-energy gam.pdf}
}

@book{bishop2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/home/taylanot/Zotero/storage/768VKD7B/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{bock1988,
  title = {A Perspective on Artificial Intelligence: {{Learning}} to Learn},
  shorttitle = {A Perspective on Artificial Intelligence},
  author = {Bock, Peter},
  year = {1988},
  month = dec,
  journal = {Annals of Operations Research},
  volume = {16},
  number = {1},
  pages = {33--52},
  issn = {0254-5330, 1572-9338},
  doi = {10.1007/BF02283734},
  abstract = {The classical approach to the acquisition of knowledge in artificial intelligence has been to program the intelligence into the machine in the form of specific rules for the application of the knowledge: expert systems. Unfortunately, the amount of time and resources required to program an expert system with sufficient knowledge for non-trivial problem-solving is prohibitively large. An alternative approach is to allow the machine to learn the rules based upon trial-and-error interaction with the environment, much as humans do. This will require endowing the machine with a sophisticated set of sensors for the perception of the external world, the ability to generate trial actions based upon this perceived information, and a dynamic evaluation policy to allow it to measure the effectiveness of its trial actions and modify its repertoire accordingly. The principles underlying this paradigm, known as collective learning systems theory, have already been applied to sophisticated gaming problems, demonstrating robust learning and dynamic adaptivity. The fundamental building block of a collective learning system is the learning cell, which may be embedded in a massively parallel, hierarchical data communications network. Such a network comprising 100 million learning cells will approach the intelligence capacity of the human cortex. In the not-too-distant future, it may be possible to build a race of robotic slaves to perform a wide variety of tasks in our culture. This goal, while irresistibly attractive, is most certainly fraught with severe social, political, moral, and economic difficulties.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/BS6KZ4YW/Bock - 1988 - A perspective on artificial intelligence Learning.pdf}
}

@article{bock2019,
  title = {A {{Review}} of the {{Application}} of {{Machine Learning}} and {{Data Mining Approaches}} in {{Continuum Materials Mechanics}}},
  author = {Bock, Frederic E. and Aydin, Roland C. and Cyron, Christian J. and Huber, Norbert and Kalidindi, Surya R. and Klusemann, Benjamin},
  year = {2019},
  month = may,
  journal = {Frontiers in Materials},
  volume = {6},
  pages = {110},
  issn = {2296-8016},
  doi = {10.3389/fmats.2019.00110},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/HEQ9HHJT/Bock et al. - 2019 - A Review of the Application of Machine Learning an.pdf}
}

@book{boyd2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-83378-3},
  langid = {english},
  lccn = {QA402.5 .B69 2004},
  keywords = {Convex functions,Mathematical optimization},
  file = {/home/taylanot/Zotero/storage/ETMATNF7/Boyd and Vandenberghe - 2004 - Convex optimization.pdf}
}

@article{bozinovski2020,
  title = {Reminder of the {{First Paper}} on {{Transfer Learning}} in {{Neural Networks}}, 1976},
  author = {Bozinovski, Stevo},
  year = {2020},
  month = sep,
  journal = {Informatica},
  volume = {44},
  number = {3},
  issn = {1854-3871, 0350-5596},
  doi = {10.31449/inf.v44i3.2828},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/YJQLWX3P/Bozinovski - 2020 - Reminder of the First Paper on Transfer Learning i.pdf}
}

@article{brunton2020,
  title = {Machine {{Learning}} for {{Fluid Mechanics}}},
  author = {Brunton, Steven L. and Noack, Bernd R. and Koumoutsakos, Petros},
  year = {2020},
  month = jan,
  journal = {Annual Review of Fluid Mechanics},
  volume = {52},
  number = {1},
  pages = {477--508},
  issn = {0066-4189, 1545-4479},
  doi = {10.1146/annurev-fluid-010719-060214},
  abstract = {The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful informationprocessing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/7DMPNI6C/Brunton et al. - 2020 - Machine Learning for Fluid Mechanics.pdf}
}

@article{burnham1999,
  title = {Latent Variable Multivariate Regression Modeling},
  author = {Burnham, Alison J. and MacGregor, John F. and Viveros, Rom{\'a}n},
  year = {1999},
  month = aug,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {48},
  number = {2},
  pages = {167--180},
  issn = {01697439},
  doi = {10.1016/S0169-7439(99)00018-0},
  abstract = {The latent variable multivariate regression \v{Z}LVMR. model is made up of two sets of variables, X and Y, both of which contain a latent variable structure plus random error. The wide applicability of this model is illustrated in this paper with several real examples. The chemometrics community has developed several empirical methods to estimate the latent structure in this model, including partial least squares regression \v{Z}PLS. and principal components regression \v{Z}PCR.. However, the majority of the statistical work in this area relies on the standard or reduced rank regression models, thus ignoring the latent variable nature of the X data. Considering methods like PLS and PCR in the context of these models has led to some misleading conclusions. This paper reaffirms the claim made frequently in the chemometrics literature that the reason PLS and PCR have been successful is that they take into account the latent variable structure in the data. It is also shown through several examples that the LVMR model provides the means to model more effectively many datasets in applied science resulting in improved techniques for process monitoring, experimental design and prediction. The focus in this paper is on the general model rather than on parameter estimation methods. q 1999 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/AJMXIHLV/Burnham et al. - 1999 - Latent variable multivariate regression modeling.pdf}
}

@article{cai2020,
  title = {Weighted {{Meta-Learning}}},
  author = {Cai, Diana and Sheth, Rishit and Mackey, Lester and Fusi, Nicolo},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.09465 [cs, stat]},
  eprint = {2003.09465},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Meta-learning leverages related source tasks to learn an initialization that can be quickly fine-tuned to a target task with limited labeled examples. However, many popular meta-learning algorithms, such as model-agnostic meta-learning (MAML), only assume access to the target samples for fine-tuning. In this work, we provide a general framework for meta-learning based on weighting the loss of different source tasks, where the weights are allowed to depend on the target samples. In this general setting, we provide upper bounds on the distance of the weighted empirical risk of the source tasks and expected target risk in terms of an integral probability metric (IPM) and Rademacher complexity, which apply to a number of meta-learning settings including MAML and a weighted MAML variant. We then develop a learning algorithm based on minimizing the error bound with respect to an empirical IPM, including a weighted MAML algorithm, {$\alpha$}-MAML. Finally, we demonstrate empirically on several regression problems that our weighted metalearning algorithm is able to find better initializations than uniformly-weighted meta-learning algorithms, such as MAML.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/SS8QRCQV/Cai et al. - 2020 - Weighted Meta-Learning.pdf}
}

@article{cai2021,
  title = {Physics-{{Informed Neural Networks}} for {{Heat Transfer Problems}}},
  author = {Cai, Shengze and Wang, Zhicheng and Wang, Sifan and Perdikaris, Paris and Karniadakis, George Em},
  year = {2021},
  month = jun,
  journal = {Journal of Heat Transfer},
  volume = {143},
  number = {6},
  pages = {060801},
  issn = {0022-1481, 1528-8943},
  doi = {10.1115/1.4050542},
  abstract = {Abstract             Physics-informed neural networks (PINNs) have gained popularity across different engineering fields due to their effectiveness in solving realistic problems with noisy data and often partially missing physics. In PINNs, automatic differentiation is leveraged to evaluate differential operators without discretization errors, and a multitask learning problem is defined in order to simultaneously fit observed data while respecting the underlying governing laws of physics. Here, we present applications of PINNs to various prototype heat transfer problems, targeting in particular realistic conditions not readily tackled with traditional computational methods. To this end, we first consider forced and mixed convection with unknown thermal boundary conditions on the heated surfaces and aim to obtain the temperature and velocity fields everywhere in the domain, including the boundaries, given some sparse temperature measurements. We also consider the prototype Stefan problem for two-phase flow, aiming to infer the moving interface, the velocity and temperature fields everywhere as well as the different conductivities of a solid and a liquid phase, given a few temperature measurements inside the domain. Finally, we present some realistic industrial applications related to power electronics to highlight the practicality of PINNs as well as the effective use of neural networks in solving general heat transfer problems of industrial complexity. Taken together, the results presented herein demonstrate that PINNs not only can solve ill-posed problems, which are beyond the reach of traditional computational methods, but they can also bridge the gap between computational and experimental heat transfer.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/Y2IYTBVY/Cai et al. - 2021 - Physics-Informed Neural Networks for Heat Transfer.pdf}
}

@article{carroll1990,
  title = {Finite Strain Solutions for a Compressible Elastic Solid},
  author = {Carroll, M. M. and Horgan, C. O.},
  year = {1990},
  journal = {Quarterly of Applied Mathematics},
  volume = {48},
  number = {4},
  pages = {767--780},
  issn = {0033-569X, 1552-4485},
  doi = {10.1090/qam/1079919},
  abstract = {Several closed form finite strain equilibrium solutions are presented for a special compressible isotropic elastic material which was proposed as a model for foam rubber by Blatz and Ko. These solutions include bending of a cylindrical sector into another sector or a rectangular block, bending of a block into a sector, expansion, compaction or eversion of cylinders or spheres, and torsion and extension of circular cylinders or tubes.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/37VXXWBE/Carroll and Horgan - 1990 - Finite strain solutions for a compressible elastic.pdf}
}

@article{caruana,
  title = {Learning {{Many Related Tasks}} at the {{Same Time}} with {{Backpropagation}}},
  author = {Caruana, Rich},
  pages = {8},
  abstract = {Hinton [6] proposed that generalization in artificial neural nets should improve if nets learn to represent the domain's underlying regularities . Abu-Mustafa's hints work [1] shows that the outputs of a backprop net can be used as inputs through which domainspecific information can be given to the net . We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multitask backprop generalizes better in real domains.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/E9X9CI4C/Caruana - Learning Many Related Tasks at the Same Time with .pdf}
}

@article{cella,
  title = {Meta-Learning with {{Stochastic Linear Bandits}}},
  author = {Cella, Leonardo and Lazaric, Alessandro and Pontil, Massimiliano},
  pages = {11},
  abstract = {We investigate meta-learning procedures in the setting of stochastic linear bandits tasks. The goal is to select a learning algorithm which works well on average over a class of bandits tasks, that are sampled from a task-distribution. Inspired by recent work on learning-to-learn linear regression, we consider a class of bandit algorithms that implement a regularized version of the wellknown OFUL algorithm, where the regularization is a square euclidean distance to a bias vector. We first study the benefit of the biased OFUL algorithm in terms of regret minimization. We then propose two strategies to estimate the bias within the learning-to-learn setting. We show both theoretically and experimentally, that when the number of tasks grows and the variance of the task-distribution is small, our strategies have a significant advantage over learning the tasks in isolation.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/QEMQH6MA/Cella et al. - Meta-learning with Stochastic Linear Bandits.pdf}
}

@article{chattopadhyay,
  title = {Multi-{{Source Domain Adaptation}} and {{Its Application}} to {{Early Detection}} of {{Fatigue}}},
  author = {Chattopadhyay, Rita and Sun, Qian and Ye, Jieping and Panchanathan, Sethuraman and Fan, Wei and Davidson, Ian},
  journal = {ACM Transactions on Knowledge Discovery from Data},
  pages = {24},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/27DJCX3W/Chattopadhyay et al. - Multi-Source Domain Adaptation and Its Application.pdf}
}

@article{chaturantabut2010,
  title = {Nonlinear {{Model Reduction}} via {{Discrete Empirical Interpolation}}},
  author = {Chaturantabut, Saifon and Sorensen, Danny C.},
  year = {2010},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {32},
  number = {5},
  pages = {2737--2764},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/090766498},
  abstract = {A dimension reduction method called discrete empirical interpolation is proposed and shown to dramatically reduce the computational complexity of the popular proper orthogonal decomposition (POD) method for constructing reduced-order models for time dependent and/or parametrized nonlinear partial differential equations (PDEs). In the presence of a general nonlinearity, the standard POD-Galerkin technique reduces dimension in the sense that far fewer variables are present, but the complexity of evaluating the nonlinear term remains that of the original problem. The original empirical interpolation method (EIM) is a modification of POD that reduces the complexity of evaluating the nonlinear term of the reduced model to a cost proportional to the number of reduced variables obtained by POD. We propose a discrete empirical interpolation method (DEIM), a variant that is suitable for reducing the dimension of systems of ordinary differential equations (ODEs) of a certain type. As presented here, it is applicable to ODEs arising from finite difference discretization of time dependent PDEs and/or parametrically dependent steady state problems. However, the approach extends to arbitrary systems of nonlinear ODEs with minor modification. Our contribution is a greatly simplified description of the EIM in a finite-dimensional setting that possesses an error bound on the quality of approximation. An application of DEIM to a finite difference discretization of the one-dimensional FitzHugh\textendash Nagumo equations is shown to reduce the dimension from 1024 to order 5 variables with negligible error over a long-time integration that fully captures nonlinear limit cycle behavior. We also demonstrate applicability in higher spatial dimensions with similar state space dimension reduction and accuracy results.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/T3IPHXJR/Chaturantabut and Sorensen - 2010 - Nonlinear Model Reduction via Discrete Empirical I.pdf}
}

@article{chau2018,
  title = {Numerical Investigation of an Artificial Neural Network Based Identification of Nonlinear Material Parameters},
  author = {Chau, Vu Minh},
  year = {2018},
  publisher = {{Unpublished}},
  doi = {10.13140/RG.2.2.24479.71840},
  abstract = {The hyperelastic materials are classified into two catalogues namely phenomenological and micro-mechanical. The former model's strain-energy function, which has the form of a polynomial function, is frequently formulated based on strain invariants. This contribution uses two machine learning (ML) approaches such as linear regression (LR) and Artificial Neural Network (ANN) in order to identify the materials parameters in the incompressible isotropic elastic strain-energy function. By applying a curve fitting procedure on Treloar's experimental dataset (Treloar, 1976), the optimal materials parameters and their corresponding relative error can be determined by minimizing the least square error between the predicted curve and experimental data. Resultantly, the identified material parameters from both methods are nearly the same and fit well with the experimental data. Generalized Rivlin Model (Rivlin and Saunders, 1951) shows the ability to well capture the large strain of Treloar's rubber-like materials data, and multiple sets of optimal parameters occurred during the fitting process. Furthermore, the relative errors computed using ML methods are better than that of literature around 66,667\% of test cases. For this small dataset, LR shows its advantage over ANN in terms of computational resources, however, significant superiority of ANN will exhibit when dealing with large-scale datasets.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/G7AAXS5Y/Chau - 2018 - Numerical investigation of an artificial neural ne.pdf}
}

@book{chaves2013,
  title = {Notes on {{Continuum Mechanics}}},
  author = {Chaves, Eduardo W. V.},
  editor = {O{\~n}ate, Eugenio},
  year = {2013},
  series = {Lecture {{Notes}} on {{Numerical Methods}} in {{Engineering}} and {{Sciences}}},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-007-5986-2},
  isbn = {978-94-007-5985-5 978-94-007-5986-2},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/3MWZL67B/Chaves - 2013 - Notes on Continuum Mechanics.pdf}
}

@inbook{chaves2013a,
  title = {Plasticity},
  booktitle = {Notes on {{Continuum Mechanics}}},
  author = {Chaves, Eduardo W. V.},
  year = {2013},
  pages = {465--546},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-007-5986-2_10},
  collaborator = {Chaves, Eduardo W. V.},
  isbn = {978-94-007-5985-5 978-94-007-5986-2},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/FVBN6QLT/Chaves - 2013 - Plasticity.pdf}
}

@article{chen,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  pages = {13},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/W7MRW566/Chen et al. - Neural Ordinary Differential Equations.pdf}
}

@book{chen1988,
  title = {Plasticity for {{Structural Engineers}}},
  author = {Chen, W. F. and Han, D. J.},
  year = {1988},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-3864-5},
  isbn = {978-1-4612-8380-5 978-1-4612-3864-5},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/ILHBDA2G/Chen and Han - 1988 - Plasticity for Structural Engineers.pdf}
}

@article{chen1995,
  title = {Universal Approximation to Nonlinear Operators by Neural Networks with Arbitrary Activation Functions and Its Application to Dynamical Systems},
  author = {Chen, Tianping and Chen, Hong},
  year = {1995},
  month = jul,
  journal = {IEEE Transactions on Neural Networks},
  volume = {6},
  number = {4},
  pages = {911--917},
  issn = {1941-0093},
  doi = {10.1109/72.392253},
  abstract = {The purpose of this paper is to investigate neural network capability systematically. The main results are: 1) every Tauber-Wiener function is qualified as an activation function in the hidden layer of a three-layered neural network; 2) for a continuous function in S'(R/sup 1/) to be a Tauber-Wiener function, the necessary and sufficient condition is that it is not a polynomial; 3) the capability of approximating nonlinear functionals defined on some compact set of a Banach space and nonlinear operators has been shown; and 4) the possibility by neural computation to approximate the output as a whole (not at a fixed point) of a dynamical system, thus identifying the system.{$<>$}},
  keywords = {Computer networks,H infinity control,Integral equations,Kernel,Mathematics,Neural networks,Nonlinear dynamical systems,Polynomials,Sufficient conditions,Sun},
  file = {/home/taylanot/Zotero/storage/GQM6C26Y/Chen and Chen - 1995 - Universal approximation to nonlinear operators by .pdf;/home/taylanot/Zotero/storage/ENWG9QEF/392253.html}
}

@article{chen2018,
  title = {Lifelong {{Machine Learning}}, {{Second Edition}}},
  author = {Chen, Zhiyuan and Liu, Bing},
  year = {2018},
  month = aug,
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume = {12},
  number = {3},
  pages = {1--207},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00832ED1V01Y201802AIM037},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/8CA8JNZ6/Chen and Liu - 2018 - Lifelong Machine Learning, Second Edition.pdf}
}

@article{chen2021,
  title = {Generalization {{Bounds For Meta-Learning}}: {{An Information-Theoretic Analysis}}},
  shorttitle = {Generalization {{Bounds For Meta-Learning}}},
  author = {Chen, Qi and Shui, Changjian and Marchand, Mario},
  year = {2021},
  month = dec,
  journal = {arXiv:2109.14595 [cs, stat]},
  eprint = {2109.14595},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We derive a novel information-theoretic analysis of the generalization property of meta-learning algorithms. Concretely, our analysis proposes a generic understanding of both the conventional learning-to-learn framework [1] and the modern model-agnostic meta learning (MAML) algorithms [2]. Moreover, we provide a data-dependent generalization bound for a stochastic variant of MAML, which is non-vacuous for deep few-shot learning. As compared to previous bounds that depend on the square norm of gradients, empirical validations on both simulated data and a well-known few-shot benchmark show that our bound is orders of magnitude tighter in most situations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/IU9LGP8J/Chen et al. - 2021 - Generalization Bounds For Meta-Learning An Inform.pdf}
}

@inproceedings{chen2021a,
  title = {Multi-{{Initialization Meta-Learning}} with {{Domain Adaptation}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Zhengyu and Wang, Donglin},
  year = {2021},
  month = jun,
  pages = {1390--1394},
  publisher = {{IEEE}},
  address = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9414554},
  abstract = {Recently, meta learning providing multiple initializations has drawn much attention due to its capability of handling multi-modal tasks drawn from diverse distributions. However, because of the difference of class distribution between meta-training and meta-test domain, the domain shift occurs in multi-modal meta-learning setting. To improve the performance on multi-modal tasks, we propose multi-initialization meta-learning with domain adaptation (MIML-DA) to tackle such domain shift. MIML-DA consists of a modulation network and a novel meta separation network (MSN), where the modulation network is to encode tasks into common and private modulation vectors, and then MSN uses these vectors separately to update the cross-domain meta-learner via a double-gradient descent process. In addition, the regularization using inequality measure is considered to improve the generalization ability of the meta-learner. Extensive experiments demonstrate the effectiveness of our MIML-DA method to new multi-modal tasks.},
  isbn = {978-1-72817-605-5},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/JJRPM2B3/Chen and Wang - 2021 - Multi-Initialization Meta-Learning with Domain Ada.pdf}
}

@article{chevreuil2012,
  title = {Model Order Reduction Based on Proper Generalized Decomposition for the Propagation of Uncertainties in Structural Dynamics: {{PGD FOR UNCERTAINTY PROPAGATION IN STRUCTURAL DYNAMICS}}},
  shorttitle = {Model Order Reduction Based on Proper Generalized Decomposition for the Propagation of Uncertainties in Structural Dynamics},
  author = {Chevreuil, Mathilde and Nouy, Anthony},
  year = {2012},
  month = jan,
  journal = {International Journal for Numerical Methods in Engineering},
  volume = {89},
  number = {2},
  pages = {241--268},
  issn = {00295981},
  doi = {10.1002/nme.3249},
  abstract = {A priori model reduction methods based on separated representations are introduced for the prediction of the low frequency response of uncertain structures within a parametric stochastic framework. The proper generalized decomposition method is used to construct a quasi-optimal separated representation of the random solution at some frequency samples. At each frequency, an accurate representation of the solution is obtained on reduced bases of spatial functions and stochastic functions. An extraction of the deterministic bases allows for the generation of a global reduced basis yielding a reduced order model of the uncertain structure, which appears to be accurate on the whole frequency band under study and for all values of input random parameters. This strategy can be seen as an alternative to traditional constructions of reduced order models in structural dynamics in the presence of parametric uncertainties. This reduced order model can then be used for further analyses such as the computation of the response at unresolved frequencies or the computation of more accurate stochastic approximations at some frequencies of interest. Because the dynamic response is highly nonlinear with respect to the input random parameters, a second level of separation of variables is introduced for the representation of functions of multiple random parameters, thus allowing the introduction of very fine approximations in each parametric dimension even when dealing with high parametric dimension. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/WD5RHFBW/Chevreuil and Nouy - 2012 - Model order reduction based on proper generalized .pdf}
}

@book{chopra2012,
  title = {Dynamics of Structures: Theory and Applications to Earthquake Engineering},
  shorttitle = {Dynamics of Structures},
  author = {Chopra, Anil K.},
  year = {2012},
  edition = {4th ed},
  publisher = {{Prentice Hall}},
  address = {{Upper Saddle River, N.J}},
  isbn = {978-0-13-285803-8},
  langid = {english},
  lccn = {TA654.6 .C466 2012},
  keywords = {Earthquake engineering,Structural dynamics},
  file = {/home/taylanot/Zotero/storage/ZBMM536D/Chopra - 2012 - Dynamics of structures theory and applications to.pdf}
}

@article{christmann2007,
  title = {Consistency and Robustness of Kernel-Based Regression in Convex Risk Minimization},
  author = {Christmann, Andreas and Steinwart, Ingo},
  year = {2007},
  month = aug,
  journal = {Bernoulli},
  volume = {13},
  number = {3},
  eprint = {0709.0626},
  eprinttype = {arxiv},
  issn = {1350-7265},
  doi = {10.3150/07-BEJ5102},
  abstract = {We investigate statistical properties for a broad class of modern kernel-based regression (KBR) methods. These kernel methods were developed during the last decade and are inspired by convex risk minimization in infinite-dimensional Hilbert spaces. One leading example is support vector regression. We first describe the relationship between the loss function \$L\$ of the KBR method and the tail of the response variable. We then establish the \$L\$-risk consistency for KBR which gives the mathematical justification for the statement that these methods are able to ``learn''. Then we consider robustness properties of such kernel methods. In particular, our results allow us to choose the loss function and the kernel to obtain computationally tractable and consistent KBR methods that have bounded influence functions. Furthermore, bounds for the bias and for the sensitivity curve, which is a finite sample version of the influence function, are developed, and the relationship between KBR and classical \$M\$ estimators is discussed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/taylanot/Zotero/storage/SDRV6L2X/Christmann and Steinwart - 2007 - Consistency and robustness of kernel-based regress.pdf}
}

@article{cohen2021,
  title = {Gradient {{Descent}} on {{Neural Networks Typically Occurs}} at the {{Edge}} of {{Stability}}},
  author = {Cohen, Jeremy M. and Kaur, Simran and Li, Yuanzhi and Kolter, J. Zico and Talwalkar, Ameet},
  year = {2021},
  month = jun,
  journal = {arXiv:2103.00065 [cs, stat]},
  eprint = {2103.00065},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value 2/(step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/8CWME5DS/Cohen et al. - 2021 - Gradient Descent on Neural Networks Typically Occu.pdf}
}

@article{collins2020,
  title = {Task-{{Robust Model-Agnostic Meta-Learning}}},
  author = {Collins, Liam and Mokhtari, Aryan and Shakkottai, Sanjay},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.04766 [cs, math, stat]},
  eprint = {2002.04766},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Meta-learning methods have shown an impressive ability to train models that rapidly learn new tasks. However, these methods only aim to perform well in expectation over tasks coming from some particular distribution that is typically equivalent across meta-training and meta-testing, rather than considering worst-case task performance. In this work we introduce the notion of ``task-robustness'' by reformulating the popular Model-Agnostic Meta-Learning (MAML) objective [Finn et al., 2017] such that the goal is to minimize the maximum loss over the observed meta-training tasks. The solution to this novel formulation is task-robust in the sense that it places equal importance on even the most difficult and/or rare tasks. This also means that it performs well over all distributions of the observed tasks, making it robust to shifts in the task distribution between meta-training and meta-testing. We present an algorithm to solve the proposed min-max problem, and show that it converges to an -accurate point at the optimal rate of O(1/ 2) in the convex setting and to an ( , {$\delta$})-stationary point at the rate of O(max\{1/ 5, 1/{$\delta$}5\}) in nonconvex settings. We also provide an upper bound on the new task generalization error that captures the advantage of minimizing the worst-case task loss, and demonstrate this advantage in sinusoid regression and image classification experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/736WH3S3/Collins et al. - 2020 - Task-Robust Model-Agnostic Meta-Learning.pdf;/home/taylanot/Zotero/storage/LTM6AK2P/Collins et al. - Task-Robust Model-Agnostic Meta-Learning.pdf}
}

@article{collins2021,
  title = {How {{Does}} the {{Task Landscape Affect MAML Performance}}?},
  author = {Collins, Liam and Mokhtari, Aryan and Shakkottai, Sanjay},
  year = {2021},
  month = oct,
  journal = {arXiv:2010.14672 [cs, math, stat]},
  eprint = {2010.14672},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Model-Agnostic Meta-Learning (MAML) has become increasingly popular for training models that can quickly adapt to new tasks via one or few stochastic gradient descent steps. However, the MAML objective is significantly more difficult to optimize compared to standard non-adaptive learning (NAL), and little is understood about how much MAML improves over NAL in terms of the fast adaptability of their solutions in various scenarios. We analytically address this issue in a linear regression setting consisting of a mixture of easy and hard tasks, where hardness is related to the rate that gradient descent converges on the task. Specifically, we prove that in order for MAML to achieve substantial gain over NAL, (i) there must be some discrepancy in hardness among the tasks, and (ii) the optimal solutions of the hard tasks must be closely packed with the center far from the center of the easy tasks optimal solutions. We also give numerical and analytical results suggesting that these insights apply to two-layer neural networks. Finally, we provide few-shot image classification experiments that support our insights for when MAML should be used and emphasize the importance of training MAML on hard tasks in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/23TQQ7IC/Collins et al. - 2021 - How Does the Task Landscape Affect MAML Performanc.pdf;/home/taylanot/Zotero/storage/IJMWMTJK/Collins et al. - 2021 - How Does the Task Landscape Affect MAML Performanc.pdf}
}

@inproceedings{cotter1990,
  title = {Fixed-Weight Networks Can Learn},
  booktitle = {1990 {{IJCNN International Joint Conference}} on {{Neural Networks}}},
  author = {Cotter, N.E. and Conwell, P.R.},
  year = {1990},
  month = jun,
  pages = {553-559 vol.3},
  doi = {10.1109/IJCNN.1990.137898},
  abstract = {A theorem describing how fixed-weight recurrent neural networks can approximate adaptive-weight learning algorithms is proved. The theorem applies to most networks and learning algorithms currently in use. It is concluded from the theorem that a system which exhibits learning behavior may exhibit no synaptic weight modifications. This idea is demonstrated by transforming a backward error propagation network into a fixed-weight system},
  file = {/home/taylanot/Zotero/storage/JSGWT7KK/Cotter and Conwell - 1990 - Fixed-weight networks can learn.pdf;/home/taylanot/Zotero/storage/LL5FK9LA/5726856.html}
}

@article{dana2021,
  title = {A Machine Learning Accelerated {{FE}}\$\^2\$ Homogenization Algorithm for Elastic Solids},
  author = {Dana, Saumik and Wheeler, Mary F.},
  year = {2021},
  month = jul,
  journal = {arXiv:2003.11372 [cs]},
  eprint = {2003.11372},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The FE2 homogenization algorithm for multiscale modeling iterates between the macroscale and the microscale (represented by a representative volume element) till convergence is achieved at every increment of macroscale loading. The information exchange between the two scales occurs at the gauss points of the macroscale finite element discretization. The microscale problem is also solved using finite elements on-the-fly thus rendering the algorithm computationally expensive for complex microstructures. We invoke machine learning to establish the input-output causality of the RVE boundary value problem using a neural network framework. This renders the RVE as a blackbox which gets the information from the macroscale as an input and gives information back to the macroscale as output, thereby eliminating the need for on-the-fly finite element solves at the RVE level. This framework has the potential to significantly accelerate the FE2 algorithm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Engineering; Finance; and Science},
  file = {/home/taylanot/Zotero/storage/WEMQ4J47/Dana and Wheeler - 2021 - A machine learning accelerated FE$^2$ homogenizati.pdf}
}

@article{day2017,
  title = {A Survey on Heterogeneous Transfer Learning},
  author = {Day, Oscar and Khoshgoftaar, Taghi M.},
  year = {2017},
  month = dec,
  journal = {Journal of Big Data},
  volume = {4},
  number = {1},
  pages = {29},
  issn = {2196-1115},
  doi = {10.1186/s40537-017-0089-0},
  abstract = {Transfer learning has been demonstrated to be effective for many real-world applications as it exploits knowledge present in labeled training data from a source domain to enhance a model's performance in a target domain, which has little or no labeled target training data. Utilizing a labeled source, or auxiliary, domain for aiding a target task can greatly reduce the cost and effort of collecting sufficient training labels to create an effective model in the new target distribution. Currently, most transfer learning methods assume the source and target domains consist of the same feature spaces which greatly limits their applications. This is because it may be difficult to collect auxiliary labeled source domain data that shares the same feature space as the target domain. Recently, heterogeneous transfer learning methods have been developed to address such limitations. This, in effect, expands the application of transfer learning to many other real-world tasks such as cross-language text categorization, text-to-image classification, and many others. Heterogeneous transfer learning is characterized by the source and target domains having differing feature spaces, but may also be combined with other issues such as differing data distributions and label spaces. These can present significant challenges, as one must develop a method to bridge the feature spaces, data distributions, and other gaps which may be present in these cross-domain learning tasks. This paper contributes a comprehensive survey and analysis of current methods designed for performing heterogeneous transfer learning tasks to provide an updated, centralized outlook into current methodologies.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/TVTINNI5/Day and Khoshgoftaar - 2017 - A survey on heterogeneous transfer learning.pdf}
}

@incollection{decherchi2019,
  title = {Simple {{Learning}} with a {{Teacher}} via {{Biased Regularized Least Squares}}},
  booktitle = {Machine {{Learning}}, {{Optimization}}, and {{Data Science}}},
  author = {Decherchi, Sergio and Cavalli, Andrea},
  editor = {Nicosia, Giuseppe and Pardalos, Panos and Giuffrida, Giovanni and Umeton, Renato and Sciacca, Vincenzo},
  year = {2019},
  volume = {11331},
  pages = {14--25},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-13709-0_2},
  abstract = {In the paradigm of learning with a teacher, introduced by Vapnik, a supervised learner is trained on an augmented features space, and a student is requested to match the teacher accuracy as much as possible in a reduced feature space. In particular, in the transfer learning mode proposed by Vapnik, a method was formalized to move the knowledge from the teacher to the student. In this paper, we use biased regularized least squares as a simple yet effective method to transfer the knowledge from one learner to another, and to assess its accuracy. We achieve this by further generalizing a semi-supervised learning method, which we previously introduced. We will show that, with this approach, the teacher can be any classifier. In particular, we will employ the Relevance Vector Machine (RVM) as teacher to assess the method's capability in transfering the knowledge in terms of classification accuracy, and in reproducing the probabilities coming from RVM. We validate the method against standard UCI datasets and systematically compare it with Vapnik's original method in terms of accuracy and execution time. We thus demonstrate the feasibility and speed of this new approach.},
  isbn = {978-3-030-13708-3 978-3-030-13709-0},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/IX9A8GNC/Decherchi and Cavalli - 2019 - Simple Learning with a Teacher via Biased Regulari.pdf}
}

@article{decost2020,
  title = {Scientific {{AI}} in Materials Science: A Path to a Sustainable and Scalable Paradigm},
  shorttitle = {Scientific {{AI}} in Materials Science},
  author = {DeCost, Brian and {Hattrick-Simpers}, Jason and Trautt, Zachary and Kusne, Aaron and Campo, Eva and Green, Martin},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.08471 [cond-mat, physics:physics]},
  eprint = {2003.08471},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics},
  abstract = {Recently there has been an ever-increasing trend in the use of machine learning (ML) and artificial intelligence (AI) methods by the materials science, condensed matter physics, and chemistry communities. This perspective article identifies key scientific, technical, and social opportunities that the materials community must prioritize to consistently develop and leverage Scientific AI (SciAI) to provide a credible path towards the advancement of current materialslimited technologies. Here we highlight the intersections of these opportunities with a series of proposed paths forward. The opportunities are roughly sorted from scientific/technical (e.g. development of robust, physically meaningful multiscale material representations) to social (e.g. promoting an AI-ready workforce). The proposed paths forward range from developing new infrastructure and capabilities to deploying them in industry and academia. We provide a brief introduction to AI in materials science and engineering, followed by detailed discussions of each of the opportunities and paths forward.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Materials Science,Physics - Computational Physics},
  file = {/home/taylanot/Zotero/storage/HSX2H36M/DeCost et al. - 2020 - Scientific AI in materials science a path to a su.pdf}
}

@article{deisenroth,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  pages = {417},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/Z6BFD3FY/Deisenroth et al. - Mathematics for Machine Learning.pdf}
}

@article{denevi,
  title = {Learning {{To Learn Around A Common Mean}}},
  author = {Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano},
  pages = {11},
  abstract = {The problem of learning-to-learn (LTL) or meta-learning is gaining increasing attention due to recent empirical evidence of its effectiveness in applications. The goal addressed in LTL is to select an algorithm that works well on tasks sampled from a meta-distribution. In this work, we consider the family of algorithms given by a variant of Ridge Regression, in which the regularizer is the square distance to an unknown mean vector. We show that, in this setting, the LTL problem can be reformulated as a Least Squares (LS) problem and we exploit a novel metaalgorithm to efficiently solve it. At each iteration the meta-algorithm processes only one dataset. Specifically, it firstly estimates the stochastic LS objective function, by splitting this dataset into two subsets used to train and test the inner algorithm, respectively. Secondly, it performs a stochastic gradient step with the estimated value. Under specific assumptions, we present a bound for the generalization error of our meta-algorithm, which suggests the right splitting parameter to choose. When the hyper-parameters of the problem are fixed, this bound is consistent as the number of tasks grows, even if the sample size is kept constant. Preliminary experiments confirm our theoretical findings, highlighting the advantage of our approach, with respect to independent task learning.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MF6MBSTE/Denevi et al. - Learning To Learn Around A Common Mean.pdf}
}

@article{denevi2018,
  title = {Incremental {{Learning-to-Learn}} with {{Statistical Guarantees}}},
  author = {Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.08089 [cs, stat]},
  eprint = {1803.08089},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In learning-to-learn the goal is to infer a learning algorithm that works well on a class of tasks sampled from an unknown meta distribution. In contrast to previous work on batch learning-tolearn, we consider a scenario where tasks are presented sequentially and the algorithm needs to adapt incrementally to improve its performance on future tasks. Key to this setting is for the algorithm to rapidly incorporate new observations into the model as they arrive, without keeping them in memory. We focus on the case where the underlying algorithm is Ridge Regression parameterized by a positive semidefinite matrix. We propose to learn this matrix by applying a stochastic strategy to minimize the empirical error incurred by Ridge Regression on future tasks sampled from the meta distribution. We study the statistical properties of the proposed algorithm and prove non-asymptotic bounds on its excess transfer risk, that is, the generalization performance on new tasks from the same meta distribution. We compare our online learning-to-learn approach with a state of the art batch method, both theoretically and empirically.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/8HGTCZH2/Denevi et al. - 2018 - Incremental Learning-to-Learn with Statistical Gua.pdf}
}

@article{denevi2020,
  title = {The {{Advantage}} of {{Conditional Meta-Learning}} for {{Biased Regularization}} and {{Fine-Tuning}}},
  author = {Denevi, Giulia and Pontil, Massimiliano and Ciliberto, Carlo},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.10857 [cs, stat]},
  eprint = {2008.10857},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Biased regularization and fine-tuning are two recent meta-learning approaches. They have been shown to be effective to tackle distributions of tasks, in which the tasks' target vectors are all close to a common meta-parameter vector. However, these methods may perform poorly on heterogeneous environments of tasks, where the complexity of the tasks' distribution cannot be captured by a single meta-parameter vector. We address this limitation by conditional meta-learning, inferring a conditioning function mapping task's side information into a meta-parameter vector that is appropriate for that task at hand. We characterize properties of the environment under which the conditional approach brings a substantial advantage over standard meta-learning and we highlight examples of environments, such as those with multiple clusters, satisfying these properties. We then propose a convex meta-algorithm providing a comparable advantage also in practice. Numerical experiments confirm our theoretical findings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/CK933KTE/Denevi et al. - 2020 - The Advantage of Conditional Meta-Learning for Bia.pdf}
}

@article{denevi2021,
  title = {Conditional {{Meta-Learning}} of {{Linear Representations}}},
  author = {Denevi, Giulia and Pontil, Massimiliano and Ciliberto, Carlo},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.16277 [cs]},
  eprint = {2103.16277},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Standard meta-learning for representation learning aims to find a common representation to be shared across multiple tasks. The effectiveness of these methods is often limited when the nuances of the tasks' distribution cannot be captured by a single representation. In this work we overcome this issue by inferring a conditioning function, mapping the tasks' side information (such as the tasks' training dataset itself) into a representation tailored to the task at hand. We study environments in which our conditional strategy outperforms standard meta-learning, such as those in which tasks can be organized in separate clusters according to the representation they share. We then propose a meta-algorithm capable of leveraging this advantage in practice. In the unconditional setting, our method yields a new estimator enjoying faster learning rates and requiring less hyper-parameters to tune than current state-of-the-art methods. Our results are supported by preliminary experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/V9AL9CX8/Denevi et al. - 2021 - Conditional Meta-Learning of Linear Representation.pdf}
}

@article{denevia,
  title = {Learning-to-{{Learn Stochastic Gradient Descent}} with {{Biased Regularization}}},
  author = {Denevi, Giulia and Ciliberto, Carlo and Grazzi, Riccardo and Pontil, Massimiliano},
  pages = {10},
  abstract = {We study the problem of learning-to-learn: inferring a learning algorithm that works well on a family of tasks sampled from an unknown distribution. As class of algorithms we consider Stochastic Gradient Descent (SGD) on the true risk regularized by the square euclidean distance from a bias vector. We present an average excess risk bound for such a learning algorithm that quantifies the potential benefit of using a bias vector with respect to the unbiased case. We then propose a novel meta-algorithm to estimate the bias term online from a sequence of observed tasks. The small memory footprint and low time complexity of our approach makes it appealing in practice while our theoretical analysis provides guarantees on the generalization properties of the meta-algorithm on new tasks. A key feature of our results is that, when the number of tasks grows and their variance is relatively small, our learning-to-learn approach has a significant advantage over learning each task in isolation by standard SGD without a bias term. Numerical experiments demonstrate the effectiveness of our approach in practice.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/BP5B74IV/Denevi et al. - Learning-to-Learn Stochastic Gradient Descent with.pdf}
}

@article{denevib,
  title = {Online {{Parameter-Free Learning}} of {{Multiple Low Variance Tasks}}},
  author = {Denevi, Giulia and Stamos, Dimitris and Pontil, Massimiliano},
  pages = {10},
  abstract = {We propose a method to learn a common bias vector for a growing sequence of low-variance tasks. Unlike state-of-the-art approaches, our method does not require tuning any hyperparameter. Our approach is presented in the non-statistical setting and can be of two variants. The ``aggressive'' one updates the bias after each datapoint, the ``lazy'' one updates the bias only at the end of each task. We derive an across-tasks regret bound for the method. When compared to state-of-the-art approaches, the aggressive variant returns faster rates, the lazy one recovers standard rates, but with no need of tuning hyper-parameters. We then adapt the methods to the statistical setting: the aggressive variant becomes a multi-task learning method, the lazy one a meta-learning method. Experiments confirm the effectiveness of our methods in practice.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/UZ5EYQ5R/Denevi et al. - Online Parameter-Free Learning of Multiple Low Var.pdf}
}

@article{denevic,
  title = {Online-{{Within-Online Meta-Learning}}},
  author = {Denevi, Giulia and Stamos, Dimitris and Ciliberto, Carlo and Pontil, Massimiliano},
  pages = {11},
  abstract = {We study the problem of learning a series of tasks in a fully online Meta-Learning setting. The goal is to exploit similarities among the tasks to incrementally adapt an inner online algorithm in order to incur a low averaged cumulative error over the tasks. We focus on a family of inner algorithms based on a parametrized variant of online Mirror Descent. The inner algorithm is incrementally adapted by an online Mirror Descent meta-algorithm using the corresponding within-task minimum regularized empirical risk as the meta-loss. In order to keep the process fully online, we approximate the meta-subgradients by the online inner algorithm. An upper bound on the approximation error allows us to derive a cumulative error bound for the proposed method. Our analysis can also be converted to the statistical setting by online-to-batch arguments. We instantiate two examples of the framework in which the meta-parameter is either a common bias vector or feature map. Finally, preliminary numerical experiments confirm our theoretical findings.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/IYJ9ABIM/Denevi et al. - Online-Within-Online Meta-Learning.pdf}
}

@article{dimiduk2018,
  title = {Perspectives on the {{Impact}} of {{Machine Learning}}, {{Deep Learning}}, and {{Artificial Intelligence}} on {{Materials}}, {{Processes}}, and {{Structures Engineering}}},
  author = {Dimiduk, Dennis M. and Holm, Elizabeth A. and Niezgoda, Stephen R.},
  year = {2018},
  month = sep,
  journal = {Integrating Materials and Manufacturing Innovation},
  volume = {7},
  number = {3},
  pages = {157--172},
  issn = {2193-9764, 2193-9772},
  doi = {10.1007/s40192-018-0117-8},
  abstract = {The fields of machining learning and artificial intelligence are rapidly expanding, impacting nearly every technological aspect of society. Many thousands of published manuscripts report advances over the last 5 years or less. Yet materials and structures engineering practitioners are slow to engage with these advancements. Perhaps the recent advances that are driving other technical fields are not sufficiently distinguished from long-known informatics methods for materials, thereby masking their likely impact to the materials, processes, and structures engineering (MPSE). Alternatively, the diverse nature and limited availability of relevant materials data pose obstacles to machine-learning implementation. The glimpse captured in this overview is intended to draw focus to selected distinguishing advances, and to show that there are opportunities for these new technologies to have transformational impacts on MPSE. Further, there are opportunities for the MPSE fields to contribute understanding to the emerging machine-learning tools from a physics basis. We suggest that there is an immediate need to expand the use of these new tools throughout MPSE, and to begin the transformation of engineering education that is necessary for ongoing adoption of the methods.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/GP9FD9KE/Dimiduk et al. - 2018 - Perspectives on the Impact of Machine Learning, De.pdf}
}

@article{domingos2020,
  title = {Every {{Model Learned}} by {{Gradient Descent Is Approximately}} a {{Kernel Machine}}},
  author = {Domingos, Pedro},
  year = {2020},
  month = nov,
  journal = {arXiv:2012.00152 [cs, stat]},
  eprint = {2012.00152},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,I.5.1,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/IVI35TVV/Domingos - 2020 - Every Model Learned by Gradient Descent Is Approxi.pdf}
}

@article{drosopoulos2021,
  title = {Data-Driven {{Computational Homogenization Using Neural Networks}}: {{FE}} {\textsuperscript{2}} -{{NN Application}} on {{Damaged Masonry}}},
  shorttitle = {Data-Driven {{Computational Homogenization Using Neural Networks}}},
  author = {Drosopoulos, Georgios A. and Stavroulakis, Georgios E.},
  year = {2021},
  month = feb,
  journal = {Journal on Computing and Cultural Heritage},
  volume = {14},
  number = {1},
  pages = {1--19},
  issn = {1556-4673, 1556-4711},
  doi = {10.1145/3423154},
  abstract = {Fusion of data mining and computational mechanics is a modern approach for the exploitation of available data within rigorous modeling. First steps in this direction have been focused on the usage of neural networks and other soft computing tools as metamodeling tools. This framework seems suitable for numerical homogenization techniques realized within the so-called FE               2               environment, where the lower-level analysis of a detailed representative volume element is replaced by a prediction based on a previously prepared database. Numerically prepared data are used here, although the method can be used with experimental data as well. In this case, the need for a constitutive description of the fine scale is bypassed. Extraction of material properties from the database, required by the upper-level finite element analysis, is based on backpropagation artificial neural networks. The method is applicable to monuments and masonry structural systems. We investigate this approach here for the analysis of masonry structures with elastoplastic behavior. Results indicate a satisfactory comparison with published research.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/9249JIW5/Drosopoulos and Stavroulakis - 2021 - Data-driven Computational Homogenization Using Neu.pdf}
}

@article{du,
  title = {Hypothesis {{Transfer Learning}} via {{Transformation Functions}}},
  author = {Du, Simon S and Koushik, Jayanth and Singh, Aarti and Poczos, Barnabas},
  pages = {11},
  abstract = {We consider the Hypothesis Transfer Learning (HTL) problem where one incorporates a hypothesis trained on the source domain into the learning procedure of the target domain. Existing theoretical analysis either only studies specific algorithms or only presents upper bounds on the generalization error but not on the excess risk. In this paper, we propose a unified algorithm-dependent framework for HTL through a novel notion of transformation function, which characterizes the relation between the source and the target domains. We conduct a general risk analysis of this framework and in particular, we show for the first time, if two domains are related, HTL enjoys faster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge Regression than those of the classical non-transfer learning settings. Experiments on real world data demonstrate the effectiveness of our framework.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/36FB738B/Du et al. - Hypothesis Transfer Learning via Transformation Fu.pdf}
}

@inproceedings{evgeniou2004,
  title = {Regularized Multi--Task Learning},
  booktitle = {Proceedings of the 2004 {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining  - {{KDD}} '04},
  author = {Evgeniou, Theodoros and Pontil, Massimiliano},
  year = {2004},
  pages = {109},
  publisher = {{ACM Press}},
  address = {{Seattle, WA, USA}},
  doi = {10.1145/1014052.1014067},
  abstract = {Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi\textendash task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single\textendash task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task\textendash coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi\textendash task learning methods and largely outperforms single\textendash task learning using SVMs.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/GI2CYFX2/Evgeniou and Pontil - 2004 - Regularized multi--task learning.pdf}
}

@article{fallah2020,
  title = {On the {{Convergence Theory}} of {{Gradient-Based Model-Agnostic Meta-Learning Algorithms}}},
  author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
  year = {2020},
  month = may,
  journal = {arXiv:1908.10400 [cs, math, stat]},
  eprint = {1908.10400},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We study the convergence of a class of gradient-based Model-Agnostic Meta-Learning (MAML) methods and characterize their overall complexity as well as their best achievable accuracy in terms of gradient norm for nonconvex loss functions. We start with the MAML method and its first-order approximation (FO-MAML) and highlight the challenges that emerge in their analysis. By overcoming these challenges not only we provide the first theoretical guarantees for MAML and FO-MAML in nonconvex settings, but also we answer some of the unanswered questions for the implementation of these algorithms including how to choose their learning rate and the batch size for both tasks and datasets corresponding to tasks. In particular, we show that MAML can find an -first-order stationary point ( -FOSP) for any positive after at most O(1/ 2) iterations at the expense of requiring second-order information. We also show that FO-MAML which ignores the second-order information required in the update of MAML cannot achieve any small desired level of accuracy, i.e., FO-MAML cannot find an -FOSP for any {$>$} 0. We further propose a new variant of the MAML algorithm called Hessian-free MAML which preserves all theoretical guarantees of MAML, without requiring access to second-order information.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/QRNVPIBS/Fallah et al. - 2020 - On the Convergence Theory of Gradient-Based Model-.pdf}
}

@article{fallah2021,
  title = {Generalization of {{Model-Agnostic Meta-Learning Algorithms}}: {{Recurring}} and {{Unseen Tasks}}},
  shorttitle = {Generalization of {{Model-Agnostic Meta-Learning Algorithms}}},
  author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
  year = {2021},
  month = nov,
  journal = {arXiv:2102.03832 [cs, math, stat]},
  eprint = {2102.03832},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {In this paper, we study the generalization properties of Model-Agnostic Meta-Learning (MAML) algorithms for supervised learning problems. We focus on the setting in which we train the MAML model over m tasks, each with n data points, and characterize its generalization error from two points of view: First, we assume the new task at test time is one of the training tasks, and we show that, for strongly convex objective functions, the expected excess population loss is bounded by O(1/mn). Second, we consider the MAML algorithm's generalization to an unseen task and show that the resulting generalization error depends on the total variation distance between the underlying distributions of the new task and the tasks observed during the training process. Our proof techniques rely on the connections between algorithmic stability and generalization bounds of algorithms. In particular, we propose a new definition of stability for meta-learning algorithms, which allows us to capture the role of both the number of tasks m and number of samples per task n on the generalization error of MAML.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/NHU2GHKD/Fallah et al. - 2021 - Generalization of Model-Agnostic Meta-Learning Alg.pdf}
}

@article{ferreira2021,
  title = {Adaptive {{Clustering-based Reduced-Order Modeling Framework}}: {{Fast}} and Accurate Modeling of Localized History-Dependent Phenomena},
  shorttitle = {Adaptive {{Clustering-based Reduced-Order Modeling Framework}}},
  author = {Ferreira, Bernardo P. and Pires, F. M. Andrade and Bessa, Miguel A.},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.11897 [cond-mat]},
  eprint = {2109.11897},
  eprinttype = {arxiv},
  primaryclass = {cond-mat},
  abstract = {This paper proposes a novel Adaptive Clustering-based Reduced-Order Modeling (ACROM) framework to significantly improve and extend the recent family of clustering-based reduced-order models (CROMs). This adaptive framework enables the clustering-based domain decomposition to evolve dynamically throughout the problem solution, ensuring optimum refinement in regions where the relevant fields present steeper gradients. It offers a new route to fast and accurate material modeling of history-dependent nonlinear problems involving highly localized plasticity and damage phenomena. The overall approach is composed of three main building blocks: target clusters selection criterion, adaptive cluster analysis, and computation of cluster interaction tensors. In addition, an adaptive clustering solution rewinding procedure and a dynamic adaptivity split factor strategy are suggested to further enhance the adaptive process. The coined Adaptive Self-Consistent Clustering Analysis (ASCA) is shown to perform better than its static counterpart when capturing the multiscale elasto-plastic behavior of a particle-matrix composite and predicting the associated fracture and toughness. Given the encouraging results shown in this paper, the ACROM framework sets the stage and opens new avenues to explore adaptivity in the context of CROMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Materials Science,Mathematics - Numerical Analysis},
  file = {/home/taylanot/Zotero/storage/DUJGMBD3/Ferreira et al. - 2021 - Adaptive Clustering-based Reduced-Order Modeling F.pdf}
}

@article{ferreira2022,
  title = {Adaptivity for Clustering-Based Reduced-Order Modeling of Localized History-Dependent Phenomena},
  author = {Ferreira, Bernardo P. and Andrade Pires, F.M. and Bessa, M.A.},
  year = {2022},
  month = apr,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {393},
  pages = {114726},
  issn = {00457825},
  doi = {10.1016/j.cma.2022.114726},
  abstract = {This article introduces adaptivity in Clustering-based Reduced Order Models (ACROMs). The strategy is demonstrated for a particular CROM called Self-Consistent Clustering Analysis (SCA), extending it into the Adaptive Self-Consistent Clustering Analysis (ASCA) method. This is shown to improve predictions of Representative Volume Elements (RVEs) of materials exhibiting history-dependent localization phenomena such as plasticity, damage and fracture. The overall approach is composed of three main building blocks: target clusters selection criterion, adaptive cluster analysis, and computation of cluster interaction tensors. In addition, an adaptive clustering solution rewinding procedure and a dynamic adaptivity split factor strategy are suggested to further enhance the adaptive process. The ASCA method is shown to perform better than its static counterpart when capturing the multi-scale elasto-plastic behavior of a particle\textendash matrix composite and predicting the associated fracture and toughness. The proposed adaptivity strategy can be followed in other CROMs to extend them into ACROMs, opening new avenues to explore adaptivity in this context.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/ME77EJ9X/Ferreira et al. - 2022 - Adaptivity for clustering-based reduced-order mode.pdf}
}

@article{feyel2003,
  title = {A Multilevel Finite Element Method ({{FE2}}) to Describe the Response of Highly Non-Linear Structures Using Generalized Continua},
  author = {Feyel, Fr{\'e}d{\'e}ric},
  year = {2003},
  month = jul,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {192},
  number = {28-30},
  pages = {3233--3244},
  issn = {00457825},
  doi = {10.1016/S0045-7825(03)00348-7},
  abstract = {A general method called FE2 has been introduced which consists in describing the behavior of heterogeneous structures using a multiscale finite element model. Instead of trying to build differential systems to establish a stressstrain relation at the macroscale, a finite element computation of the representative volume element is carried out simultaneously. Doing so does not require any constitutive equations to be written at the macroscopic scale: all nonlinearities come directly from the microscale.},
  langid = {english}
}

@article{finn,
  title = {Learning to {{Learn}} with {{Gradients}}},
  author = {Finn, Chelsea},
  pages = {198},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/85DAU9YY/Finn - Learning to Learn with Gradients.pdf}
}

@article{finn2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  journal = {arXiv:1703.03400 [cs]},
  eprint = {1703.03400},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/taylanot/Zotero/storage/4NHI76JH/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf}
}

@article{finn2019,
  title = {Probabilistic {{Model-Agnostic Meta-Learning}}},
  author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  year = {2019},
  month = oct,
  journal = {arXiv:1806.02817 [cs, stat]},
  eprint = {1806.02817},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/RWL5L7AC/Finn et al. - 2019 - Probabilistic Model-Agnostic Meta-Learning.pdf}
}

@article{finna,
  title = {Online {{Meta-Learning}}},
  author = {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
  pages = {11},
  abstract = {A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the tasks are available together as a batch. In contrast, online (regret based) learning considers a setting where tasks are revealed one after the other, but conventionally trains a single model without task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T ) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different largescale problems suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/ZJCM8TS4/Finn et al. - Online Meta-Learning.pdf}
}

@article{flennerhag2019,
  title = {{{TRANSFERRING KNOWLEDGE ACROSS LEARNING PROCESSES}}},
  author = {Flennerhag, Sebastian and Moreno, Pablo G and Lawrence, Neil D and Damianou, Andreas},
  year = {2019},
  pages = {23},
  abstract = {In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at a higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/YKG58USK/Flennerhag et al. - 2019 - TRANSFERRING KNOWLEDGE ACROSS LEARNING PROCESSES.pdf}
}

@article{flennerhag2020,
  title = {Meta-{{Learning}} with {{Warped Gradient Descent}}},
  author = {Flennerhag, Sebastian and Rusu, Andrei A. and Pascanu, Razvan and Visin, Francesco and Yin, Hujun and Hadsell, Raia},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.00025 [cs, stat]},
  eprint = {1909.00025},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their metagradients, leading to methods that can not scale beyond few-shot task adaptation. In this work, we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution. Preconditioning arises by interleaving non-linear layers, referred to as warp-layers, between the layers of a task-learner. Warp-layers are meta-learned without backpropagating through the task training process in a manner similar to methods that learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can scale to arbitrarily large meta-learning problems. We provide a geometrical interpretation of the approach and evaluate its effectiveness in a variety of settings, including few-shot, standard supervised, continual and reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/P3IMUNJN/Flennerhag et al. - 2020 - Meta-Learning with Warped Gradient Descent.pdf}
}

@article{fradkov2020,
  title = {Early {{History}} of {{Machine Learning}}},
  author = {Fradkov, Alexander L.},
  year = {2020},
  journal = {IFAC-PapersOnLine},
  volume = {53},
  number = {2},
  pages = {1385--1390},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2020.12.1888},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/SZH4UUPW/Fradkov - 2020 - Early History of Machine Learning.pdf}
}

@article{frankel2019,
  title = {Tensor {{Basis Gaussian Process Models}} of {{Hyperelastic Materials}}},
  author = {Frankel, Ari and Jones, Reese and Swiler, Laura},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.10872 [cs, stat]},
  eprint = {1912.10872},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this work, we develop Gaussian process regression (GPR) models of hyperelastic material behavior. First, we consider the direct approach of modeling the components of the Cauchy stress tensor as a function of the components of the Finger stretch tensor in a Gaussian process. We then consider an improvement on this approach that embeds rotational invariance of the stress-stretch constitutive relation in the GPR representation. This approach requires fewer training examples and achieves higher accuracy while maintaining invariance to rotations exactly. Finally, we consider an approach that recovers the strain-energy density function and derives the stress tensor from this potential. Although the error of this model for predicting the stress tensor is higher, the strain-energy density is recovered with high accuracy from limited training data. The approaches presented here are examples of physics-informed machine learning. They go beyond purely data-driven approaches by embedding the physical system constraints directly into the Gaussian process representation of materials models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/2YMNUNXB/Frankel et al. - 2019 - Tensor Basis Gaussian Process Models of Hyperelast.pdf}
}

@article{furtado2021,
  title = {A Methodology to Generate Design Allowables of Composite Laminates Using Machine Learning},
  author = {Furtado, C. and Pereira, L.F. and Tavares, R.P. and Salgado, M. and Otero, F. and Catalanotti, G. and Arteiro, A. and Bessa, M.A. and Camanho, P.P.},
  year = {2021},
  month = dec,
  journal = {International Journal of Solids and Structures},
  volume = {233},
  pages = {111095},
  issn = {00207683},
  doi = {10.1016/j.ijsolstr.2021.111095},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/8EC6QLXI/Furtado et al. - 2021 - A methodology to generate design allowables of com.pdf}
}

@article{garcia2021,
  title = {Meta-{{Learning}} with {{MAML}} on {{Trees}}},
  author = {Garcia, Jezabel R. and Freddi, Federica and Liao, Feng-Ting and McGowan, Jamie and Nieradzik, Tim and Shiu, Da-shan and Tian, Ye and Bernacchia, Alberto},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.04691 [cs]},
  eprint = {2103.04691},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related. Sharing information between unrelated tasks might hurt performance, and it is unclear how to transfer knowledge across tasks with a hierarchical structure. Our research extends a model agnostic meta-learning model, MAML, by exploiting hierarchical task relationships. Our algorithm, TreeMAML, adapts the model to each task with a few gradient steps, but the adaptation follows the hierarchical tree structure: in each step, gradients are pooled across tasks clusters, and subsequent steps follow down the tree. We also implement a clustering algorithm that generates the tasks tree without previous knowledge of the task structure, allowing us to make use of implicit relationships between the tasks. We show that the new algorithm, which we term TreeMAML, performs better than MAML when the task structure is hierarchical for synthetic experiments. To study the performance of the method in real-world data, we apply this method to Natural Language Understanding, we use our algorithm to finetune Language Models taking advantage of the language phylogenetic tree. We show that TreeMAML improves the state of the art results for cross-lingual Natural Language Inference. This result is useful, since most languages in the world are under-resourced and the improvement on cross-lingual transfer allows the internationalization of NLP models. This results open the window to use this algorithm in other realworld hierarchical datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/JGETAFVX/Garcia et al. - 2021 - Meta-Learning with MAML on Trees.pdf}
}

@article{garoz2019,
  title = {Consistent Application of Periodic Boundary Conditions in Implicit and Explicit Finite Element Simulations of Damage in Composites},
  author = {Garoz, D. and Gilabert, F.A. and Sevenois, R.D.B. and Spronk, S.W.F. and Van Paepegem, W.},
  year = {2019},
  month = jul,
  journal = {Composites Part B: Engineering},
  volume = {168},
  pages = {254--266},
  issn = {13598368},
  doi = {10.1016/j.compositesb.2018.12.023},
  abstract = {This paper presents an implementation-dedicated analysis of Periodic Boundary Conditions (PBCs) for Finite Element (FE) models incorporating highly non-linear effects due to plasticity and damage. This research addresses fiber-reinforced composite materials modeled at micros-scale level using a Representative Volume Element (RVE), where its overall mechanical response is obtained via homogenization techniques. For the sake of clearness, a unidirectional ply with randomly distributed fibers RVE model is assumed. PBCs are implemented for implicit and explicit FE solvers, where conformal and non-conformal meshes can be used. The influence of applying PBCs in the reliability of the mechanical response under tension and shear loading is assessed. Furthermore, the Poisson effect and the consistency of damage and fiber debonding propagation through the periodic boundaries are reported as well as their impact on the homogenized results. Likewise, numerical aspects like computational performance and accuracy are evaluated comparing implicit- versus explicit-based solutions.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/G9LGY6T6/Garoz et al. - 2019 - Consistent application of periodic boundary condit.pdf}
}

@article{geers2010,
  title = {Multi-Scale Computational Homogenization: {{Trends}} and Challenges},
  shorttitle = {Multi-Scale Computational Homogenization},
  author = {Geers, M.G.D. and Kouznetsova, V.G. and Brekelmans, W.A.M.},
  year = {2010},
  month = aug,
  journal = {Journal of Computational and Applied Mathematics},
  volume = {234},
  number = {7},
  pages = {2175--2182},
  issn = {03770427},
  doi = {10.1016/j.cam.2009.08.077},
  abstract = {In the past decades, considerable progress had been made in bridging the mechanics of materials to other disciplines, e.g. downscaling to the field of materials science or upscaling to the field of structural engineering. Within this wide context, this paper reviews the stateof-the-art of a particular, yet powerful, method, i.e. computational homogenization. The paper discusses the main trends since the early developments of this approach up to the ongoing contributions and upcoming challenges in the field.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/5LULS2T9/Geers et al. - 2010 - Multi-scale computational homogenization Trends a.pdf}
}

@article{geers2016,
  title = {Multiscale Modeling of Microstructure\textendash Property Relations},
  author = {Geers, M.G.D. and Yvonnet, J.},
  year = {2016},
  month = aug,
  journal = {MRS Bulletin},
  volume = {41},
  number = {08},
  pages = {610--616},
  issn = {0883-7694, 1938-1425},
  doi = {10.1557/mrs.2016.165},
  abstract = {Abstract                                                                                    ,              The recent decades have seen significant progress in linking the mechanical performance of materials to their underlying microstructure. This article presents an overview of some of these achievements, trends, and challenges. Attention is given to methods initially developed for micromechanics and their gradual evolution toward powerful multiscale methods. Various methods have been proposed for bridging scales in mechanics of materials, all aiming for efficiency and accuracy. Computational homogenization is one of these powerful approaches, now used systematically for the assessment of structure\textendash property relations. Novel solution methods and model reduction techniques provide tools to speed up the structure\textendash property analysis, whereby large-scale computations have been made possible. Truly fast analyses of microstructures may be expected in the near future.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/QJARNCWY/Geers and Yvonnet - 2016 - Multiscale modeling of microstructureâ€“property rel.pdf}
}

@incollection{geers2017,
  title = {Homogenization {{Methods}} and {{Multiscale Modeling}}: {{Nonlinear Problems}}},
  shorttitle = {Homogenization {{Methods}} and {{Multiscale Modeling}}},
  booktitle = {Encyclopedia of {{Computational Mechanics Second Edition}}},
  author = {Geers, Marc G. D. and Kouznetsova, Varvara G. and Matou{\v s}, Karel and Yvonnet, Julien},
  editor = {Stein, Erwin and {\noopsort{borst}}{de Borst}, Ren{\'e} and Hughes, Thomas J R},
  year = {2017},
  month = dec,
  pages = {1--34},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781119176817.ecm2107},
  isbn = {978-1-119-00379-3 978-1-119-17681-7},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/R6PUA596/Geers et al. - 2017 - Homogenization Methods and Multiscale Modeling No.pdf}
}

@article{ghaboussi1991,
  title = {Knowledge-{{Based Modeling}} of {{Material Behavior}} with {{Neural Networks}}},
  author = {Ghaboussi, J. and Garrett, J. H. and Wu, X.},
  year = {1991},
  month = jan,
  journal = {Journal of Engineering Mechanics},
  volume = {117},
  number = {1},
  pages = {132--153},
  issn = {0733-9399, 1943-7889},
  doi = {10.1061/(ASCE)0733-9399(1991)117:1(132)},
  abstract = {TO date, material modeling has involved the development of mathematical models of material behavior derived from human observation of, and reasoning with, experimental data. An alternative, discussed in this paper, is to use a computation and knowledge representation paradigm, called neural networks, developed by researchers in connectionism (a subfield of artificial intelligence) to model material behavior. The main benefits in using a neural-network approach are that all behavior can be represented within a unified environment of a neural network and that the network is built directly from experimental data using the self-organizing capabilities of the neural network, i.e., the network is presented with the experimental data and "learns" the relationships between stresses and strains. Such a modeling strategy has important implications for modeling the behavior of modern, complex materials, such as composites. In this paper, the behaviors of concrete in the state of plane stress under monotonic biaxial loading and compressive uniaxial cycle loading are modeled with a back-propagation neural network. The preliminary results of using neural networks to model materials look very promising.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/8YDAW2HK/Ghaboussi et al. - 1991 - Knowledgeâ€Based Modeling of Material Behavior with.pdf}
}

@article{ghavamian2017,
  title = {{{POD}}\textendash{{DEIM}} Model Order Reduction for Strain-Softening Viscoplasticity},
  author = {Ghavamian, F. and Tiso, P. and Simone, A.},
  year = {2017},
  month = apr,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {317},
  pages = {458--479},
  issn = {00457825},
  doi = {10.1016/j.cma.2016.11.025},
  abstract = {We demonstrate a Model Order Reduction technique for a system of nonlinear equations arising from the Finite Element Method (FEM) discretization of the three-dimensional quasistatic equilibrium equation equipped with a Perzyna viscoplasticity constitutive model. The procedure employs the Proper Orthogonal Decomposition-Galerkin (POD-G) in conjunction with the Discrete Empirical Interpolation Method (DEIM). For this purpose, we collect samples from a standard full order FEM analysis in the offline phase and cluster them using a novel k-means clustering algorithm. The POD and the DEIM algorithms are then employed to construct a corresponding reduced order model. In the online phase, a sample from the current state of the system is passed, at each time step, to a nearest neighbor classifier in which the cluster that best describes it is identified. The force vector and its derivative with respect to the displacement vector are approximated using DEIM, and the system of nonlinear equations is projected onto a lower dimensional subspace using the POD-G. The constructed reduced order model is applied to two typical solid mechanics problems showing strain-localization (a tensile bar and a wall under compression) and a three-dimensional squarefooting problem.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/LLI7I8UM/Ghavamian et al. - 2017 - PODâ€“DEIM model order reduction for strain-softenin.pdf}
}

@article{ghavamian2019,
  title = {Accelerating Multiscale Finite Element Simulations of History-Dependent Materials Using a Recurrent Neural Network},
  author = {Ghavamian, F. and Simone, A.},
  year = {2019},
  month = dec,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {357},
  pages = {112594},
  issn = {00457825},
  doi = {10.1016/j.cma.2019.112594},
  abstract = {FE2 multiscale simulations of history-dependent materials are accelerated by means of a recurrent neural network (RNN) surrogate for the history-dependent micro level response. We propose a simple strategy to efficiently collect stress\textendash strain data from the micro model, and we modify the RNN model such that it resembles a nonlinear finite element analysis procedure during training. We then implement the trained RNN model in the FE2 scheme and employ automatic differentiation to compute the consistent tangent. The exceptional performance of the proposed model is demonstrated through a number of academic examples using strain-softening Perzyna viscoplasticity as the nonlinear material model at the micro level.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/5QIIX527/Ghavamian and Simone - 2019 - Accelerating multiscale finite element simulations.pdf}
}

@phdthesis{ghavamian2021,
  title = {Accelerating Finite Element Analysis Using Machine Learning},
  author = {Ghavamian, F.},
  year = {2021},
  doi = {10.4233/UUID:015BBF35-5E29-4630-B466-1A29D4C5BFB3},
  langid = {english},
  school = {Delft University of Technology},
  file = {/home/taylanot/Zotero/storage/VKD86B27/Ghavamian - 2021 - Accelerating finite element analysis using machine.pdf}
}

@article{gill2022,
  title = {Quantum Computing: {{A}} Taxonomy, Systematic Review and Future Directions},
  shorttitle = {Quantum Computing},
  author = {Gill, Sukhpal Singh and Kumar, Adarsh and Singh, Harvinder and Singh, Manmeet and Kaur, Kamalpreet and Usman, Muhammad and Buyya, Rajkumar},
  year = {2022},
  month = jan,
  journal = {Software: Practice and Experience},
  volume = {52},
  number = {1},
  pages = {66--114},
  issn = {0038-0644, 1097-024X},
  doi = {10.1002/spe.3039},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/55JSJ9GY/Gill et al. - 2022 - Quantum computing A taxonomy, systematic review a.pdf}
}

@article{glorot,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author = {Glorot, Xavier and Bengio, Yoshua},
  pages = {8},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/J7IMGX8J/Glorot and Bengio - Understanding the difï¬culty of training deep feedf.pdf}
}

@article{goldblum2020,
  title = {Truth or {{Backpropaganda}}? {{An Empirical Investigation}} of {{Deep Learning Theory}}},
  shorttitle = {Truth or {{Backpropaganda}}?},
  author = {Goldblum, Micah and Geiping, Jonas and Schwarzschild, Avi and Moeller, Michael and Goldstein, Tom},
  year = {2020},
  month = apr,
  journal = {arXiv:1910.00359 [cs, math, stat]},
  eprint = {1910.00359},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/KZUY53JL/Goldblum et al. - 2020 - Truth or Backpropaganda An Empirical Investigatio.pdf}
}

@article{gong2020,
  title = {A Deep Transfer Learning Model for Inclusion Defect Detection of Aeronautics Composite Materials},
  author = {Gong, Yanfeng and Shao, Hongliang and Luo, Jun and Li, Zhixue},
  year = {2020},
  month = nov,
  journal = {Composite Structures},
  volume = {252},
  pages = {112681},
  issn = {02638223},
  doi = {10.1016/j.compstruct.2020.112681},
  abstract = {Composite materials are increasingly used as structural components in military and civilian aircraft. To ensure their high reliability, numerous non-destructive testing (NDT) techniques have been used to detect defects during production and maintenance. However, most of these techniques are non-automatic, with diagnostic results determined subjectively by operators. Some deep learning methods have been proposed to identify defects in images obtained through NDT, but they need labeled image samples with defects, which can be expensive or unavailable. We propose a deep transfer learning model to accurately extract features for the inclusion of defects in X-ray images of aeronautics composite materials (ACM), whose samples are scarce. We researched an automatic inclusion defect detection method for X-ray images of ACM using our proposed model. Experimental results show that the model can reach 96\% classification accuracy (F1\_measure) with satisfactory detection results.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/3RR7YJ93/Gong et al. - 2020 - A deep transfer learning model for inclusion defec.pdf}
}

@article{gordon2019,
  title = {Meta-{{Learning Probabilistic Inference For Prediction}}},
  author = {Gordon, Jonathan and Bronskill, John and Bauer, Matthias and Nowozin, Sebastian and Turner, Richard E.},
  year = {2019},
  month = aug,
  journal = {arXiv:1805.09921 [cs, stat]},
  eprint = {1805.09921},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/GRLRK38U/Gordon et al. - 2019 - Meta-Learning Probabilistic Inference For Predicti.pdf}
}

@article{goswami2020,
  title = {Transfer Learning Enhanced Physics Informed Neural Network for Phase-Field Modeling of Fracture},
  author = {Goswami, Somdatta and Anitescu, Cosmin and Chakraborty, Souvik and Rabczuk, Timon},
  year = {2020},
  month = apr,
  journal = {Theoretical and Applied Fracture Mechanics},
  volume = {106},
  pages = {102447},
  issn = {01678442},
  doi = {10.1016/j.tafmec.2019.102447},
  abstract = {In this work, we present a new physics informed neural network (PINN) algorithm for solving brittle fracture problems. While most of the PINN algorithms available in the literature minimize the residual of the governing partial differential equation, the proposed approach takes a different path by minimizing the variational energy of the system. Additionally, we modify the neural network output such that the boundary conditions associated with the problem are exactly satisfied. Compared to the conventional residual based PINN, the proposed approach has two major advantages. First, the imposition of boundary conditions is relatively simpler and more robust. Second, the order of derivatives present in the functional form of the variational energy is of lower order than in the residual form used in conventional PINN and hence, training the network is faster. To compute the total variational energy of the system, an efficient scheme that takes as input a geometry described by spline based CAD model and employs Gauss quadrature rules for numerical integration, has been proposed. Moreover, we note that for obtaining the crack path, the proposed PINN has to be trained at each load/displacement step, which can potentially make the algorithm computationally inefficient. To address this issue, we propose to use the concept `transfer learning' wherein, instead of re-training the complete network, we only re-train the network partially while keeping the weights and the biases corresponding to the other portions fixed. With this setup, the computational efficiency of the proposed approach is significantly enhanced. The proposed approach is used to solve six fracture mechanics problems. For all the examples, results obtained using the proposed approach match closely with the results available in the literature. For the first two examples, we compare the results obtained using the proposed approach with the conventional residual based neural network results. For both the problems, the proposed approach is found to yield better accuracy compared to conventional residual based PINN algorithms.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/TRYZ9RK3/Goswami et al. - 2020 - Transfer learning enhanced physics informed neural.pdf}
}

@article{goury2016,
  title = {Automatised Selection of Load Paths to Construct Reduced-Order Models in Computational Damage Micromechanics: From Dissipation-Driven Random Selection to {{Bayesian}} Optimization},
  shorttitle = {Automatised Selection of Load Paths to Construct Reduced-Order Models in Computational Damage Micromechanics},
  author = {Goury, Olivier and Amsallem, David and Bordas, St{\'e}phane Pierre Alain and Liu, Wing Kam and Kerfriden, Pierre},
  year = {2016},
  month = aug,
  journal = {Computational Mechanics},
  volume = {58},
  number = {2},
  pages = {213--234},
  issn = {0178-7675, 1432-0924},
  doi = {10.1007/s00466-016-1290-2},
  abstract = {In this paper, we present new reliable model order reduction strategies for computational micromechanics. The difficulties rely mainly upon the high dimensionality of the parameter space represented by any load path applied onto the representative volume element. We take special care of the challenge of selecting an exhaustive snapshot set. This is treated by first using a random sampling of energy dissipating load paths and then in a more advanced way using Bayesian optimization associated with an interlocked division of the parameter space. Results show that we can insure the selection of an exhaustive snapshot set from which a reliable reduced-order model can be built.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MNCHBQHN/Goury et al. - 2016 - Automatised selection of load paths to construct r.pdf}
}

@article{grant2018,
  title = {Recasting {{Gradient-Based Meta-Learning}} as {{Hierarchical Bayes}}},
  author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.08930 [cs]},
  eprint = {1801.08930},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/XMP5QXD6/Grant et al. - 2018 - Recasting Gradient-Based Meta-Learning as Hierarch.pdf}
}

@article{guedj2019,
  title = {A {{Primer}} on {{PAC-Bayesian Learning}}},
  author = {Guedj, Benjamin},
  year = {2019},
  month = may,
  journal = {arXiv:1901.05353 [cs, stat]},
  eprint = {1901.05353},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Generalised Bayesian learning algorithms are increasingly popular in machine learning, due to their PAC generalisation properties and flexibility. The present paper aims at providing a self-contained survey on the resulting PAC-Bayes framework and some of its main theoretical and algorithmic developments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/S45VH57X/Guedj - 2019 - A Primer on PAC-Bayesian Learning.pdf}
}

@article{guiroy2019,
  title = {Towards {{Understanding Generalization}} in {{Gradient-Based Meta-Learning}}},
  author = {Guiroy, Simon and Verma, Vikas and Pal, Christopher},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.07287 [cs, stat]},
  eprint = {1907.07287},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this work we study generalization of neural networks in gradient-based metalearning by analyzing various properties of the objective landscapes. We experimentally demonstrate that as meta-training progresses, the meta-test solutions, obtained after adapting the meta-train solution of the model, to new tasks via few steps of gradient-based fine-tuning, become flatter, lower in loss, and further away from the meta-train solution. We also show that those meta-test solutions become flatter even as generalization starts to degrade, thus providing an experimental evidence against the correlation between generalization and flat minima in the paradigm of gradient-based meta-leaning. Furthermore, we provide empirical evidence that generalization to new tasks is correlated with the coherence between their adaptation trajectories in parameter space, measured by the average cosine similarity between task-specific trajectory directions, starting from a same meta-train solution. We also show that coherence of meta-test gradients, measured by the average inner product between the task-specific gradient vectors evaluated at meta-train solution, is also correlated with generalization. Based on these observations, we propose a novel regularizer for MAML and provide experimental evidence for its effectiveness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/BKKJ4BVH/Guiroy et al. - 2019 - Towards Understanding Generalization in Gradient-B.pdf}
}

@article{guo2006,
  title = {Application of a New Constitutive Model for the Description of Rubber-like Materials under Monotonic Loading},
  author = {Guo, Z. and Sluys, L.J.},
  year = {2006},
  month = may,
  journal = {International Journal of Solids and Structures},
  volume = {43},
  number = {9},
  pages = {2799--2819},
  issn = {00207683},
  doi = {10.1016/j.ijsolstr.2005.06.026},
  abstract = {The present paper is focused on the development of a constitutive model for the computational analysis of the mechanical behaviour of hyperelastic materials. One of the main obstacles in solving nonlinear elastic problems is the constitutive equation that must be as simple as possible but also realistic in the large strain range, especially for engineering purposes. Gao has proposed a relatively simple model that shows a good performance in tension as well as in compression. In this paper, the capabilities of Gao\~Os model have been discussed. Three sets of experimental data of different types of deformation are used to identify the model parameters. Numerical simulations are in good agreement with experimental data. Comparisons with Ogden\~Os formula and Mooney\textendash Rivlin\~Os formula by means of a theoretical and a numerical analysis demonstrate that Gao\~Os model performs well for the description of hyperelastic material behaviour and covers a very large range of deformation. Another advantage of this model is that it only needs two parameters to predict the mechanical behaviour of hyperelastic materials. As an application, a singular problem of a wedge loaded by a concentrated tensile force is analysed.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/4JBBQR7F/Guo and Sluys - 2006 - Application of a new constitutive model for the de.pdf}
}

@article{guo2021,
  title = {Learning Constitutive Models from Microstructural Simulations via a Non-Intrusive Reduced Basis Method},
  author = {Guo, Theron and Roko{\v s}, Ond{\v r}ej and Veroy, Karen},
  year = {2021},
  month = oct,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {384},
  eprint = {2104.04451},
  eprinttype = {arxiv},
  pages = {113924},
  issn = {00457825},
  doi = {10.1016/j.cma.2021.113924},
  abstract = {In order to optimally design materials, it is crucial to understand the structure-property relations in the material by analyzing the effect of microstructure parameters on the macroscopic properties. In computational homogenization, the microstructure is thus explicitly modeled inside the macrostructure, leading to a coupled two-scale formulation. Unfortunately, the high computational costs of such multiscale simulations often render the solution of design, optimization, or inverse problems infeasible. To address this issue, we propose in this work a non-intrusive reduced basis method to construct inexpensive surrogates for parametrized microscale problems; the method is specifically well-suited for multiscale simulations since the coupled simulation is decoupled into two independent problems: (1) solving the microscopic problem for different (loading or material) parameters and learning a surrogate model from the data; and (2) solving the macroscopic problem with the learned material model. The proposed method has three key features. First, the microscopic stress field can be fully recovered, which is useful for instance for revealing local stress concentrations inside the microstructure. Second, the method is able to accurately predict the stress field for a wide range of material parameters; furthermore, the derivatives of the effective stress with respect to the material parameters are available and can be readily utilized in solving optimization problems. Finally, it is more data efficient, i.e. requiring less training data, as compared to directly performing a regression on the effective stress. To construct the surrogate model, first, a proper orthogonal decomposition is performed on precomputed microscopic stress field snapshots to find a reduced basis for the stress. Second, a regression is employed to infer the coefficients of the reduced basis approximation for any arbitrary parameter value, thus enabling a rapid online evaluation of the microscopic stress. Equipped with the stress field, the effective stress and its partial derivatives can then be derived analytically. For the microstructures in the two test problems considered, the mean approximation error of the effective stress is as low as 0.1\% despite using a relatively small training dataset. Embedded into the macroscopic problem, the reduced order model leads to an online computational speed up of approximately three orders of magnitude while maintaining a high accuracy as compared to the FE2 solver.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Engineering; Finance; and Science},
  file = {/home/taylanot/Zotero/storage/IY7943GC/Guo et al. - 2021 - Learning constitutive models from microstructural .pdf}
}

@article{guo2022,
  title = {Multi-Fidelity Regression Using Artificial Neural Networks: Efficient Approximation of Parameter-Dependent Output Quantities},
  shorttitle = {Multi-Fidelity Regression Using Artificial Neural Networks},
  author = {Guo, Mengwu and Manzoni, Andrea and Amendt, Maurice and Conti, Paolo and Hesthaven, Jan S.},
  year = {2022},
  month = feb,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {389},
  eprint = {2102.13403},
  eprinttype = {arxiv},
  pages = {114378},
  issn = {00457825},
  doi = {10.1016/j.cma.2021.114378},
  abstract = {Highly accurate numerical or physical experiments are often very time-consuming or expensive to obtain. When time or budget restrictions prohibit the generation of additional data, the amount of available samples may be too limited to provide satisfactory model results. Multi-fidelity methods deal with such problems by incorporating information from other sources, which are ideally well-correlated with the high-fidelity data, but can be obtained at a lower cost. By leveraging correlations between different data sets, multi-fidelity methods often yield superior generalization when compared to models based solely on a small amount of high-fidelity data. In the current work, we present the use of artificial neural networks applied to multifidelity regression problems. By elaborating a few existing approaches, we propose new neural network architectures for multi-fidelity regression. The introduced models are compared against a traditional multifidelity regression scheme \textendash{} co-kriging. A collection of artificial benchmarks are presented to measure the performance of the analyzed models. The results show that cross-validation in combination with Bayesian optimization consistently leads to neural network models that outperform the co-kriging scheme. Additionally, we show an application of multi-fidelity regression to an engineering problem. The propagation of a pressure wave into an acoustic horn with parametrized shape and frequency is considered, and the index of reflection intensity is approximated using the proposed multi-fidelity models. A finite element, full-order model and a reduced-order model built through the reduced basis method are adopted as the high- and low-fidelity, respectively. It is shown that the multi-fidelity neural network returns outputs that achieve a comparable accuracy to those from the expensive, full-order model, using only very few full-order evaluations combined with a larger amount of inaccurate but cheap evaluations of a reduced order model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {/home/taylanot/Zotero/storage/QK3DQY2G/Guo et al. - 2022 - Multi-fidelity regression using artificial neural .pdf}
}

@article{haghighat2020,
  title = {A Deep Learning Framework for Solution and Discovery in Solid Mechanics},
  author = {Haghighat, Ehsan and Raissi, Maziar and Moure, Adrian and Gomez, Hector and Juanes, Ruben},
  year = {2020},
  month = may,
  journal = {arXiv:2003.02751 [cs, stat]},
  eprint = {2003.02751},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present the application of a class of deep learning, known as Physics Informed Neural Networks (PINN), to learning and discovery in solid mechanics. We explain how to incorporate the momentum balance and constitutive relations into PINN, and explore in detail the application to linear elasticity, and illustrate its extension to nonlinear problems through an example that showcases von Mises elastoplasticity. While common PINN algorithms are based on training one deep neural network (DNN), we propose a multi-network model that results in more accurate representation of the field variables. To validate the model, we test the framework on synthetic data generated from analytical and numerical reference solutions. We study convergence of the PINN model, and show that Isogeometric Analysis (IGA) results in superior accuracy and convergence characteristics compared with classic low-order Finite Element Method (FEM). We also show the applicability of the framework for transfer learning, and find vastly accelerated convergence during network re-training. Finally, we find that honoring the physics leads to improved robustness: when trained only on a few parameters, we find that the PINN model can accurately predict the solution for a wide range of parameters new to the network\textemdash thus pointing to an important application of this framework to sensitivity analysis and surrogate modeling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {74S30 (primary); 74S05; 74B05; 74L05; 74L10 (secondary),Computer Science - Computational Engineering; Finance; and Science,Computer Science - Machine Learning,J.2,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/6ZG2L5KH/Haghighat et al. - 2020 - A deep learning framework for solution and discove.pdf}
}

@article{harrison2018,
  title = {Meta-{{Learning Priors}} for {{Efficient Online Bayesian Regression}}},
  author = {Harrison, James and Sharma, Apoorva and Pavone, Marco},
  year = {2018},
  month = oct,
  journal = {arXiv:1807.08912 [cs]},
  eprint = {1807.08912},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Gaussian Process (GP) regression has seen widespread use in robotics due to its generality, simplicity of use, and the utility of Bayesian predictions. The predominant implementation of GP regression is a nonparameteric kernel-based approach, as it enables fitting of arbitrary nonlinear functions. However, this approach suffers from two main drawbacks: (1) it is computationally inefficient, as computation scales poorly with the number of samples; and (2) it can be data inefficient, as encoding prior knowledge that can aid the model through the choice of kernel and associated hyperparameters is often challenging and unintuitive. In this work, we propose ALPaCA, an algorithm for efficient Bayesian regression which addresses these issues. ALPaCA uses a dataset of sample functions to learn a domain-specific, finite-dimensional feature encoding, as well as a prior over the associated weights, such that Bayesian linear regression in this feature space yields accurate online predictions of the posterior predictive density. These features are neural networks, which are trained via a meta-learning (or ``learning-to-learn'') approach. ALPaCA extracts all prior information directly from the dataset, rather than restricting prior information to the choice of kernel hyperparameters. Furthermore, by operating in the weight space, it substantially reduces sample complexity. We investigate the performance of ALPaCA on two simple regression problems, two simulated robotic systems, and on a lane-change driving task performed by humans. We find our approach outperforms kernel-based GP regression, as well as state of the art meta-learning approaches, thereby providing a promising plug-in tool for many regression tasks in robotics where scalability and data-efficiency are important.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/taylanot/Zotero/storage/JVLDSIQS/Harrison et al. - 2018 - Meta-Learning Priors for Efficient Online Bayesian.pdf}
}

@article{harrison2020,
  title = {Continuous {{Meta-Learning}} without {{Tasks}}},
  author = {Harrison, James and Sharma, Apoorva and Finn, Chelsea and Pavone, Marco},
  year = {2020},
  month = oct,
  journal = {arXiv:1912.08866 [cs, stat]},
  eprint = {1912.08866},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Meta-learning is a promising strategy for learning to efficiently learn using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with unsegmented time series data. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on three nonlinear meta-regression benchmarks as well as two meta-image-classification benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/6M6I8YHT/Harrison et al. - 2020 - Continuous Meta-Learning without Tasks.pdf}
}

@article{hauseux2018,
  title = {Quantifying the Uncertainty in a Hyperelastic Soft Tissue Model with Stochastic Parameters},
  author = {Hauseux, Paul and Hale, Jack S. and Cotin, St{\'e}phane and Bordas, St{\'e}phane P.A.},
  year = {2018},
  month = oct,
  journal = {Applied Mathematical Modelling},
  volume = {62},
  pages = {86--102},
  issn = {0307904X},
  doi = {10.1016/j.apm.2018.04.021},
  abstract = {We present a simple open-source semi-intrusive computational method to propagate uncertainties through hyperelastic models of soft tissues. The proposed method is up to two orders of magnitude faster than the standard Monte Carlo method. The material model of interest can be altered by adjusting few lines of (FEniCS) code. The method is able to (1) provide the user with statistical confidence intervals on quantities of practical interest, such as the displacement of a tumour or target site in an organ; (2) quantify the sensitivity of the response of the organ to the associated parameters of the material model. We exercise the approach on the determination of a confidence interval on the motion of a target in the brain. We also show that for the boundary conditions under consideration five parameters of the Ogden-Holzapfel-like model have negligible influence on the displacement of the target zone compared to the three most influential parameters. The benchmark problems and all associated data are made available as supplementary material.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/K6KFWBW6/Hauseux et al. - 2018 - Quantifying the uncertainty in a hyperelastic soft.pdf}
}

@article{he2019,
  title = {Task {{Agnostic Continual Learning}} via {{Meta Learning}}},
  author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.05201 [cs, stat]},
  eprint = {1906.05201},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering \textendash{} i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implications of the proposed formalism.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/U5TZVECK/He et al. - 2019 - Task Agnostic Continual Learning via Meta Learning.pdf}
}

@article{he2021,
  title = {A Comparative Study of 85 Hyperelastic Constitutive Models for Both Unfilled Rubber and Highly Filled Rubber Nanocomposite Material},
  author = {He, Hong and Zhang, Qiang and Zhang, Yaru and Chen, Jianfeng and Zhang, Liqun and Li, Fanzhu},
  year = {2021},
  month = jul,
  journal = {Nano Materials Science},
  pages = {S2589965121000490},
  issn = {25899651},
  doi = {10.1016/j.nanoms.2021.07.003},
  abstract = {Nonlinear finite element analysis is widely used for structural optimization of the design and the reliability analysis of complex elastomeric components. However, high-precision numerical results cannot be achieved without reliable strain energy functions (SEFs) of the rubber or rubber nanocomposite material. Although hyperelastic constitutive models have been studied for nearly 80 years, selecting one that accurately describes rubber's mechanical response is still a challenge. This work reviews 85 isotropic SEFs based on both the phenomenological theory and the micromechanical network theory proposed from the 1940s to 2019. A fitting algorithm which can realize the automatic fitting optimization and determination of the parameters of all SEFs reviewed is developed. The ability of each SEF to reproduce the experimental data of both the unfilled and highly filled rubber nanocomposite is quantitatively assessed based on a new proposed evaluation index. The top 30 SEFs for the unfilled rubber and the top 14 SEFs for the highly filled rubber nanocomposite are presented in the ranking lists. Finally, some suggestions on how to select an appropriate hyperelastic constitutive model are given, and the perspective on the future progress of constitutive models is summarized.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/CERCZY7P/He et al. - 2021 - A comparative study of 85 hyperelastic constitutiv.pdf}
}

@article{he2021a,
  title = {A Comparative Study of 85 Hyperelastic Constitutive Models for Both Unfilled Rubber and Highly Filled Rubber Nanocomposite Material},
  author = {He, Hong and Zhang, Qiang and Zhang, Yaru and Chen, Jianfeng and Zhang, Liqun and Li, Fanzhu},
  year = {2021},
  month = jul,
  journal = {Nano Materials Science},
  issn = {2589-9651},
  doi = {10.1016/j.nanoms.2021.07.003},
  abstract = {Nonlinear finite element analysis is widely used for structural optimization of the design and the reliability analysis of complex elastomeric components. However, high-precision numerical results cannot be achieved without reliable strain energy functions (SEFs) of the rubber or rubber nanocomposite material. Although hyperelastic constitutive models have been studied for nearly 80 years, selecting one that accurately describes rubber's mechanical response is still a challenge. This work reviews 85 isotropic SEFs based on both the phenomenological theory and the micromechanical network theory proposed from the 1940s to 2019. A fitting algorithm which can realize the automatic fitting optimization and determination of the parameters of all SEFs reviewed is developed. The ability of each SEF to reproduce the experimental data of both the unfilled and highly filled rubber nanocomposite is quantitatively assessed based on a new proposed evaluation index. The top 30 SEFs for the unfilled rubber and the top 14 SEFs for the highly filled rubber nanocomposite are presented in the ranking lists. Finally, some suggestions on how to select an appropriate hyperelastic constitutive model are given, and the perspective on the future progress of constitutive models is summarized.},
  langid = {english},
  keywords = {Hyperelastic constitutive model,Micromechanical network model,Phenomenological model,Rubber-like materials,Strain energy function,UHYPER subroutine},
  file = {/home/taylanot/Zotero/storage/8R7G96IB/He et al. - 2021 - A comparative study of 85 hyperelastic constitutiv.pdf}
}

@article{he2021b,
  title = {A Comparative Study of 85 Hyperelastic Constitutive Models for Both Unfilled Rubber and Highly Filled Rubber Nanocomposite Material},
  author = {He, Hong and Zhang, Qiang and Zhang, Yaru and Chen, Jianfeng and Zhang, Liqun and Li, Fanzhu},
  year = {2021},
  month = jul,
  journal = {Nano Materials Science},
  pages = {S2589965121000490},
  issn = {25899651},
  doi = {10.1016/j.nanoms.2021.07.003},
  abstract = {Nonlinear finite element analysis is widely used for structural optimization of the design and the reliability analysis of complex elastomeric components. However, high-precision numerical results cannot be achieved without reliable strain energy functions (SEFs) of the rubber or rubber nanocomposite material. Although hyperelastic constitutive models have been studied for nearly 80 years, selecting one that accurately describes rubber's mechanical response is still a challenge. This work reviews 85 isotropic SEFs based on both the phenomenological theory and the micromechanical network theory proposed from the 1940s to 2019. A fitting algorithm which can realize the automatic fitting optimization and determination of the parameters of all SEFs reviewed is developed. The ability of each SEF to reproduce the experimental data of both the unfilled and highly filled rubber nanocomposite is quantitatively assessed based on a new proposed evaluation index. The top 30 SEFs for the unfilled rubber and the top 14 SEFs for the highly filled rubber nanocomposite are presented in the ranking lists. Finally, some suggestions on how to select an appropriate hyperelastic constitutive model are given, and the perspective on the future progress of constitutive models is summarized.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/BISGQQVM/He et al. - 2021 - A comparative study of 85 hyperelastic constitutiv.pdf}
}

@article{hernandez2017,
  title = {Dimensional Hyper-Reduction of Nonlinear Finite Element Models via Empirical Cubature},
  author = {Hern{\'a}ndez, J.A. and Caicedo, M.A. and Ferrer, A.},
  year = {2017},
  month = jan,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {313},
  pages = {687--722},
  issn = {00457825},
  doi = {10.1016/j.cma.2016.10.022},
  abstract = {We present a general framework for the dimensional reduction, in terms of number of degrees of freedom as well as number of integration points (``hyper-reduction''), of nonlinear parameterized finite element (FE) models. The reduction process is divided into two sequential stages. The first stage consists in a common Galerkin projection onto a reduced-order space, as well as in the condensation of boundary conditions and external forces. For the second stage (reduction in number of integration points), we present a novel cubature scheme that efficiently determines optimal points and associated positive weights so that the error in integrating reduced internal forces is minimized. The distinguishing features of the proposed method are: (1) The minimization problem is posed in terms of orthogonal basis vector (obtained via a partitioned Singular Value Decomposition) rather that in terms of snapshots of the integrand. (2) The volume of the domain is exactly integrated. (3) The selection algorithm need not solve in all iterations a nonnegative least-squares problem to force the positiveness of the weights. Furthermore, we show that the proposed method converges to the absolute minimum (zero integration error) when the number of selected points is equal to the number of internal force modes included in the objective function. We illustrate this model reduction methodology by two nonlinear, structural examples (quasi-static bending and resonant vibration of elastoplastic composite plates). In both examples, the number of integration points is reduced three order of magnitudes (with respect to FE analyses) without significantly sacrificing accuracy.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/J2U2FPX4/HernÃ¡ndez et al. - 2017 - Dimensional hyper-reduction of nonlinear finite el.pdf}
}

@article{horgan2004,
  title = {Constitutive {{Models}} for {{Compressible Nonlinearly Elastic Materials}} with {{Limiting Chain Extensibility}}},
  author = {Horgan, Cornelius O. and Saccomandi, Giuseppe},
  year = {2004},
  month = nov,
  journal = {Journal of Elasticity},
  volume = {77},
  number = {2},
  pages = {123--138},
  issn = {0374-3535, 1573-2681},
  doi = {10.1007/s10659-005-4408-x},
  abstract = {Constitutive models are proposed for compressible isotropic hyperelastic materials that reflect limiting chain extensibility. These are generalizations of the model proposed by Gent for incompressible materials. The goal is to understand the effects of limiting chain extensibility when the compressibility of polymeric materials is taken into account. The basic homogeneous deformation of simple tension is considered and simple closed-form relations for the deformation characteristics are obtained for slightly compressible materials. An explicit first-order approximation is obtained for the lateral contraction and for the Poisson function in terms of the axial extension which is shown to be valid for each of two specific compressible versions of the Gent model. One of the main results obtained is that the effect of limiting chain extensibility is to stiffen the material relative to the neo-Hookean compressible case.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/WTQHZMCG/Horgan and Saccomandi - 2004 - Constitutive Models for Compressible Nonlinearly E.pdf}
}

@article{hornik1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  abstract = {This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f\&v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward networks are u class of universul rlpproximators.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/LP8BI3WK/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf}
}

@article{hsissou2021,
  title = {Polymer Composite Materials: {{A}} Comprehensive Review},
  shorttitle = {Polymer Composite Materials},
  author = {Hsissou, Rachid and Seghiri, Rajaa and Benzekri, Zakaria and Hilali, Miloudi and Rafik, Mohamed and Elharfi, Ahmed},
  year = {2021},
  month = apr,
  journal = {Composite Structures},
  volume = {262},
  pages = {113640},
  issn = {02638223},
  doi = {10.1016/j.compstruct.2021.113640},
  abstract = {Herein, in the present review we developed and investigated a comprehensive review of advanced composite materials based on thermoplastic polymers, elastomer polymers and thermosetting polymers. These advanced composite materials were reinforced by organic and/or inorganic fibers and formulated using various fillers such as organic, mineral and metallic. Further, we present the development and the synthesis of several macromolecular matrices namely polycarbonate, polyhexamethylene sebacic, polyether sulfone, polyether ether ketone, polyether ketone ketone, polyether imide, polyethylene terephthalate, phenoplasts, epoxy resin and polyurethane. The advantage of composite materials formulating is to have excellent mechanical performances, high thermal resistance, good fire behavior, high impact resistance, best abrasion resistance, exceptional electric insulation and good rigidity. Then, composite materials formulation were examined and discussed in detail.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/UTCFT7EN/Hsissou et al. - 2021 - Polymer composite materials A comprehensive revie.pdf}
}

@article{hutchinson2017,
  title = {Overcoming Data Scarcity with Transfer Learning},
  author = {Hutchinson, Maxwell L. and Antono, Erin and Gibbons, Brenna M. and Paradiso, Sean and Ling, Julia and Meredig, Bryce},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.05099 [cond-mat, stat]},
  eprint = {1711.05099},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, stat},
  abstract = {Despite increasing focus on data publication and discovery in materials science and related fields, the global view of materials data is highly sparse. This sparsity encourages training models on the union of multiple datasets, but simple unions can prove problematic as (ostensibly) equivalent properties may be measured or computed differently depending on the data source. These hidden contextual differences introduce irreducible errors into analyses, fundamentally limiting their accuracy. Transfer learning, where information from one dataset is used to inform a model on another, can be an effective tool for bridging sparse data while preserving the contextual differences in the underlying measurements. Here, we describe and compare three techniques for transfer learning: multi-task, difference, and explicit latent variable architectures. We show that difference architectures are most accurate in the multi-fidelity case of mixed DFT and experimental band gaps, while multi-task most improves classification performance of color with band gaps. For activation energies of steps in NO reduction, the explicit latent variable method is not only the most accurate, but also enjoys cancellation of errors in functions that depend on multiple tasks. These results motivate the publication of high quality materials datasets that encode transferable information, independent of industrial or academic interest in the particular labels, and encourage further development and application of transfer learning methods to materials informatics problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Materials Science,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/4LR2KT9L/Hutchinson et al. - 2017 - Overcoming data scarcity with transfer learning.pdf}
}

@book{hutter2019,
  title = {Automated {{Machine Learning}}: {{Methods}}, {{Systems}}, {{Challenges}}},
  shorttitle = {Automated {{Machine Learning}}},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-05318-5},
  isbn = {978-3-030-05317-8 978-3-030-05318-5},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/TYXLRJUT/Hutter et al. - 2019 - Automated Machine Learning Methods, Systems, Chal.pdf}
}

@article{jacot2020,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  shorttitle = {Neural {{Tangent Kernel}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  year = {2020},
  month = feb,
  journal = {arXiv:1806.07572 [cs, math, stat]},
  eprint = {1806.07572},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit (16; 4; 7; 13; 6), thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function f\texttheta{} (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/L8QUGFBN/Jacot et al. - 2020 - Neural Tangent Kernel Convergence and Generalizat.pdf}
}

@article{jaegle2022,
  title = {Perceiver {{IO}}: {{A General Architecture}} for {{Structured Inputs}} \& {{Outputs}}},
  shorttitle = {Perceiver {{IO}}},
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and H{\'e}naff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Jo{\=a}o},
  year = {2022},
  month = mar,
  journal = {arXiv:2107.14795 [cs, eess]},
  eprint = {2107.14795},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/taylanot/Zotero/storage/JN8RB88S/Jaegle et al. - 2022 - Perceiver IO A General Architecture for Structure.pdf}
}

@article{jamal2018,
  title = {Task-{{Agnostic Meta-Learning}} for {{Few-shot Learning}}},
  author = {Jamal, Muhammad Abdullah and Qi, Guo-Jun and Shah, Mubarak},
  year = {2018},
  month = may,
  journal = {arXiv:1805.07722 [cs, stat]},
  eprint = {1805.07722},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Meta-learning approaches have been proposed to tackle the few-shot learning problem. Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it is over-trained on existing tasks during meta-training phase. In other words, the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially when only very few examples are available to update the model. To avoid a biased meta-learner and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we present an entropy-based approach that meta-learns an unbiased initial model with the largest uncertainty over the output labels by preventing it from over-performing in classification tasks. Alternatively, a more general inequality-minimization TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial losses beyond the classification tasks wherever a suitable loss can be defined. Experiments on benchmarked datasets demonstrate that the proposed approaches outperform compared meta-learning algorithms in both few-shot classification and reinforcement learning tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/BMNP3T9M/Jamal et al. - 2018 - Task-Agnostic Meta-Learning for Few-shot Learning.pdf}
}

@article{jang2021,
  title = {Machine Learning-Based Constitutive Model for {{J2-}} Plasticity},
  author = {Jang, Dong Phill and Fazily, Piemaan and Yoon, Jeong Whan},
  year = {2021},
  month = mar,
  journal = {International Journal of Plasticity},
  volume = {138},
  pages = {102919},
  issn = {07496419},
  doi = {10.1016/j.ijplas.2020.102919},
  abstract = {This research aims to propose a machine learning (ML)-based constitutive model to predict elastoplastic behavior for J2-plasticity. An artificial neural network (ANN) was constructed to replace the nonlinear stress-integration scheme conducted in the conventional theoretical constitutive model under isotropic hardening and associated flow rule. The training dataset required for the ANN Model was numerically generated based on the conventional return map\- ping scheme in the principal stress space. The training has been effectively carried out with one element simulation along all the possible plastic loading paths for problem independent training. A conventional theoretical method is used for the unloading procedure. Therefore, ANN is selectively utilized only for nonlinear plastic loading while keeping linear elastic loading and the unloading with a physics-based model. After one element training, the ML-based constitutive model was implemented in Abaqus User MATerial (UMAT) and its performance was verified. For this purpose, one element and tensile test simulations were applied to examine the accuracy of the ANN-based model. Also, for fully nonlinear strain-paths, a circular cup drawing simulation was applied to predict the cup profiles which was compared with that to the conventional J2 plas\- ticity. It was concluded that the simulation results predicted from the ANN-based model show good agreement with those from the conventional J2-based constitutive model. Also, according to simulation time, the ANN-based model shows an advantage in computational efficiency.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/P9EM5UWB/Jang et al. - 2021 - Machine learning-based constitutive model for J2- .pdf}
}

@article{jerfel2019,
  title = {Reconciling Meta-Learning and Continual Learning with Online Mixtures of Tasks},
  author = {Jerfel, Ghassen and Grant, Erin and Griffiths, Thomas L. and Heller, Katherine},
  year = {2019},
  month = jun,
  journal = {arXiv:1812.06080 [cs, stat]},
  eprint = {1812.06080},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/DSDGFFQ7/Jerfel et al. - 2019 - Reconciling meta-learning and continual learning w.pdf}
}

@article{ji2020,
  title = {Theoretical {{Convergence}} of {{Multi-Step Model-Agnostic Meta-Learning}}},
  author = {Ji, Kaiyi and Yang, Junjie and Liang, Yingbin},
  year = {2020},
  month = jul,
  journal = {arXiv:2002.07836 [cs, math, stat]},
  eprint = {2002.07836},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {As a popular meta-learning approach, the model-agnostic meta-learning (MAML) algorithm has been widely used due to its simplicity and effectiveness. However, the convergence of the general multi-step MAML still remains unexplored. In this paper, we develop a new theoretical framework to provide such convergence guarantee for two types of objective functions that are of interest in practice: (a) resampling case (e.g., reinforcement learning), where loss functions take the form in expectation and new data are sampled as the algorithm runs; and (b) finite-sum case (e.g., supervised learning), where loss functions take the finite-sum form with given samples. For both cases, we characterize the convergence rate and the computational complexity to attain an \k{o}-accurate solution for multi-step MAML in the general nonconvex setting. In particular, our results suggest that an inner-stage stepsize needs to be chosen inversely proportional to the number N of inner-stage steps in order for N -step MAML to have guaranteed convergence. From the technical perspective, we develop novel techniques to deal with the nested structure of the meta gradient for multi-step MAML, which can be of independent interest.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/4CNSKSKH/Ji et al. - 2020 - Theoretical Convergence of Multi-Step Model-Agnost.pdf}
}

@article{jiang2022,
  title = {Regularized {{RKHS-Based Subspace Learning}} for {{Motor Imagery Classification}}},
  author = {Jiang, Linzhi and Liu, Shuyu and Ma, Zhengming and Lei, Wenjie and Chen, Cheng},
  year = {2022},
  month = jan,
  journal = {Entropy},
  volume = {24},
  number = {2},
  pages = {195},
  issn = {1099-4300},
  doi = {10.3390/e24020195},
  abstract = {Brain\textendash computer interface (BCI) technology allows people with disabilities to communicate with the physical environment. One of the most promising signals is the non-invasive electroencephalogram (EEG) signal. However, due to the non-stationary nature of EEGs, a subject's signal may change over time, which poses a challenge for models that work across time. Recently, domain adaptive learning (DAL) has shown its superior performance in various classification tasks. In this paper, we propose a regularized reproducing kernel Hilbert space (RKHS) subspace learning algorithm with K-nearest neighbors (KNNs) as a classifier for the task of motion imagery signal classification. First, we reformulate the framework of RKHS subspace learning with a rigorous mathematical inference. Secondly, since the commonly used maximum mean difference (MMD) criterion measures the distribution variance based on the mean value only and ignores the local information of the distribution, a regularization term of source domain linear discriminant analysis (SLDA) is proposed for the first time, which reduces the variance of similar data and increases the variance of dissimilar data to optimize the distribution of source domain data. Finally, the RKHS subspace framework was constructed sparsely considering the sensitivity of the BCI data. We test the proposed algorithm in this paper, first on four standard datasets, and the experimental results show that the other baseline algorithms improve the average accuracy by 2\textendash 9\% after adding SLDA. In the motion imagery classification experiments, the average accuracy of our algorithm is 3\% higher than the other algorithms, demonstrating the adaptability and effectiveness of the proposed algorithm.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/US5PALL8/Jiang et al. - 2022 - Regularized RKHS-Based Subspace Learning for Motor.pdf}
}

@article{jose2020,
  title = {Transfer {{Meta-Learning}}: {{Information-Theoretic Bounds}} and {{Information Meta-Risk Minimization}}},
  shorttitle = {Transfer {{Meta-Learning}}},
  author = {Jose, Sharu Theresa and Simeone, Osvaldo and Durisi, Giuseppe},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.02872 [cs, eess, math]},
  eprint = {2011.02872},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, math},
  abstract = {Meta-learning automatically infers an inductive bias by observing data from a number of related tasks. The inductive bias is encoded by hyperparameters that determine aspects of the model class or training algorithm, such as initialization or learning rate. Meta-learning assumes that the learning tasks belong to a task environment, and that tasks are drawn from the same task environment both during meta-training and meta-testing. This, however, may not hold true in practice. In this paper, we introduce the problem of transfer meta-learning, in which tasks are drawn from a target task environment during meta-testing that may differ from the source task environment observed during meta-training. Novel information-theoretic upper bounds are obtained on the transfer meta-generalization gap, which measures the difference between the meta-training loss, available at the meta-learner, and the average loss on meta-test data from a new, randomly selected, task in the target task environment. The first bound, on the average transfer meta-generalization gap, captures the meta-environment shift between source and target task environments via the KL divergence between source and target data distributions. The second, PAC-Bayesian bound, and the third, single-draw bound, account for this shift via the loglikelihood ratio between source and target task distributions. Furthermore, two transfer meta-learning solutions are introduced. For the first, termed Empirical Meta-Risk Minimization (EMRM), we derive bounds on the average optimality gap. The second, referred to as Information Meta-Risk Minimization (IMRM), is obtained by minimizing the PAC-Bayesian bound. IMRM is shown via experiments to potentially outperform EMRM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  file = {/home/taylanot/Zotero/storage/T5M7S3UE/Jose et al. - 2020 - Transfer Meta-Learning Information-Theoretic Boun.pdf}
}

@article{kalan2020,
  title = {Minimax {{Lower Bounds}} for {{Transfer Learning}} with {{Linear}} and {{One-hidden Layer Neural Networks}}},
  author = {Kalan, Seyed Mohammadreza Mousavi and Fabian, Zalan and Avestimehr, A. Salman and Soltanolkotabi, Mahdi},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.10581 [cs, math, stat]},
  eprint = {2006.10581},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Transfer learning has emerged as a powerful technique for improving the performance of machine learning models on new domains where labeled training data may be scarce. In this approach a model trained for a source task, where plenty of labeled training data is available, is used as a starting point for training a model on a related target task with only few labeled training data. Despite recent empirical success of transfer learning approaches, the benefits and fundamental limits of transfer learning are poorly understood. In this paper we develop a statistical minimax framework to characterize the fundamental limits of transfer learning in the context of regression with linear and one-hidden layer neural network models. Specifically, we derive a lower-bound for the target generalization error achievable by any algorithm as a function of the number of labeled source and target data as well as appropriate notions of similarity between the source and target tasks. Our lower bound provides new insights into the benefits and limitations of transfer learning. We further corroborate our theoretical finding with various experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/K3ZHNPKC/Kalan et al. - 2020 - Minimax Lower Bounds for Transfer Learning with Li.pdf}
}

@article{kang,
  title = {Transferable {{Meta Learning Across Domains}}},
  author = {Kang, Bingyi and Feng, Jiashi},
  pages = {11},
  abstract = {Meta learning algorithms are effective at obtaining meta models with the capability of solving new tasks quickly. However, they critically require sufficient tasks for meta model training and the resulted model can only solve new tasks similar to the training ones. These limitations make them suffer performance decline in presence of insufficiency of training tasks in target domains and task heterogeneity\textemdash the source (model training) tasks presents different characteristics from target (model application) tasks. To overcome these two significant limitations of existing meta learning algorithms, we introduce the cross-domain meta learning framework and propose a new transferable meta learning (TML) algorithm. TML performs meta task adaptation jointly with meta model learning, which effectively narrows divergence between source and target tasks and enables transferring source metaknowledge to solve target tasks. Thus, the resulted transferable meta model can solve new learning tasks in new domains quickly. We apply the proposed TML to cross-domain fewshot classification problems and evaluate its performance on multiple benchmarks. It performs significantly better and faster than wellestablished meta learning algorithms and finetuned domain-adapted models.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/DTITUDSW/Kang and Feng - Transferable Meta Learning Across Domains.pdf}
}

@article{karniadakis2021,
  title = {Physics-Informed Machine Learning},
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Physics},
  volume = {3},
  number = {6},
  pages = {422--440},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00314-5},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-d imensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-t ime domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-b ased regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-d imensional problems.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/K5BDCYEZ/Karniadakis et al. - 2021 - Physics-informed machine learning.pdf}
}

@misc{kavokin2016,
  title = {Bunching of Numbers in a Non-Ideal Roulette: The Key to Winning Strategies},
  shorttitle = {Bunching of Numbers in a Non-Ideal Roulette},
  author = {Kavokin, A. V. and Sheremet, A. S. and Petrov, M. Yu},
  year = {2016},
  month = jan,
  number = {arXiv:1602.06943},
  eprint = {1602.06943},
  eprinttype = {arxiv},
  primaryclass = {q-fin},
  publisher = {{arXiv}},
  abstract = {Chances of a gambler are always lower than chances of a casino in the case of an ideal, mathematically perfect roulette, if the capital of the gambler is limited and the minimum and maximum allowed bets are limited by the casino. However, a realistic roulette is not ideal: the probabilities of realisation of different numbers slightly deviate. Describing this deviation by a statistical distribution with a width \{\textbackslash delta\} we find a critical \{\textbackslash delta\} that equalizes chances of gambler and casino in the case of a simple strategy of the game: the gambler always puts equal bets to the last N numbers. For up-critical \{\textbackslash delta\} the expected return of the roulette becomes positive. We show that the dramatic increase of gambler's chances is a manifestation of bunching of numbers in a non-ideal roulette. We also estimate the critical starting capital needed to ensure the low risk game for an indefinite time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Finance - General Finance},
  file = {/home/taylanot/Zotero/storage/38ANEEI3/Kavokin et al. - 2016 - Bunching of numbers in a non-ideal roulette the k.pdf}
}

@article{kerfriden2011,
  title = {Bridging Proper Orthogonal Decomposition Methods and Augmented {{Newton}}\textendash{{Krylov}} Algorithms: {{An}} Adaptive Model Order Reduction for Highly Nonlinear Mechanical Problems},
  shorttitle = {Bridging Proper Orthogonal Decomposition Methods and Augmented {{Newton}}\textendash{{Krylov}} Algorithms},
  author = {Kerfriden, P. and Gosselet, P. and Adhikari, S. and Bordas, S.P.A.},
  year = {2011},
  month = jan,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {200},
  number = {5-8},
  pages = {850--866},
  issn = {00457825},
  doi = {10.1016/j.cma.2010.10.009},
  abstract = {This article describes a bridge between POD-based model order reduction techniques and the classical Newton/Krylov solvers. This bridge is used to derive an efficient algorithm to correct, ``on-the-fly'', the reduced order modelling of highly nonlinear problems undergoing strong topological changes. Damage initiation problems tackled via a corrected hyperreduction method are used as an example. It is shown that the relevancy of reduced order model can be significantly improved with reasonable additional costs when using this algorithm, even when strong topological changes are involved.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/CSW5LUBC/Kerfriden et al. - 2011 - Bridging proper orthogonal decomposition methods a.pdf}
}

@article{kerfriden2013,
  title = {A Partitioned Model Order Reduction Approach to Rationalise Computational Expenses in Nonlinear Fracture Mechanics},
  author = {Kerfriden, P. and Goury, O. and Rabczuk, T. and Bordas, S.P.A.},
  year = {2013},
  month = apr,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {256},
  pages = {169--188},
  issn = {00457825},
  doi = {10.1016/j.cma.2012.12.004},
  abstract = {We propose in this paper a reduced order modelling technique based on domain partitioning for parametric problems of fracture. We show that coupling domain decomposition and projection-based model order reduction permits to focus the numerical effort where it is most needed: around the zones where damage propagates. No a priori knowledge of the damage pattern is required, the extraction of the corresponding spatial regions being based solely on algebra. The efficiency of the proposed approach is demonstrated numerically with an example relevant to engineering fracture.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/VW4MTATV/Kerfriden et al. - 2013 - A partitioned model order reduction approach to ra.pdf}
}

@inproceedings{kerinec2018,
  title = {When Does Deep Multi-Task Learning Work for Loosely Related Document Classification Tasks?},
  booktitle = {Proceedings of the 2018 {{EMNLP Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Kerinec, Emma and Braud, Chlo{\'e} and S{\o}gaard, Anders},
  year = {2018},
  pages = {1--8},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-5401},
  abstract = {This work aims to contribute to our understanding of when multi-task learning through parameter sharing in deep neural networks leads to improvements over single-task learning. We focus on the setting of learning from loosely related tasks, for which no theoretical guarantees exist. We therefore approach the question empirically, studying which properties of datasets and single-task learning characteristics correlate with improvements from multi-task learning. We are the first to study this in a text classification setting and across more than 500 different task pairs.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/FTQKC9GX/Kerinec et al. - 2018 - When does deep multi-task learning work for loosel.pdf}
}

@article{khodak,
  title = {Provable {{Guarantees}} for {{Gradient-Based Meta-Learning}}},
  author = {Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet},
  pages = {10},
  abstract = {We study the problem of meta-learning through the lens of online convex optimization, developing a meta-algorithm bridging the gap between popular gradient-based meta-learning and classical regularization-based multi-task transfer methods. Our method is the first to simultaneously satisfy good sample efficiency guarantees in the convex setting, with generalization bounds that improve with task-similarity, while also being computationally scalable to modern deep learning architectures and the many-task setting. Despite its simplicity, the algorithm matches, up to a constant factor, a lower bound on the performance of any such parameter-transfer method under natural task similarity assumptions. We use experiments in both convex and deep learning settings to verify and demonstrate the applicability of our theory.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/LRGZI8U5/Khodak et al. - Provable Guarantees for Gradient-Based Meta-Learni.pdf}
}

@inproceedings{kienzle2006,
  title = {Personalized Handwriting Recognition via Biased Regularization},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Kienzle, Wolf and Chellapilla, Kumar},
  year = {2006},
  pages = {457--464},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania}},
  doi = {10.1145/1143844.1143902},
  abstract = {We present a new approach to personalized handwriting recognition. The problem, also known as writer adaptation, consists of converting a generic (user-independent) recognizer into a personalized (user-dependent) one, which has an improved recognition rate for a particular user. The adaptation step usually involves user-specific samples, which leads to the fundamental question of how to fuse this new information with that captured by the generic recognizer. We propose adapting the recognizer by minimizing a regularized risk functional (a modified SVM) where the prior knowledge from the generic recognizer enters through a modified regularization term. The result is a simple personalization framework with very good practical properties. Experiments on a 100 class realworld data set show that the number of errors can be reduced by over 40\% with as few as five user samples per character.},
  isbn = {978-1-59593-383-6},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/SF465R23/Kienzle and Chellapilla - 2006 - Personalized handwriting recognition via biased re.pdf}
}

@book{kim2015,
  title = {Introduction to {{Nonlinear Finite Element Analysis}}},
  author = {Kim, Nam-Ho},
  year = {2015},
  publisher = {{Springer US}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-1746-1},
  isbn = {978-1-4419-1745-4 978-1-4419-1746-1},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/HTECZD5U/Kim - 2015 - Introduction to Nonlinear Finite Element Analysis.pdf}
}

@article{kim2018,
  title = {Bayesian {{Model-Agnostic Meta-Learning}}},
  author = {Kim, Taesup and Yoon, Jaesik and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
  year = {2018},
  month = nov,
  journal = {arXiv:1806.03836 [cs, stat]},
  eprint = {1806.03836},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Due to the inherent model uncertainty, learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines efficient gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. Unlike previous methods, during fast adaptation, the method is capable of learning complex uncertainty structure beyond a simple Gaussian approximation, and during meta-update, a novel Bayesian mechanism prevents meta-level overfitting. Remaining a gradientbased method, it is also the first Bayesian model-agnostic meta-learning method applicable to various tasks including reinforcement learning. Experiment results show the accuracy and robustness of the proposed method in sinusoidal regression, image classification, active learning, and reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/R76ARVBA/Kim et al. - 2018 - Bayesian Model-Agnostic Meta-Learning.pdf}
}

@article{kim2020,
  title = {Multi-Step {{Estimation}} for {{Gradient-based Meta-learning}}},
  author = {Kim, Jin-Hwa and Park, Junyoung and Choi, Yongseok},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.04298 [cs, stat]},
  eprint = {2006.04298},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gradient-based meta-learning approaches have been successful in few-shot learning, transfer learning, and a wide range of other domains. Despite its efficacy and simplicity, the burden of calculating the Hessian matrix with large memory footprints is the critical challenge in large-scale applications. To tackle this issue, we propose a simple yet straightforward method to reduce the cost by reusing the same gradient in a window of inner steps. We describe the dynamics of the multi-step estimation in the Lagrangian formalism and discuss how to reduce evaluating second-order derivatives estimating the dynamics. To validate our method, we experiment on meta-transfer learning and few-shot learning tasks for multiple settings. The experiment on meta-transfer emphasizes the applicability of training meta-networks, where other approximations are limited. For few-shot learning, we evaluate time and memory complexities compared with popular baselines. We show that our method significantly reduces training time and memory usage, maintaining competitive accuracies, or even outperforming in some cases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/YQCBTDYR/Kim et al. - 2020 - Multi-step Estimation for Gradient-based Meta-lear.pdf}
}

@article{kirkpatrick2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  month = jan,
  journal = {arXiv:1612.00796 [cs, stat]},
  eprint = {1612.00796},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/6UQJPX2F/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf}
}

@article{kordik2018,
  title = {Discovering Predictive Ensembles for Transfer Learning and Meta-Learning},
  author = {Kord{\'i}k, Pavel and {\v C}ern{\'y}, Jan and Fr{\'y}da, Tom{\'a}{\v s}},
  year = {2018},
  month = jan,
  journal = {Machine Learning},
  volume = {107},
  number = {1},
  pages = {177--207},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5682-0},
  abstract = {Recent meta-learning approaches are oriented towards algorithm selection, optimization or recommendation of existing algorithms. In this article we show how data-tailored algorithms can be constructed from building blocks on small data sub-samples. Building blocks, typically weak learners, are optimized and evolved into data-tailored hierarchical ensembles. Good-performing algorithms discovered by evolutionary algorithm can be reused on data sets of comparable complexity. Furthermore, these algorithms can be scaled up to model large data sets. We demonstrate how one particular template (simple ensemble of fast sigmoidal regression models) outperforms state-of-the-art approaches on the Airline data set. Evolved hierarchical ensembles can therefore be beneficial as algorithmic building blocks in meta-learning, including meta-learning at scale.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/76MI7QBD/KordÃ­k et al. - 2018 - Discovering predictive ensembles for transfer lear.pdf}
}

@article{kouw2019,
  title = {An Introduction to Domain Adaptation and Transfer Learning},
  author = {Kouw, Wouter M. and Loog, Marco},
  year = {2019},
  month = jan,
  journal = {arXiv:1812.11806 [cs, stat]},
  eprint = {1812.11806},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is not an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. Domain adaptation and transfer learning are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, we present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? We will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, we discuss three special cases of data set shift, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant spaces, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which we will discuss in the last section. We conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/GALJAPK3/Kouw and Loog - 2019 - An introduction to domain adaptation and transfer .pdf}
}

@article{kovachki2021,
  title = {Neural {{Operator}}: {{Learning Maps Between Function Spaces}}},
  shorttitle = {Neural {{Operator}}},
  author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  year = {2021},
  month = dec,
  journal = {arXiv:2108.08481 [cs, math]},
  eprint = {2108.08481},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators that map between infinite dimensional function spaces. We formulate the approximation of operators by composition of a class of linear integral operators and nonlinear activation functions, so that the composed operator can approximate complex nonlinear operators. We prove a universal approximation theorem for our construction. Furthermore, we introduce four classes of operator parameterizations: graph-based operators, low-rank operators, multipole graph-based operators, and Fourier operators and describe efficient algorithms for computing with each one. The proposed neural operators are resolution-invariant: they share the same network parameters between different discretizations of the underlying function spaces and can be used for zero-shot super-resolution. Learning surrogate maps for the solution operators of partial differential equations (PDEs) provide a motivating class of examples. Numerical experiments are presented in which the solution operator (mapping initial condition to solution) is learned for Burgers' equation and the Navier-Stokes equation, and in which the coefficient to solution map is learned in the Darcy model of porous medium flow; a variety of linear operators, defined by linear PDEs, are also used to illustrate key ideas. In a variety of settings, the proposed neural networks show superior performance compared to existing machine learning based methodologies applied to these problems, while being several order of magnitude faster than conventional PDE solvers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {/home/taylanot/Zotero/storage/8PHC5A4X/Kovachki et al. - 2021 - Neural Operator Learning Maps Between Function Sp.pdf}
}

@article{krijthe2019,
  title = {The {{Pessimistic Limits}} and {{Possibilities}} of {{Margin-based Losses}} in {{Semi-supervised Learning}}},
  author = {Krijthe, Jesse H. and Loog, Marco},
  year = {2019},
  month = jan,
  journal = {arXiv:1612.08875 [cs, stat]},
  eprint = {1612.08875},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Consider a classification problem where we have both labeled and unlabeled data available. We show that for linear classifiers defined by convex margin-based surrogate losses that are decreasing, it is impossible to construct any semi-supervised approach that is able to guarantee an improvement over the supervised classifier measured by this surrogate loss on the labeled and unlabeled data. For convex margin-based loss functions that also increase, we demonstrate safe improvements are possible.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/XSQE3KGH/Krijthe and Loog - 2019 - The Pessimistic Limits and Possibilities of Margin.pdf}
}

@article{krishnapriyan2021,
  title = {Characterizing Possible Failure Modes in Physics-Informed Neural Networks},
  author = {Krishnapriyan, Aditi S. and Gholami, Amir and Zhe, Shandian and Kirby, Robert M. and Mahoney, Michael W.},
  year = {2021},
  month = nov,
  journal = {arXiv:2109.01050 [physics]},
  eprint = {2109.01050},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Recent work in scientific machine learning has developed so-called physicsinformed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Physics - Computational Physics},
  file = {/home/taylanot/Zotero/storage/L8RRTJEB/Krishnapriyan et al. - 2021 - Characterizing possible failure modes in physics-i.pdf}
}

@article{kumar2020,
  title = {Active {{Learning Query Strategies}} for {{Classification}}, {{Regression}}, and {{Clustering}}: {{A Survey}}},
  shorttitle = {Active {{Learning Query Strategies}} for {{Classification}}, {{Regression}}, and {{Clustering}}},
  author = {Kumar, Punit and Gupta, Atul},
  year = {2020},
  month = jul,
  journal = {Journal of Computer Science and Technology},
  volume = {35},
  number = {4},
  pages = {913--945},
  issn = {1000-9000, 1860-4749},
  doi = {10.1007/s11390-020-9487-4},
  abstract = {Generally, data is available abundantly in unlabeled form, and its annotation requires some cost. The labeling, as well as learning cost, can be minimized by learning with the minimum labeled data instances. Active learning (AL), learns from a few labeled data instances with the additional facility of querying the labels of instances from an expert annotator or oracle. The active learner uses an instance selection strategy for selecting those critical query instances, which reduce the generalization error as fast as possible. This process results in a refined training dataset, which helps in minimizing the overall cost. The key to the success of AL is query strategies that select the candidate query instances and help the learner in learning a valid hypothesis. This survey reviews AL query strategies for classification, regression, and clustering under the pool-based AL scenario. The query strategies under classification are further divided into: informative-based, representative-based, informative- and representative-based, and others. Also, more advanced query strategies based on reinforcement learning and deep learning, along with query strategies under the realistic environment setting, are presented. After a rigorous mathematical analysis of AL strategies, this work presents a comparative analysis of these strategies. Finally, implementation guide, applications, and challenges of AL are discussed.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/JQXQBUND/Kumar and Gupta - 2020 - Active Learning Query Strategies for Classificatio.pdf}
}

@article{kunc2019,
  title = {Finite {{Strain Homogenization Using}} a {{Reduced Basis}} and {{Efficient Sampling}}},
  author = {Kunc, Oliver and Fritzen, Felix},
  year = {2019},
  month = may,
  journal = {Mathematical and Computational Applications},
  volume = {24},
  number = {2},
  eprint = {1904.01521},
  eprinttype = {arxiv},
  pages = {56},
  issn = {2297-8747},
  doi = {10.3390/mca24020056},
  abstract = {The computational homogenization of hyperelastic solids in the geometrically nonlinear context has yet to be treated with sufficient efficiency in order to allow for real-world applications in true multiscale settings. This problem is addressed by a problem-specific surrogate model founded on a reduced basis approximation of the deformation gradient on the microscale. The setup phase is based upon a snapshot POD on deformation gradient fluctuations, in contrast to the widespread displacement-based approach. In order to reduce the computational offline costs, the space of relevant macroscopic stretch tensors is sampled efficiently by employing the Hencky strain. Numerical results show speed-up factors in the order of 5\textendash 100 and significantly improved robustness while retaining good accuracy. An open-source demonstrator tool with 50 lines of code emphasizes the simplicity and efficiency of the method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {74Q05; 74B20; 74S30,Computer Science - Computational Engineering; Finance; and Science,Mathematics - Numerical Analysis},
  file = {/home/taylanot/Zotero/storage/TBEGEYKS/Kunc and Fritzen - 2019 - Finite Strain Homogenization Using a Reduced Basis.pdf}
}

@article{kus2021,
  title = {Sparse Quantum {{Gaussian}} Processes to Counter the Curse of Dimensionality},
  author = {Ku{\'s}, Gawe{\l} I. and {\noopsort{zwaag}}{van der Zwaag}, Sybrand and Bessa, Miguel A.},
  year = {2021},
  month = jun,
  journal = {Quantum Machine Intelligence},
  volume = {3},
  number = {1},
  pages = {6},
  issn = {2524-4906, 2524-4914},
  doi = {10.1007/s42484-020-00032-8},
  abstract = {Gaussian processes are well-established Bayesian machine learning algorithms with significant merits, despite a strong limitation: lack of scalability. Clever solutions address this issue by inducing sparsity through low-rank approximations, often based on the Nystrom method. Here, we propose a different method to achieve better scalability and higher accuracy using quantum computing, outperforming classical Bayesian neural networks for large datasets significantly. Unlike other approaches to quantum machine learning, the computationally expensive linear algebra operations are not just replaced with their quantum counterparts. Instead, we start from a recent study that proposed a quantum circuit for implementing quantum Gaussian processes and then we use quantum phase estimation to induce a low-rank approximation analogous to that in classical sparse Gaussian processes. We provide evidence through numerical tests, mathematical error bound estimation, and complexity analysis that the method can address the ``curse of dimensionality,'' where each additional input parameter no longer leads to an exponential growth of the computational cost. This is also demonstrated by applying the algorithm in a practical setting and using it in the data-driven design of a recently proposed metamaterial. The algorithm, however, requires significant quantum computing hardware improvements before quantum advantage can be achieved.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/TAZWJ8DZ/KuÅ› et al. - 2021 - Sparse quantum Gaussian processes to counter the c.pdf}
}

@article{kuzborskij,
  title = {Stability and {{Hypothesis Transfer Learning}}},
  author = {Kuzborskij, Ilja and Orabona, Francesco},
  pages = {9},
  abstract = {We consider the transfer learning scenario, where the learner does not have access to the source domain directly, but rather operates on the basis of hypotheses induced from it \textendash{} the Hypothesis Transfer Learning (HTL) problem. Particularly, we conduct a theoretical analysis of HTL by considering the algorithmic stability of a class of HTL algorithms based on Regularized Least Squares with biased regularization. We show that the relatedness of source and target domains accelerates the convergence of the Leave-OneOut error to the generalization error, thus enabling the use of the Leave-One-Out error to find the optimal transfer parameters, even in the presence of a small training set. In case of unrelated domains we also suggest a theoretically principled way to prevent negative transfer, so that in the limit we recover the performance of the algorithm not using any knowledge from the source domain.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/PMMCRNXW/Kuzborskij and Orabona - Stability and Hypothesis Transfer Learning.pdf}
}

@article{kuzborskij2017,
  title = {Nonparametric {{Online Regression}} While {{Learning}} the {{Metric}}},
  author = {Kuzborskij, Ilja and {Cesa-Bianchi}, Nicol{\`o}},
  year = {2017},
  month = oct,
  journal = {arXiv:1705.07853 [cs]},
  eprint = {1705.07853},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix G of the regression function (automatically adapting to the effective rank of this matrix), while simultaneously bounding the regret \textemdash on the same data sequence\textemdash{} in terms of the spectrum of G. As a preliminary step in our analysis, we extend a nonparametric online learning algorithm by Hazan and Megiddo enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/YZNLTRZK/Kuzborskij and Cesa-Bianchi - 2017 - Nonparametric Online Regression while Learning the.pdf}
}

@article{kuzborskij2017a,
  title = {Scalable {{Greedy Algorithms}} for {{Transfer Learning}}},
  author = {Kuzborskij, Ilja and Orabona, Francesco and Caputo, Barbara},
  year = {2017},
  month = mar,
  journal = {Computer Vision and Image Understanding},
  volume = {156},
  eprint = {1408.1292},
  eprinttype = {arxiv},
  pages = {174--185},
  issn = {10773142},
  doi = {10.1016/j.cviu.2016.09.003},
  abstract = {In this paper we consider the binary transfer learning problem, focusing on how to select and combine sources from a large pool to yield a good performance on a target task. Constraining our scenario to real world, we do not assume the direct access to the source data, but rather we employ the source hypotheses trained from them. We propose an efficient algorithm that selects relevant source hypotheses and feature dimensions simultaneously, building on the literature on the best subset selection problem. Our algorithm achieves state-of-the-art results on three computer vision datasets, substantially outperforming both transfer learning and popular feature selection baselines in a small-sample setting. We also present a randomized variant that achieves the same results with the computational cost independent from the number of source hypotheses and feature dimensions. Also, we theoretically prove that, under reasonable assumptions on the source hypotheses, our algorithm can learn effectively from few examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/Y6XNB897/Kuzborskij et al. - 2017 - Scalable Greedy Algorithms for Transfer Learning.pdf}
}

@article{lake2016,
  title = {Building {{Machines That Learn}} and {{Think Like People}}},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2016},
  month = nov,
  journal = {arXiv:1604.00289 [cs, stat]},
  eprint = {1604.00289},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/DIX8MBPY/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf}
}

@misc{lake2019,
  title = {The {{Omniglot}} Challenge: A 3-Year Progress Report},
  shorttitle = {The {{Omniglot}} Challenge},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  year = {2019},
  month = may,
  number = {arXiv:1902.03477},
  eprint = {1902.03477},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Three years ago, we released the Omniglot dataset for one-shot learning, along with five challenge tasks and a computational model that addresses these tasks. The model was not meant to be the final word on Omniglot; we hoped that the community would build on our work and develop new approaches. In the time since, we have been pleased to see wide adoption of the dataset. There has been notable progress on one-shot classification, but researchers have adopted new splits and procedures that make the task easier. There has been less progress on the other four tasks. We conclude that recent approaches are still far from human-like concept learning on Omniglot, a challenge that requires performing many tasks with a single model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/VC4H2BTK/Lake et al. - 2019 - The Omniglot challenge a 3-year progress report.pdf}
}

@inproceedings{lam2015,
  title = {Numba: A {{LLVM-based Python JIT}} Compiler},
  shorttitle = {Numba},
  booktitle = {Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}} - {{LLVM}} '15},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  year = {2015},
  pages = {1--6},
  publisher = {{ACM Press}},
  address = {{Austin, Texas}},
  doi = {10.1145/2833157.2833162},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
  isbn = {978-1-4503-4005-2},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/LHPFW6BZ/Lam et al. - 2015 - Numba a LLVM-based Python JIT compiler.pdf}
}

@article{landolfo,
  title = {{{ECCS}} \textendash{} {{SCI EUROCODE DESIGN MANUALS}}},
  author = {Landolfo, Raffaele and Mazzolani, Federico and Dubina, Dan},
  pages = {505},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MYY853YR/Landolfo et al. - ECCS â€“ SCI EUROCODE DESIGN MANUALS.pdf}
}

@book{langtangen2016,
  title = {Solving {{PDEs}} in {{Python}}},
  author = {Langtangen, Hans Petter and Logg, Anders},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-52462-7},
  isbn = {978-3-319-52461-0 978-3-319-52462-7},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/8KCIRADS/Langtangen and Logg - 2016 - Solving PDEs in Python.pdf}
}

@book{larson2013,
  title = {The {{Finite Element Method}}: {{Theory}}, {{Implementation}}, and {{Applications}}},
  shorttitle = {The {{Finite Element Method}}},
  author = {Larson, Mats G. and Bengzon, Fredrik},
  year = {2013},
  series = {Texts in {{Computational Science}} and {{Engineering}}},
  volume = {10},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33287-6},
  isbn = {978-3-642-33286-9 978-3-642-33287-6},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/CPUMZR6T/Larson and Bengzon - 2013 - The Finite Element Method Theory, Implementation,.pdf}
}

@inproceedings{lawrence2004,
  title = {Learning to Learn with the Informative Vector Machine},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Lawrence, Neil D. and Platt, John C.},
  year = {2004},
  pages = {65},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015382},
  abstract = {This paper describes an e cient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An e cient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more e cient than random sub-sampling on an arti cial data-set and more e ective than the traditional IVM in a speaker dependent phoneme recognition task.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/LEIENNRV/Lawrence and Platt - 2004 - Learning to learn with the informative vector mach.pdf}
}

@article{lee2021,
  title = {On the {{Redundancy}} in the {{Rank}} of {{Neural Network Parameters}} and {{Its Controllability}}},
  author = {Lee, Chanhee and Kim, Young-Bum and Ji, Hyesung and Lee, Yeonsoo and Hur, Yuna and Lim, Heuiseok},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {2},
  pages = {725},
  issn = {2076-3417},
  doi = {10.3390/app11020725},
  abstract = {In this paper, we show that parameters of a neural network can have redundancy in their ranks, both theoretically and empirically. When viewed as a function from one space to another, neural networks can exhibit feature correlation and slower training due to this redundancy. Motivated by this, we propose a novel regularization method to reduce the redundancy in the rank of parameters. It is a combination of an objective function that makes the parameter rank-deficient and a dynamic low-rank factorization algorithm that gradually reduces the size of this parameter by fusing linearly dependent vectors together. This regularization-by-pruning approach leads to a neural network with better training dynamics and fewer trainable parameters. We also present experimental results that verify our claims. When applied to a neural network trained to classify images, this method provides statistically significant improvement in accuracy and 7.1 times speedup in terms of number of steps required for training. Furthermore, this approach has the side benefit of reducing the network size, which led to a model with 30.65\% fewer trainable parameters.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/WFM2NRQH/Lee et al. - 2021 - On the Redundancy in the Rank of Neural Network Pa.pdf}
}

@article{li2017,
  title = {Deeper, {{Broader}} and {{Artier Domain Generalization}}},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.03077 [cs]},
  eprint = {1710.03077},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/taylanot/Zotero/storage/Y4IRJMG2/Li et al. - 2017 - Deeper, Broader and Artier Domain Generalization.pdf}
}

@article{li2017a,
  title = {Learning without {{Forgetting}}},
  author = {Li, Zhizhong and Hoiem, Derek},
  year = {2017},
  month = feb,
  journal = {arXiv:1606.09282 [cs, stat]},
  eprint = {1606.09282},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/NX939SC7/Li and Hoiem - 2017 - Learning without Forgetting.pdf}
}

@article{li2017b,
  title = {Meta-{{SGD}}: {{Learning}} to {{Learn Quickly}} for {{Few-Shot Learning}}},
  shorttitle = {Meta-{{SGD}}},
  author = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  year = {2017},
  month = sep,
  journal = {arXiv:1707.09835 [cs]},
  eprint = {1707.09835},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/TBFP8SGF/Li et al. - 2017 - Meta-SGD Learning to Learn Quickly for Few-Shot L.pdf}
}

@misc{li2017c,
  title = {Meta-{{SGD}}: {{Learning}} to {{Learn Quickly}} for {{Few-Shot Learning}}},
  shorttitle = {Meta-{{SGD}}},
  author = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  year = {2017},
  month = sep,
  number = {arXiv:1707.09835},
  eprint = {1707.09835},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/WB9DYX4V/Li et al. - 2017 - Meta-SGD Learning to Learn Quickly for Few-Shot L.pdf}
}

@article{li2018,
  title = {A {{Transfer Learning Approach}} for {{Microstructure Reconstruction}} and {{Structure-property Predictions}}},
  author = {Li, Xiaolin and Zhang, Yichi and Zhao, He and Burkhart, Craig and Brinson, L. Catherine and Chen, Wei},
  year = {2018},
  month = dec,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {13461},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-31571-7},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/DUD962EE/Li et al. - 2018 - A Transfer Learning Approach for Microstructure Re.pdf}
}

@article{li2020,
  title = {A {{Concise Review}} of {{Recent Few-shot Meta-learning Methods}}},
  author = {Li, Xiaoxu and Sun, Zhuo and Xue, Jing-Hao and Ma, Zhanyu},
  year = {2020},
  month = may,
  journal = {arXiv:2005.10953 [cs, stat]},
  eprint = {2005.10953},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Few-shot meta-learning has been recently reviving with expectations to mimic humanity's fast adaption to new concepts based on prior knowledge. In this short communication, we give a concise review on recent representative methods in few-shot meta-learning, which are categorized into four branches according to their technical characteristics. We conclude this review with some vital current challenges and future prospects in few-shot meta-learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/DNATAAC4/Li et al. - 2020 - A Concise Review of Recent Few-shot Meta-learning .pdf}
}

@article{li2020a,
  title = {Towards {{Explaining}} the {{Regularization Effect}} of {{Initial Large Learning Rate}} in {{Training Neural Networks}}},
  author = {Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  year = {2020},
  month = apr,
  journal = {arXiv:1907.04595 [cs, stat]},
  eprint = {1907.04595},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Stochastic gradient descent with a large initial learning rate is widely used for training modern neural net architectures. Although a small initial learning rate allows for faster training and better test performance initially, the large learning rate achieves better generalization soon after the learning rate is annealed. Towards explaining this phenomenon, we devise a setting in which we can prove that a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start. The key insight in our analysis is that the order of learning different types of patterns is crucial: because the small learning rate model first memorizes easy-to-generalize, hard-to-fit patterns, it generalizes worse on hard-to-generalize, easier-to-fit patterns than its large learning rate counterpart. This concept translates to a larger-scale setting: we demonstrate that one can add a small patch to CIFAR-10 images that is immediately memorizable by a model with small initial learning rate, but ignored by the model with large learning rate until after annealing. Our experiments show that this causes the small learning rate model's accuracy on unmodified images to suffer, as it relies too much on the patch early on.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/GISY28PK/Li et al. - 2020 - Towards Explaining the Regularization Effect of In.pdf}
}

@article{linka2021,
  title = {Constitutive Artificial Neural Networks: {{A}} Fast and General Approach to Predictive Data-Driven Constitutive Modeling by Deep Learning},
  shorttitle = {Constitutive Artificial Neural Networks},
  author = {Linka, Kevin and Hillg{\"a}rtner, Markus and Abdolazizi, Kian P. and Aydin, Roland C. and Itskov, Mikhail and Cyron, Christian J.},
  year = {2021},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {429},
  pages = {110010},
  issn = {00219991},
  doi = {10.1016/j.jcp.2020.110010},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/626VS3M8/Linka et al. - 2021 - Constitutive artificial neural networks A fast an.pdf}
}

@article{liu2016,
  title = {Self-Consistent Clustering Analysis: {{An}} Efficient Multi-Scale Scheme for Inelastic Heterogeneous Materials},
  shorttitle = {Self-Consistent Clustering Analysis},
  author = {Liu, Zeliang and Bessa, M.A. and Liu, Wing Kam},
  year = {2016},
  month = jul,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {306},
  pages = {319--341},
  issn = {00457825},
  doi = {10.1016/j.cma.2016.04.004},
  abstract = {The discovery of efficient and accurate descriptions for the macroscopic behavior of materials with complex microstructure is an outstanding challenge in mechanics of materials. A mechanistic, data-driven, two-scale approach is developed for predicting the behavior of general heterogeneous materials under irreversible processes such as inelastic deformation. The proposed approach includes two major innovations: (1) the use of a data compression algorithm, k-means clustering, during the offline stage of the method to homogenize the local features of the material microstructure into a group of clusters; and (2) a new method called self-consistent clustering analysis used in the online stage that is valid for any local plasticity laws of each material phase without the need for additional calibration. A particularly important feature of the proposed approach is that the offline stage only uses the linear elastic properties of each material phase, making it efficient. This work is believed to open new avenues in parameter-free multi-scale modeling of complex materials, and perhaps in other fields that require homogenization of irreversible processes.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/D3DEZCKG/Liu et al. - 2016 - Self-consistent clustering analysis An efficient .pdf}
}

@article{liu2018,
  title = {Microstructural Material Database for Self-Consistent Clustering Analysis of Elastoplastic Strain Softening Materials},
  author = {Liu, Zeliang and Fleming, Mark and Liu, Wing Kam},
  year = {2018},
  month = mar,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {330},
  pages = {547--577},
  issn = {00457825},
  doi = {10.1016/j.cma.2017.11.005},
  abstract = {Multiscale modeling of heterogeneous material undergoing strain softening poses computational challenges for localization of the microstructure, material instability in the macrostructure, and the computational requirement for accurate and efficient concurrent calculation. In the paper, a stable micro-damage homogenization algorithm is presented which removes the material instability issues in the microstructure with representative volume elements (RVE) that are not sensitive to size when computing the homogenized stress-strain response.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/CL68L798/Liu et al. - 2018 - Microstructural material database for self-consist.pdf}
}

@article{liu2018a,
  title = {Rotate Your {{Networks}}: {{Better Weight Consolidation}} and {{Less Catastrophic Forgetting}}},
  shorttitle = {Rotate Your {{Networks}}},
  author = {Liu, Xialei and Masana, Marc and Herranz, Luis and {Van de Weijer}, Joost and Lopez, Antonio M. and Bagdanov, Andrew D.},
  year = {2018},
  month = dec,
  journal = {arXiv:1802.02950 [cs]},
  eprint = {1802.02950},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to the state-of-the-art in lifelong learning without forgetting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/taylanot/Zotero/storage/C7ZPXK9Q/Liu et al. - 2018 - Rotate your Networks Better Weight Consolidation .pdf}
}

@article{liu2019,
  title = {A Deep Material Network for Multiscale Topology Learning and Accelerated Nonlinear Modeling of Heterogeneous Materials},
  author = {Liu, Zeliang and Wu, C.T. and Koishi, M.},
  year = {2019},
  month = mar,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {345},
  pages = {1138--1168},
  issn = {00457825},
  doi = {10.1016/j.cma.2018.09.020},
  abstract = {In this paper, a new data-driven multiscale material modeling method, which we refer to as deep material network, is developed based on mechanistic homogenization theory of representative volume element (RVE) and advanced machine learning techniques. We propose to use a collection of connected mechanistic building blocks with analytical homogenization solutions to describe complex overall material responses which avoids the loss of essential physics in generic neural network. This concept is demonstrated for 2-dimensional RVE problems and network depth up to 7. Based on linear elastic RVE data from offline direct numerical simulations, the material network can be effectively trained using stochastic gradient descent with backpropagation algorithm, further enhanced by model compression methods. Importantly, the trained network is valid for any local material laws without the need for additional calibration or micromechanics assumption. Its extrapolations to unknown material and loading spaces for a wide range of problems are validated through numerical experiments, including linear elasticity with high contrast of phase properties, nonlinear history-dependent plasticity and finite-strain hyperelasticity under large deformations.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/6TEZPN9N/Liu et al. - 2019 - A deep material network for multiscale topology le.pdf}
}

@article{liu2019a,
  title = {Transfer Learning of Deep Material Network for Seamless Structure\textendash Property Predictions},
  author = {Liu, Zeliang and Wu, C. T. and Koishi, M.},
  year = {2019},
  month = aug,
  journal = {Computational Mechanics},
  volume = {64},
  number = {2},
  pages = {451--465},
  issn = {0178-7675, 1432-0924},
  doi = {10.1007/s00466-019-01704-4},
  abstract = {Modern materials design requires reliable and consistent structure\textendash property relationships. The paper addresses the need through transfer learning of deep material network (DMN). In the proposed learning strategy, we store the knowledge of a pre-trained network and reuse it to generate the initial structure for a new material via a naive approach. Significant improvements in the training accuracy and learning convergence are attained. Since all the databases share the same base network structure, their fitting parameters can be interpolated to seamlessly create intermediate databases. The new transferred models are shown to outperform the analytical micromechanics methods in predicting the volume fraction effects. We then apply the unified DMN databases to the design of failure properties, where the failure criteria are defined upon the distribution of microscale plastic strains. The Pareto frontier of toughness and ultimate tensile strength is extracted from a large-scale design space enabled by the efficiency of DMN extrapolation.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/DZKXDYX3/Liu et al. - 2019 - Transfer learning of deep material network for sea.pdf}
}

@article{liu2020,
  title = {Intelligent Multiscale Simulation Based on Process-Guided Composite Database},
  author = {Liu, Zeliang and Wei, Haoyan and Huang, Tianyu and Wu, C T},
  year = {2020},
  pages = {15},
  abstract = {In the paper, we present an integrated data-driven modeling framework based on process modeling, material homogenization, mechanistic machine learning, and concurrent multiscale simulation. We are interested in the injection-molded short fiber reinforced composites, which have been identified as key material systems in automotive, aerospace, and electronics industries. The molding process induces spatially varying microstructures across various length scales, while the resulting strongly anisotropic and nonlinear material properties are still challenging to be captured by conventional modeling approaches. To prepare the linear elastic training data for our machine learning tasks, Representative Volume Elements (RVE) with different fiber orientations and volume fractions are generated through stochastic reconstruction. More importantly, we utilize the recently proposed Deep Material Network (DMN) to learn the hidden microscale morphologies from data. With essential physics embedded in its building blocks, this data-driven material model can be extrapolated to predict nonlinear material behaviors efficiently and accurately. Through the transfer learning of DMN, we create a unified process-guided material database that covers a full range of geometric descriptors for short fiber reinforced composites. Finally, this unified DMN database is implemented and coupled with macroscale finite element model to enable concurrent multiscale simulations. From our perspective, the proposed framework is also promising in many other emergent multiscale engineering systems, such as additive manufacturing and compressive molding.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/43AXFNVJ/Liu et al. - 2020 - Intelligent multiscale simulation based on process.pdf}
}

@article{liu2020a,
  title = {Meta-Learning {{Transferable Representations}} with a {{Single Target Domain}}},
  author = {Liu, Hong and HaoChen, Jeff Z. and Wei, Colin and Ma, Tengyu},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01418 [cs]},
  eprint = {2011.01418},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent works found that fine-tuning and joint training\textemdash two popular approaches for transfer learning\textemdash do not always improve accuracy on downstream tasks. First, we aim to understand more about when and why fine-tuning and joint training can be suboptimal or even harmful for transfer learning. We design semi-synthetic datasets where the source task can be solved by either source-specific features or transferable features. We observe that (1) pre-training may not have incentive to learn transferable features and (2) joint training may simultaneously learn source-specific features and overfit to the target. Second, to improve over fine-tuning and joint training, we propose Meta Representation Learning (MeRLin) to learn transferable features. MeRLin meta-learns representations by ensuring that a head fit on top of the representations with target training data also performs well on target validation data. We also prove that MeRLin recovers the target ground-truth model with a quadratic neural net parameterization and a source distribution that contains both transferable and source-specific features. On the same distribution, pre-training and joint training provably fail to learn transferable features. MeRLin empirically outperforms previous state-of-the-art transfer learning algorithms on various real-world vision and NLP transfer learning benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/FAJTFSBV/Liu et al. - 2020 - Meta-learning Transferable Representations with a .pdf}
}

@article{liu2020b,
  title = {When Does {{MAML Work}} the {{Best}}? {{An Empirical Study}} on {{Model-Agnostic Meta-Learning}} in {{NLP Applications}}},
  shorttitle = {When Does {{MAML Work}} the {{Best}}?},
  author = {Liu, Zequn and Zhang, Ruiyi and Song, Yiping and Zhang, Ming},
  year = {2020},
  month = may,
  journal = {arXiv:2005.11700 [cs]},
  eprint = {2005.11700},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Model-Agnostic Meta-Learning (MAML), a model-agnostic meta-learning method, is successfully employed in NLP applications including few-shot text classification and multi-domain lowresource language generation. Many impacting factors, including data quantity, similarity among tasks, and the balance between general language model and task-specific adaptation, can affect the performance of MAML in NLP, but few works have thoroughly studied them. In this paper, we conduct an empirical study to investigate these impacting factors and conclude when MAML works the best based on the experimental results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/taylanot/Zotero/storage/BHRNTSYC/Liu et al. - 2020 - When does MAML Work the Best An Empirical Study o.pdf}
}

@article{liu2021,
  title = {Knowledge Extraction and Transfer in Data-Driven Fracture Mechanics},
  author = {Liu, Xing and Athanasiou, Christos E. and Padture, Nitin P. and Sheldon, Brian W. and Gao, Huajian},
  year = {2021},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {23},
  pages = {e2104765118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2104765118},
  abstract = {Significance             Data-driven approaches have launched a new paradigm in scientific research that is bound to have an impact on all disciplines of science and engineering. However, at this juncture, the exploration of data-driven techniques in the century-old field of fracture mechanics is highly limited, and there are key challenges including accurate and intelligent knowledge extraction and transfer in a data-limited regime. Here, we propose a framework for data-driven knowledge extraction in fracture mechanics with rigorous accuracy assessment which employs active learning for optimizing data usage and for data-driven knowledge transfer that allows efficient treatment of three-dimensional fracture problems based on two-dimensional solutions.           ,              Data-driven approaches promise to usher in a new phase of development in fracture mechanics, but very little is currently known about how data-driven knowledge extraction and transfer can be accomplished in this field. As in many other fields, data scarcity presents a major challenge for knowledge extraction, and knowledge transfer among different fracture problems remains largely unexplored. Here, a data-driven framework for knowledge extraction with rigorous metrics for accuracy assessments is proposed and demonstrated through a nontrivial linear elastic fracture mechanics problem encountered in small-scale toughness measurements. It is shown that a tailored active learning method enables accurate knowledge extraction even in a data-limited regime. The viability of knowledge transfer is demonstrated through mining the hidden connection between the selected three-dimensional benchmark problem and a well-established auxiliary two-dimensional problem. The combination of data-driven knowledge extraction and transfer is expected to have transformative impact in this field over the coming decades.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/PH4XDCH3/Liu et al. - 2021 - Knowledge extraction and transfer in data-driven f.pdf}
}

@book{logg2012,
  title = {Automated {{Solution}} of {{Differential Equations}} by the {{Finite Element Method}}},
  editor = {Logg, Anders and Mardal, Kent-Andre and Wells, Garth},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computational Science}} and {{Engineering}}},
  volume = {84},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23099-8},
  isbn = {978-3-642-23098-1 978-3-642-23099-8},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/QIU92M2R/Logg et al. - 2012 - Automated Solution of Differential Equations by th.pdf}
}

@inproceedings{long2013,
  title = {Transfer {{Feature Learning}} with {{Joint Distribution Adaptation}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Sun, Jiaguang and Yu, Philip S.},
  year = {2013},
  month = dec,
  pages = {2200--2207},
  publisher = {{IEEE}},
  address = {{Sydney, Australia}},
  doi = {10.1109/ICCV.2013.274},
  abstract = {Transfer learning is established as an effective technology in computer vision for leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robust for substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.},
  isbn = {978-1-4799-2840-8},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/ZERYNNNL/Long et al. - 2013 - Transfer Feature Learning with Joint Distribution .pdf}
}

@article{loog,
  title = {Minimizers of the {{Empirical Risk}} and {{Risk Monotonicity}}},
  author = {Loog, Marco and Viering, Tom and Mey, Alexander},
  pages = {10},
  abstract = {Plotting a learner's average performance against the number of training samples results in a learning curve. Studying such curves on one or more data sets is a way to get to a better understanding of the generalization properties of this learner. The behavior of learning curves is, however, not very well understood and can display (for most researchers) quite unexpected behavior. Our work introduces the formal notion of risk monotonicity, which asks the risk to not deteriorate with increasing training set sizes in expectation over the training samples. We then present the surprising result that various standard learners, specifically those that minimize the empirical risk, can act nonmonotonically irrespective of the training sample size. We provide a theoretical underpinning for specific instantiations from classification, regression, and density estimation. Altogether, the proposed monotonicity notion opens up a whole new direction of research.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/KEPGNJ5U/Loog et al. - Minimizers of the Empirical Risk and Risk Monotoni.pdf}
}

@incollection{loog2012,
  title = {The {{Dipping Phenomenon}}},
  booktitle = {Structural, {{Syntactic}}, and {{Statistical Pattern Recognition}}},
  author = {Loog, Marco and Duin, Robert P. W.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gimel'farb, Georgy and Hancock, Edwin and Imiya, Atsushi and Kuijper, Arjan and Kudo, Mineichi and Omachi, Shinichiro and Windeatt, Terry and Yamada, Keiji},
  year = {2012},
  volume = {7626},
  pages = {310--317},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-34166-3_34},
  abstract = {One typically expects classifiers to demonstrate improved performance with increasing training set sizes or at least to obtain their best performance in case one has an infinite number of training samples at ones's disposal. We demonstrate, however, that there are classification problems on which particular classifiers attain their optimum performance at a training set size which is finite. Whether or not this phenomenon, which we term dipping, can be observed depends on the choice of classifier in relation to the underlying class distributions. We give some simple examples, for a few classifiers, that illustrate how the dipping phenomenon can occur. Additionally, we speculate about what generally is needed for dipping to emerge. What is clear is that this kind of learning curve behavior does not emerge due to mere chance and that the pattern recognition practitioner ought to take note of it.},
  isbn = {978-3-642-34165-6 978-3-642-34166-3},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/PI3X8EVA/Loog and Duin - 2012 - The Dipping Phenomenon.pdf}
}

@article{loog2016,
  title = {Contrastive {{Pessimistic Likelihood Estimation}} for {{Semi-Supervised Classification}}},
  author = {Loog, Marco},
  year = {2016},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {38},
  number = {3},
  pages = {462--475},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2015.2452921},
  abstract = {Improvement guarantees for semi-supervised classifiers can currently only be given under restrictive conditions on the data. We propose a general way to perform semi-supervised parameter estimation for likelihood-based classifiers for which, on the full training set, the estimates are never worse than the supervised solution in terms of the log-likelihood. We argue, moreover, that we may expect these solutions to really improve upon the supervised classifier in particular cases. In a worked-out example for LDA, we take it one step further and essentially prove that its semi-supervised version is strictly better than its supervised counterpart. The two new concepts that form the core of our estimation principle are contrast and pessimism. The former refers to the fact that our objective function takes the supervised estimates into account, enabling the semi-supervised solution to explicitly control the potential improvements over this estimate. The latter refers to the fact that our estimates are conservative and therefore resilient to whatever form the true labeling of the unlabeled data takes on. Experiments demonstrate the improvements in terms of both the log-likelihood and the classification error rate on independent test sets.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/Y53ZM9CY/Loog - 2016 - Contrastive Pessimistic Likelihood Estimation for .pdf}
}

@article{loog2017,
  title = {Supervised {{Classification}}: {{Quite}} a {{Brief Overview}}},
  shorttitle = {Supervised {{Classification}}},
  author = {Loog, Marco},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.09230 [cs, stat]},
  eprint = {1710.09230},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The original problem of supervised classification considers the task of automatically assigning objects to their respective classes on the basis of numerical measurements derived from these objects. Classifiers are the tools that implement the actual functional mapping from these measurements\textemdash also called features or inputs\textemdash to the so-called class label\textemdash or output. The fields of pattern recognition and machine learning study ways of constructing such classifiers. The main idea behind supervised methods is that of learning from examples: given a number of example input-output relations, to what extent can the general mapping be learned that takes any new and unseen feature vector to its correct class? This chapter provides a basic introduction to the underlying ideas of how to come to a supervised classification problem. In addition, it provides an overview of some specific classification techniques, delves into the issues of object representation and classifier evaluation, and (very) briefly covers some variations on the basic supervised classification task that may also be of interest to the practitioner.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/IQW4JILT/Loog - 2017 - Supervised Classification Quite a Brief Overview.pdf}
}

@article{lu2021,
  title = {A {{Stochastic FE2 Data-Driven Method}} for {{Nonlinear Multiscale Modeling}}},
  author = {Lu, Xiaoxin and Yvonnet, Julien and Papadopoulos, Leonidas and Kalogeris, Ioannis and Papadopoulos, Vissarion},
  year = {2021},
  month = may,
  journal = {Materials},
  volume = {14},
  number = {11},
  pages = {2875},
  issn = {1996-1944},
  doi = {10.3390/ma14112875},
  abstract = {A stochastic data-driven multilevel finite-element (FE2) method is introduced for random nonlinear multiscale calculations. A hybrid neural-network\textendash interpolation (NN\textendash I) scheme is proposed to construct a surrogate model of the macroscopic nonlinear constitutive law from representativevolume-element calculations, whose results are used as input data. Then, a FE2 method replacing the nonlinear multiscale calculations by the NN\textendash I is developed. The NN\textendash I scheme improved the accuracy of the neural-network surrogate model when insufficient data were available. Due to the achieved reduction in computational time, which was several orders of magnitude less than that to direct FE2, the use of such a machine-learning method is demonstrated for performing Monte Carlo simulations in nonlinear heterogeneous structures and propagating uncertainties in this context, and the identification of probabilistic models at the macroscale on some quantities of interest. Applications to nonlinear electric conduction in graphene\textendash polymer composites are presented.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/7DEWNKVG/Lu et al. - 2021 - A Stochastic FE2 Data-Driven Method for Nonlinear .pdf;/home/taylanot/Zotero/storage/PKNDZPU9/Lu et al. - 2021 - A Stochastic FE2 Data-Driven Method for Nonlinear .pdf}
}

@article{lu2021a,
  title = {{{DeepONet}}: {{Learning}} Nonlinear Operators for Identifying Differential Equations Based on the Universal Approximation Theorem of Operators},
  shorttitle = {{{DeepONet}}},
  author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  year = {2021},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {3},
  eprint = {1910.03193},
  eprinttype = {arxiv},
  pages = {218--229},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00302-5},
  abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator [5]. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks (DeepONets) to learn operators accurately and efficiently from a relatively small dataset. A DeepONet consists of two sub-networks, one for encoding the input function at a fixed number of sensors xi, i = 1, . . . , m (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that DeepONet significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/6YCTZ8ML/Lu et al. - 2021 - DeepONet Learning nonlinear operators for identif.pdf;/home/taylanot/Zotero/storage/CEQTTP2Q/Lu et al. - 2021 - Learning nonlinear operators via DeepONet based on.pdf}
}

@inproceedings{luo2019,
  title = {Taking a {{Closer Look}} at {{Domain Shift}}: {{Category-Level Adversaries}} for {{Semantics Consistent Domain Adaptation}}},
  shorttitle = {Taking a {{Closer Look}} at {{Domain Shift}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Luo, Yawei and Zheng, Liang and Guan, Tao and Yu, Junqing and Yang, Yi},
  year = {2019},
  month = jun,
  pages = {2502--2511},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00261},
  abstract = {We consider the problem of unsupervised domain adaptation in semantic segmentation. A key in this campaign consists in reducing the domain shift, i.e., enforcing the data distributions of the two domains to be similar. One of the common strategies is to align the marginal distribution in the feature space through adversarial learning. However, this global alignment strategy does not consider the category-level joint distribution. A possible consequence of such global movement is that some categories which are originally well aligned between the source and target may be incorrectly mapped, thus leading to worse segmentation results in target domain. To address this problem, we introduce a category-level adversarial network, aiming to enforce local semantic consistency during the trend of global alignment. Our idea is to take a close look at the category-level joint distribution and align each class with an adaptive adversarial loss. Specifically, we reduce the weight of the adversarial loss for category-level aligned features while increasing the adversarial force for those poorly aligned. In this process, we decide how well a feature is category-level aligned between source and target by a co-training approach. In two domain adaptation tasks, i.e., GTA5 \textrightarrow{} Cityscapes and SYNTHIA \textrightarrow{} Cityscapes, we validate that the proposed method matches the state of the art in segmentation accuracy.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/QGRWJWL5/Luo et al. - 2019 - Taking a Closer Look at Domain Shift Category-Lev.pdf}
}

@article{mackay,
  title = {Bayesian {{Methods}} for {{Adaptive Models}}},
  author = {MacKay, David J C},
  pages = {98},
  abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non\textendash linear models. This framework quantitatively embodies `Occam's razor'. Over\textendash complex and under\textendash regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/C25G7U5R/MacKay - Bayesian Methods for Adaptive Models.pdf}
}

@misc{martinez2016,
  title = {Biased {{Roulette Wheel}}: {{A Quantitative Trading Strategy Approach}}},
  shorttitle = {Biased {{Roulette Wheel}}},
  author = {Mart{\'i}nez, Giancarlo Salirrosas},
  year = {2016},
  month = sep,
  number = {arXiv:1609.09601},
  eprint = {1609.09601},
  eprinttype = {arxiv},
  primaryclass = {q-fin},
  publisher = {{arXiv}},
  abstract = {The purpose of this research paper it is to present a new approach in the framework of a biased roulette wheel. It is used the approach of a quantitative trading strategy, commonly used in quantitative finance, in order to assess the profitability of the strategy in the short term. The tools of backtesting and walk-forward optimization were used to achieve such task. The data has been generated from a real European roulette wheel from an on-line casino based in Riga, Latvia. It has been recorded 10,980 spins1 and sent to the computer through a voice-to-text software for further numerical analysis in R. It has been observed that the probabilities of occurrence of the numbers at the roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown that a flat betting system against Kelly Criterion was more profitable in the short term.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Finance - Computational Finance,Quantitative Finance - General Finance},
  file = {/home/taylanot/Zotero/storage/HGJFEWTJ/MartÃ­nez - 2016 - Biased Roulette Wheel A Quantitative Trading Stra.pdf}
}

@book{martins2021,
  title = {Engineering {{Design Optimization}}},
  author = {Martins, Joaquim R. R. A. and Ning, Andrew},
  year = {2021},
  month = nov,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108980647},
  abstract = {Based on course-tested material, this rigorous yet accessible graduate textbook covers both fundamental and advanced optimization theory and algorithms. It covers a wide range of numerical methods and topics, including both gradient-based and gradient-free algorithms, multidisciplinary design optimization, and uncertainty, with instruction on how to determine which algorithm should be used for a given application. It also provides an overview of models and how to prepare them for use with numerical optimization, including derivative computation. Over 400 high-quality visualizations and numerous examples facilitate understanding of the theory, and practical tips address common issues encountered in practical engineering design optimization and how to address them. Numerous end-of-chapter homework problems, progressing in difficulty, help put knowledge into practice. Accompanied online by a solutions manual for instructors and source code for problems, this is ideal for a one- or two-semester graduate course on optimization in aerospace, civil, mechanical, electrical, and chemical engineering departments.},
  isbn = {978-1-108-98064-7 978-1-108-83341-7},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/TH23WAPK/Martins and Ning - 2021 - Engineering Design Optimization.pdf}
}

@article{masi2021,
  title = {Thermodynamics-Based {{Artificial Neural Networks}} for Constitutive Modeling},
  author = {Masi, Filippo and Stefanou, Ioannis and Vannucci, Paolo and {Maffi-Berthier}, Victor},
  year = {2021},
  month = feb,
  journal = {Journal of the Mechanics and Physics of Solids},
  volume = {147},
  eprint = {2005.12183},
  eprinttype = {arxiv},
  pages = {104277},
  issn = {00225096},
  doi = {10.1016/j.jmps.2020.104277},
  abstract = {Machine Learning methods and, in particular, Artificial Neural Networks (ANNs) have demonstrated promising capabilities in material constitutive modeling. One of the main drawbacks of such approaches is the lack of a rigorous frame based on the laws of physics. This may render physically inconsistent the predictions of a trained network, which can be even dangerous for real applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/NJ3YY953/Masi et al. - 2021 - Thermodynamics-based Artificial Neural Networks fo.pdf}
}

@article{maurer,
  title = {Algorithmic {{Stability}} and {{Meta-Learning}}},
  author = {Maurer, Andreas},
  pages = {28},
  abstract = {A mechnism of transfer learning is analysed, where samples drawn from different learning tasks of an environment are used to improve the learners performance on a new task. We give a general method to prove generalisation error bounds for such meta-algorithms. The method can be applied to the bias learning model of J. Baxter and to derive novel generalisation bounds for metaalgorithms searching spaces of uniformly stable algorithms. We also present an application to regularized least squares regression.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/2FCDT86I/Maurer - Algorithmic Stability and Meta-Learning.pdf}
}

@book{meer2010,
  title = {Computational Modeling of Failure in Composite Laminates.},
  author = {{\noopsort{meer}}van der Meer, Frans Paul},
  year = {2010},
  publisher = {{s.n.}},
  address = {{S.l.}},
  isbn = {978-90-90-25697-9},
  langid = {english},
  annotation = {OCLC: 839642040},
  file = {/home/taylanot/Zotero/storage/488W8G9J/Meer - 2010 - Computational modeling of failure in composite lam.pdf}
}

@article{melly2021,
  title = {A Review on Material Models for Isotropic Hyperelasticity},
  author = {Melly, Stephen K. and Liu, Liwu and Liu, Yanju and Leng, Jinsong},
  year = {2021},
  month = sep,
  journal = {International Journal of Mechanical System Dynamics},
  volume = {1},
  number = {1},
  pages = {71--88},
  issn = {2767-1402, 2767-1402},
  doi = {10.1002/msd2.12013},
  abstract = {Dozens of hyperelastic models have been formulated and have been extremely handy in understanding the complex mechanical behavior of materials that exhibit hyperelastic behavior (characterized by large nonlinear elastic deformations that are completely recoverable) such as elastomers, polymers, and even biological tissues. These models are indispensable in the design of complex engineering components such as engine mounts and structural bearings in the automotive and aerospace industries and vibration isolators and shock absorbers in mechanical systems. Particularly, the problem of vibration control in mechanical system dynamics is extremely important and, therefore, knowledge of accurate hyperelastic models facilitates optimum designs and the development of three-dimensional finite element system dynamics for studying the large and nonlinear deformation behavior. This review work intends to enhance the knowledge of 15 of the most commonly used hyperelastic models and consequently help design engineers and scientists make informed decisions on the right ones to use. For each of the models, expressions for the strain-energy function and the Cauchy stress for both arbitrary loading assuming compressibility and each of the three loading modes (uniaxial tension, equibiaxial tension, and pure shear) assuming incompressibility are provided. Furthermore, the stress\textendash strain or stress\textendash stretch plots of the model's predictions in each of the loading modes are compared with that of the classical experimental data of Treloar and the coefficient of determination is utilized as a measure of the model's predictive ability. Lastly, a ranking scheme is proposed based on the model's ability to predict each of the loading modes with minimum deviations and the overall coefficient of determination.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/9BQ8TRUS/Melly et al. - 2021 - A review on material models for isotropic hyperela.pdf}
}

@article{melly2021a,
  title = {A Review on Material Models for Isotropic Hyperelasticity},
  author = {Melly, Stephen K. and Liu, Liwu and Liu, Yanju and Leng, Jinsong},
  year = {2021},
  journal = {International Journal of Mechanical System Dynamics},
  volume = {1},
  number = {1},
  pages = {71--88},
  issn = {2767-1402},
  doi = {10.1002/msd2.12013},
  abstract = {Dozens of hyperelastic models have been formulated and have been extremely handy in understanding the complex mechanical behavior of materials that exhibit hyperelastic behavior (characterized by large nonlinear elastic deformations that are completely recoverable) such as elastomers, polymers, and even biological tissues. These models are indispensable in the design of complex engineering components such as engine mounts and structural bearings in the automotive and aerospace industries and vibration isolators and shock absorbers in mechanical systems. Particularly, the problem of vibration control in mechanical system dynamics is extremely important and, therefore, knowledge of accurate hyperelastic models facilitates optimum designs and the development of three-dimensional finite element system dynamics for studying the large and nonlinear deformation behavior. This review work intends to enhance the knowledge of 15 of the most commonly used hyperelastic models and consequently help design engineers and scientists make informed decisions on the right ones to use. For each of the models, expressions for the strain-energy function and the Cauchy stress for both arbitrary loading assuming compressibility and each of the three loading modes (uniaxial tension, equibiaxial tension, and pure shear) assuming incompressibility are provided. Furthermore, the stress\textendash strain or stress\textendash stretch plots of the model's predictions in each of the loading modes are compared with that of the classical experimental data of Treloar and the coefficient of determination is utilized as a measure of the model's predictive ability. Lastly, a ranking scheme is proposed based on the model's ability to predict each of the loading modes with minimum deviations and the overall coefficient of determination.},
  langid = {english},
  keywords = {constitutive models,finite deformation,hyperelastic,mechanical system dynamics,strain energy density},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/msd2.12013},
  file = {/home/taylanot/Zotero/storage/9NVZBAAJ/Melly et al. - 2021 - A review on material models for isotropic hyperela.pdf;/home/taylanot/Zotero/storage/N2WSFRC9/msd2.html}
}

@article{mendelson2010,
  title = {Regularization in Kernel Learning},
  author = {Mendelson, Shahar and Neeman, Joseph},
  year = {2010},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {38},
  number = {1},
  eprint = {1001.2094},
  eprinttype = {arxiv},
  issn = {0090-5364},
  doi = {10.1214/09-AOS728},
  abstract = {Under mild assumptions on the kernel, we obtain the best known error rates in a regularized learning scenario taking place in the corresponding reproducing kernel Hilbert space (RKHS). The main novelty in the analysis is a proof that one can use a regularization term that grows significantly slower than the standard quadratic growth in the RKHS norm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68Q32 (Primary) 60G99 (Secondary),Mathematics - Statistics Theory},
  file = {/home/taylanot/Zotero/storage/T4IN5GUS/Mendelson and Neeman - 2010 - Regularization in kernel learning.pdf}
}

@article{mendizabal2020,
  title = {Simulation of Hyperelastic Materials in Real-Time Using Deep Learning},
  author = {Mendizabal, Andrea and {M{\'a}rquez-Neila}, Pablo and Cotin, St{\'e}phane},
  year = {2020},
  month = jan,
  journal = {Medical Image Analysis},
  volume = {59},
  pages = {101569},
  issn = {13618415},
  doi = {10.1016/j.media.2019.101569},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/YF3N5EIB/Mendizabal et al. - 2020 - Simulation of hyperelastic materials in real-time .pdf}
}

@article{mianroodi2021,
  title = {Teaching Solid Mechanics to Artificial Intelligence\textemdash a Fast Solver for Heterogeneous Materials},
  author = {Mianroodi, Jaber Rezaei and H. Siboni, Nima and Raabe, Dierk},
  year = {2021},
  month = dec,
  journal = {npj Computational Materials},
  volume = {7},
  number = {1},
  pages = {99},
  issn = {2057-3960},
  doi = {10.1038/s41524-021-00571-z},
  abstract = {Abstract             We propose a deep neural network (DNN) as a fast surrogate model for local stress calculations in inhomogeneous non-linear materials. We show that the DNN predicts the local stresses with 3.8\% mean absolute percentage error (MAPE) for the case of heterogeneous elastic media and a mechanical contrast of up to factor of 1.5 among neighboring domains, while performing 103 times faster than spectral solvers. The DNN model proves suited for reproducing the stress distribution in geometries different from those used for training. In the case of elasto-plastic materials with up to 4 times mechanical contrast in yield stress among adjacent regions, the trained model simulates the micromechanics with a MAPE of 6.4\% in one single forward evaluation of the network, without any iteration. The results reveal an efficient approach to solve non-linear mechanical problems, with an acceleration up to a factor of 8300 for elastic-plastic materials compared to typical solvers.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/VB5W47BU/Mianroodi et al. - 2021 - Teaching solid mechanics to artificial intelligenc.pdf}
}

@article{miehe1999,
  title = {Computational Homogenization Analysis in Finite Plasticity {{Simulation}} of Texture Development in Polycrystalline Materials},
  author = {Miehe, Christian and Schr{\"o}der, J{\"o}rg and Schotte, Jan},
  year = {1999},
  month = apr,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {171},
  number = {3-4},
  pages = {387--418},
  issn = {00457825},
  doi = {10.1016/S0045-7825(98)00218-7},
  abstract = {The paper presents a framework for the treatment of a homogenized macro-continuum with locally attached micro-structure, which undergoes non-isothermal inelastic deformations at large strains. The proposed concept is applied to the simulation of texture evolution in polycrystalline metals, where the micro-structure consists of a representative assembly of single crystal grains. The deformation of this micro-structure is coupled with the local deformation at a typical material point of the macro-continuum by three alternative constraints of the microscopic fluctuation field. In a deformation driven process, extensive macroscopic variables, like stresses and dissipation are defined as volume averages of their microscopic counterparts in an accompanying local equilibrium state of the micro-structure. The proposed numerical implementation is based in the general setting on a finite element discretization of the macro-continuum which is locally coupled at each Gauss point with a finite element discretization of the attached micro-:\textasciitilde tructure. In the first part of the paper we set up the two coupled boundary value problems associated with the macro-continuum and the pointwise attached micro-structure and consider aspects of their finite element solutions. The second part presents details of a robust algorithmic model of finite plasticity for single crystals which governs the response of the grains in a typical micro-structure. The paper concludes with some representative numerical examples by demonstrating the performance of the proposed concept with regard to the prediction of texture evolution in polycrystals. \textcopyright{} 1999 Elsevier Science S.A. All rights reserved.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/U6J7KPX2/Miehe et al. - 1999 - Computational homogenization analysis in finite pl.pdf}
}

@article{miehe2002,
  title = {Homogenization of Inelastic Solid Materials at \"Ynite Strains Based on Incremental Minimization Principles. {{Application}} to the Texture Analysis of Polycrystals},
  author = {Miehe, C and Schotte, J and Lambrecht, M},
  year = {2002},
  journal = {J. Mech. Phys. Solids},
  pages = {45},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/RJ2Q72WM/Miehe et al. - 2002 - Homogenization of inelastic solid materials at Ã¿ni.pdf}
}

@article{mishra2018,
  title = {A {{Simple Neural Attentive Meta-Learner}}},
  author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  year = {2018},
  month = feb,
  journal = {arXiv:1707.03141 [cs, stat]},
  eprint = {1707.03141},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/PP7FQ2PH/Mishra et al. - 2018 - A Simple Neural Attentive Meta-Learner.pdf}
}

@article{mitchell,
  title = {The {{Need}} for {{Biases}} in {{Learning Generalizations}}},
  author = {Mitchell, Tom M},
  pages = {4},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/BIWXF8CT/Mitchell - The Need for Biases in Learning Generalizations.pdf}
}

@inproceedings{molybog2021,
  title = {When {{Does MAML Objective Have Benign Landscape}}?},
  booktitle = {2021 {{IEEE Conference}} on {{Control Technology}} and {{Applications}} ({{CCTA}})},
  author = {Molybog, Igor and Lavaei, Javad},
  year = {2021},
  month = aug,
  pages = {220--227},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/CCTA48906.2021.9659038},
  abstract = {The paper studies the landscape of the optimization problem behind the Model-Agnostic Meta-Learning (MAML) algorithm. The goal of the study is to determine the global convergence of MAML on sequential decision-making tasks possessing a common structure. We investigate in what scenarios the benign optimization landscape of the underlying tasks results in a benign landscape of the corresponding MAML objective. For illustration, we analyze the landscape of the MAML objective on LQR tasks to determine what types of similarities in their structures enable the algorithm to converge to the globally optimal solution.},
  isbn = {978-1-66543-643-4},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/XP55CX35/Molybog and Lavaei - 2021 - When Does MAML Objective Have Benign Landscape.pdf}
}

@article{moore1965,
  title = {Cramming More Components onto Integrated Circuits},
  author = {Moore, Gordon E},
  year = {1965},
  volume = {38},
  number = {8},
  pages = {4},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MT74UYPZ/Moore - 1965 - Cramming more components onto integrated circuits.pdf}
}

@article{moreno-torres2012,
  title = {A Unifying View on Dataset Shift in Classification},
  author = {{Moreno-Torres}, Jose G. and Raeder, Troy and {Alaiz-Rodr{\'i}guez}, Roc{\'i}o and Chawla, Nitesh V. and Herrera, Francisco},
  year = {2012},
  month = jan,
  journal = {Pattern Recognition},
  volume = {45},
  number = {1},
  pages = {521--530},
  issn = {00313203},
  doi = {10.1016/j.patcog.2011.06.019},
  abstract = {The field of dataset shift has received a growing amount of interest in the last few years. The fact that most real-world applications have to cope with some form of shift makes its study highly relevant. The literature on the topic is mostly scattered, and different authors use different names to refer to the same concepts, or use the same name for different concepts. With this work, we attempt to present a unifying framework through the review and comparison of some of the most important works in the literature.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/CUP9RDBD/Moreno-Torres et al. - 2012 - A unifying view on dataset shift in classification.pdf}
}

@article{mozaffar2019,
  title = {Deep Learning Predicts Path-Dependent Plasticity},
  author = {Mozaffar, M. and Bostanabad, R. and Chen, W. and Ehmann, K. and Cao, J. and Bessa, M. A.},
  year = {2019},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {52},
  pages = {26414--26420},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1911815116},
  abstract = {Significance             We show that material plasticity can be precisely and efficiently predicted by deep-learning methods. This approach is fundamentally different from the century-old theory of continuum plasticity because it is not iteratively tracing the yield surface, neither does it require the notion of effective strain or stress at the macroscopic level. Instead, we use representative computer simulations of materials, including microstructure and constituents, load them along different deformation paths, and then learn the reversible, irreversible, and history-dependent phenomena directly from data. We demonstrate that complex phenomena such as distortional hardening can be predicted within 0.5\% error. The generality of the methodology and widespread importance of plasticity in designing structures and materials make it useful to a myriad of fields.           ,              Plasticity theory aims at describing the yield loci and work hardening of a material under general deformation states. Most of its complexity arises from the nontrivial dependence of the yield loci on the complete strain history of a material and its microstructure. This motivated 3 ingenious simplifications that underpinned a century of developments in this field: 1) yield criteria describing yield loci location; 2) associative or nonassociative flow rules defining the direction of plastic flow; and 3) effective stress\textendash strain laws consistent with the plastic work equivalence principle. However, 2 key complications arise from these simplifications. First, finding equations that describe these 3 assumptions for materials with complex microstructures is not trivial. Second, yield surface evolution needs to be traced iteratively, i.e., through a return mapping algorithm. Here, we show that these assumptions are not needed in the context of sequence learning when using recurrent neural networks, diverting the above-mentioned complications. This work offers an alternative to currently established plasticity formulations by providing the foundations for finding history- and microstructure-dependent constitutive models through deep learning.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/D5299ZN9/Mozaffar et al. - 2019 - Deep learning predicts path-dependent plasticity.pdf}
}

@article{munkhdalai2017,
  title = {Meta {{Networks}}},
  author = {Munkhdalai, Tsendsuren and Yu, Hong},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.00837 [cs, stat]},
  eprint = {1703.00837},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6\% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/S8MH7JRK/Munkhdalai and Yu - 2017 - Meta Networks.pdf}
}

@book{murphy2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  isbn = {978-0-262-01802-9},
  langid = {english},
  lccn = {Q325.5 .M87 2012},
  keywords = {Machine learning,Probabilities},
  file = {/home/taylanot/Zotero/storage/8L9UY4UE/Murphy - 2012 - Machine learning a probabilistic perspective.pdf}
}

@article{nakkiran2020,
  title = {Learning {{Rate Annealing Can Provably Help Generalization}}, {{Even}} for {{Convex Problems}}},
  author = {Nakkiran, Preetum},
  year = {2020},
  month = may,
  journal = {arXiv:2005.07360 [cs, stat]},
  eprint = {2005.07360},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning rate schedule can significantly affect generalization performance in modern neural networks, but the reasons for this are not yet understood. Li et al. (2019) recently proved this behavior can exist in a simplified non-convex neural-network setting. In this note, we show that this phenomenon can exist even for convex learning problems \textendash{} in particular, linear regression in 2 dimensions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/T5PGW6V8/Nakkiran - 2020 - Learning Rate Annealing Can Provably Help Generali.pdf}
}

@misc{nakkiran2021,
  title = {Optimal {{Regularization Can Mitigate Double Descent}}},
  author = {Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  year = {2021},
  month = apr,
  number = {arXiv:2003.01897},
  eprint = {2003.01897},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  abstract = {Recent empirical and theoretical studies have shown that many learning algorithms \textendash{} from linear regression to neural networks \textendash{} can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as ``double descent'', has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned 2 regularization achieves monotonic test performance as we grow either the sample size or the model size. We also demonstrate empirically that optimally-tuned 2 regularization can mitigate double descent for more general models, including neural networks. Our results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/NKWX2GBH/Nakkiran et al. - 2021 - Optimal Regularization Can Mitigate Double Descent.pdf}
}

@article{naumova2012,
  title = {A Meta-Learning Approach to the Regularized Learning\textemdash{{Case}} Study: {{Blood}} Glucose Prediction},
  shorttitle = {A Meta-Learning Approach to the Regularized Learning\textemdash{{Case}} Study},
  author = {Naumova, V. and Pereverzyev, S.V. and Sivananthan, S.},
  year = {2012},
  month = sep,
  journal = {Neural Networks},
  volume = {33},
  pages = {181--193},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.05.004},
  abstract = {In this paper we present a new scheme of a kernel-based regularization learning algorithm, in which the kernel and the regularization parameter are adaptively chosen on the base of previous experience with similar learning tasks. The construction of such scheme is motivated by the problem of prediction of the blood glucose levels of diabetic patients. We describe how the proposed scheme can be used for this problem and report the results of the tests with real clinical data as well as compare them with existing literature.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/5QLI8CQA/Naumova et al. - 2012 - A meta-learning approach to the regularized learni.pdf}
}

@book{neal1996,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  year = {1996},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {118},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-0745-0},
  abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties | the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
  isbn = {978-0-387-94724-2 978-1-4612-0745-0},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/WTISHVW7/Neal - 1996 - Bayesian Learning for Neural Networks.pdf}
}

@inproceedings{nguyen2020,
  title = {Uncertainty in {{Model-Agnostic Meta-Learning}} Using {{Variational Inference}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Nguyen, Cuong and Do, Thanh-Toan and Carneiro, Gustavo},
  year = {2020},
  month = mar,
  pages = {3079--3089},
  publisher = {{IEEE}},
  address = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093536},
  abstract = {We introduce a new, rigorously-formulated Bayesian meta-learning algorithm that learns a probability distribution of model parameter prior for few-shot learning. The proposed algorithm employs a gradient-based variational inference to infer the posterior of model parameters for a new task. Our algorithm can be applied to any model architecture and can be implemented in various machine learning paradigms, including regression and classification. We show that the models trained with our proposed meta-learning algorithm are well calibrated and accurate, with state-of-the-art calibration and classification results on three few-shot classification benchmarks (Omniglot, mini-ImageNet and tiered-ImageNet), and competitive results in a multi-modal task-distribution regression.},
  isbn = {978-1-72816-553-0},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/ZQTWE3X4/Nguyen et al. - 2020 - Uncertainty in Model-Agnostic Meta-Learning using .pdf}
}

@article{nichol2018,
  title = {On {{First-Order Meta-Learning Algorithms}}},
  author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  year = {2018},
  month = oct,
  journal = {arXiv:1803.02999 [cs]},
  eprint = {1803.02999},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only firstorder derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/72U6587I/Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf}
}

@article{niroomandi2010,
  title = {Model Order Reduction for Hyperelastic Materials: {{MODEL ORDER REDUCTION FOR HYPERELASTIC MATERIALS}}},
  shorttitle = {Model Order Reduction for Hyperelastic Materials},
  author = {Niroomandi, Siamak and Alfaro, Ic{\'i}ar and Cueto, El{\'i}as and Chinesta, Francisco},
  year = {2010},
  month = feb,
  journal = {International Journal for Numerical Methods in Engineering},
  volume = {81},
  number = {9},
  pages = {1180--1206},
  issn = {00295981},
  doi = {10.1002/nme.2733},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/VG2TR7KX/Niroomandi et al. - 2010 - Model order reduction for hyperelastic materials .pdf}
}

@article{niu2020,
  title = {A {{Decade Survey}} of {{Transfer Learning}} (2010\textendash 2020)},
  author = {Niu, Shuteng and Liu, Yongxin and Wang, Jian and Song, Houbing},
  year = {2020},
  month = oct,
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {1},
  number = {2},
  pages = {151--166},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3054609},
  abstract = {Transfer learning (TL) has been successfully applied to many real-world problems that traditional machine learning (ML) cannot handle, such as image processing, speech recognition, and natural language processing (NLP). Commonly, TL tends to address three main problems of traditional machine learning: (1) insufficient labeled data, (2) incompatible computation power, and (3) distribution mismatch. In general, TL can be organized into four categories: transductive learning, inductive learning, unsupervised learning, and negative learning. Furthermore, each category can be organized into four learning types: learning on instances, learning on features, learning on parameters, and learning on relations. This article presents a comprehensive survey on TL. In addition, this article presents the state of the art, current trends, applications, and open challenges.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/BB4A7NBW/Niu et al. - 2020 - A Decade Survey of Transfer Learning (2010â€“2020).pdf}
}

@book{nocedal2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer Series in Operations Research},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-30303-1},
  langid = {english},
  lccn = {QA402.5 .N62 2006},
  keywords = {Mathematical optimization},
  annotation = {OCLC: ocm68629100},
  file = {/home/taylanot/Zotero/storage/AL9UCZ27/Nocedal and Wright - 2006 - Numerical optimization.pdf}
}

@book{ochoa1992,
  title = {Finite {{Element Analysis}} of {{Composite Laminates}}},
  author = {Ochoa, O. O. and Reddy, J. N.},
  editor = {Gladwell, G. M. L.},
  year = {1992},
  series = {Solid {{Mechanics}} and {{Its Applications}}},
  volume = {7},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-015-7995-7},
  isbn = {978-90-481-4084-8 978-94-015-7995-7},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/H47PMMAJ/Ochoa and Reddy - 1992 - Finite Element Analysis of Composite Laminates.pdf}
}

@article{orr,
  title = {Centre for {{Cognitive Science}}, {{University}} of {{Edinburgh}}, 2, {{Buccleuch Place}}, {{Edinburgh EH8 9LW}}, {{Scotland April}} 1996},
  author = {Orr, Mark J L},
  pages = {67},
  abstract = {This document is an introduction to radial basis function (RBF) networks, a type of arti cial neural network for application to problems of supervised learning (e.g. regression, classi cation and time series prediction). It is now only available in PostScript2 (an older and now unsupported hyper-text version3 may be available for a while longer).},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/WTQU7GSL/Orr - Centre for Cognitive Science, University of Edinbu.pdf}
}

@book{orr1998,
  title = {Neural Networks: Tricks of the Trade},
  shorttitle = {Neural Networks},
  editor = {Orr, Genevieve and M{\"u}ller, Klaus-Robert},
  year = {1998},
  series = {Lecture Notes in Computer Science},
  number = {1524},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  isbn = {978-3-540-65311-0},
  langid = {english},
  lccn = {QA76.87 .N4913 1998},
  keywords = {Neural networks (Computer science)},
  file = {/home/taylanot/Zotero/storage/S69WQDIC/Orr and MÃ¼ller - 1998 - Neural networks tricks of the trade.pdf}
}

@article{owhadi2019,
  title = {Kernel {{Flows}}: {{From}} Learning Kernels from Data into the Abyss},
  shorttitle = {Kernel {{Flows}}},
  author = {Owhadi, Houman and Yoo, Gene Ryan},
  year = {2019},
  month = jul,
  journal = {Journal of Computational Physics},
  volume = {389},
  pages = {22--47},
  issn = {00219991},
  doi = {10.1016/j.jcp.2019.03.040},
  abstract = {Learning can be seen as approximating an unknown function by interpolating the training data. Although Kriging offers a solution to this problem, it requires the prior specification of a kernel and it is not scalable to large datasets. We explore a numerical approximation approach to kernel selection/construction based on the simple premise that a kernel must be good if the number of interpolation points can be halved without significant loss in accuracy (measured using the intrinsic RKHS norm {$\cdot$} associated with the kernel). We first test and motivate this idea on a simple problem of recovering the Green's function of an elliptic PDE (with inhomogeneous coefficients) from the sparse observation of one of its solutions. Next we consider the problem of learning non-parametric families of deep kernels of the form K1(Fn(x), Fn(x )) with Fn+1 = (Id + Gn+1) {$\smwhtcircle$} Fn and Gn+1 {$\in$} span\{K1(Fn(xi), {$\cdot$})\}. With the proposed approach constructing the kernel becomes equivalent to integrating a stochastic data driven dynamical system, which allows for the training of very deep (bottomless) networks and the exploration of their properties. These networks learn by constructing flow maps in the kernel and input spaces via incremental data-dependent deformations/perturbations (appearing as the cooperative counterpart of adversarial examples) and, at profound depths, they (1) can achieve accurate classification from only one data point per class (2) appear to learn archetypes of each class (3) expand distances between points that are in different classes and contract distances between points in the same class. For kernels parameterized by the weights of Convolutional Neural Networks, minimizing approximation errors incurred by halving random subsets of interpolation points, appears to outperform training (the same CNN architecture) with relative entropy and dropout.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/72IDE3R3/Owhadi and Yoo - 2019 - Kernel Flows From learning kernels from data into.pdf}
}

@incollection{padua2011,
  title = {Massive-{{Scale Analytics}}},
  booktitle = {Encyclopedia of {{Parallel Computing}}},
  author = {Padua, David and Ghoting, Amol and Gunnels, John A. and Squillante, Mark S. and Meseguer, Jos{\'e} and Cownie, James H. and Roweth, Duncan and Adve, Sarita V. and Boehm, Hans J. and McKee, Sally A. and Wisniewski, Robert W. and Karypis, George and Malony, Allen D. and Gottlieb, Steven and Riesen, Rolf and Maccabe, Arthur B. and Bilardi, Gianfranco and Pietracaprina, Andrea and Kejariwal, Arun and Nicolau, Alexandru and Lengauer, Christian and Gustafson, John L. and Gropp, William and Prost, Jean-Pierre and Padua, David and Lowney, Geoff and Amestoy, Patrick and Buttari, Alfredo and Duff, Iain and Guermouche, Abdou and L'Excellent, Jean-Yves and U{\c c}ar, Bora and Halstead, Robert H. and Nemirovsky, Mario and Amestoy, Patrick and Buttari, Alfredo and Duff, Iain and Guermouche, Abdou and L'Excellent, Jean-Yves and U{\c c}ar, Bora and Pakin, Scott},
  editor = {Padua, David},
  year = {2011},
  pages = {1089--1095},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-09766-4_418},
  isbn = {978-0-387-09765-7 978-0-387-09766-4},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/ZAPZCFDU/Padua et al. - 2011 - Massive-Scale Analytics.pdf}
}

@article{pan2010,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  month = oct,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.191},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MWAYFXS5/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf}
}

@inproceedings{panchapakesan1998,
  title = {Effects of Moving the Centers in an {{RBF}} Network},
  booktitle = {1998 {{IEEE International Joint Conference}} on {{Neural Networks Proceedings}}. {{IEEE World Congress}} on {{Computational Intelligence}} ({{Cat}}. {{No}}.{{98CH36227}})},
  author = {Panchapakesan, C. and Ralph, D. and Palaniswami, M.},
  year = {1998},
  volume = {2},
  pages = {1256--1260},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1109/IJCNN.1998.685954},
  abstract = {In Radial Basis Function Networks, placement of centers has been one of the problems addressed and is said to have a significant effect on the performance of the network. Supervised learning of center locations in some applications show that they are superior to the networks whose centers are located using unsupervised methods. But such networks can take the same training time as that of sigmoid networks. Supervised learning of centers seem to offset the advantages acheived by the two stage learning of the RBF networks. One way to overcome this may be to train the network with a set of centers selected by unsupervised methods and then to fine tune the centers. Tliis can be done by first evaluating whether moving the centers would decrease the error and depending on the needed level of accuracy position of the centers can be changed. In this paper we have calculated bounds for the gradient and hessian of the error considered as a function of centers for networks of fixed size. Using these bounds it will be possible to know by how much one can reduce the error by changing the centers. Further, step size can be specified to acheive a guaranteed amount of reduction in error.},
  isbn = {978-0-7803-4859-2},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/T9WVC9ZC/Panchapakesan et al. - 1998 - Effects of moving the centers in an RBF network.pdf}
}

@article{pandey2021,
  title = {Machine Learning Based Surrogate Modeling Approach for Mapping Crystal Deformation in Three Dimensions},
  author = {Pandey, Anup and Pokharel, Reeju},
  year = {2021},
  month = mar,
  journal = {Scripta Materialia},
  volume = {193},
  pages = {1--5},
  issn = {13596462},
  doi = {10.1016/j.scriptamat.2020.10.028},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/7I5DRDMK/Pandey and Pokharel - 2021 - Machine learning based surrogate modeling approach.pdf}
}

@article{parisi2019,
  title = {Continual Lifelong Learning with Neural Networks: {{A}} Review},
  shorttitle = {Continual Lifelong Learning with Neural Networks},
  author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  year = {2019},
  month = may,
  journal = {Neural Networks},
  volume = {113},
  pages = {54--71},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.01.012},
  abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-theart deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/NK7YWMSV/Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf}
}

@article{parker-jenkins2018,
  title = {Mind the Gap: Developing the Roles, Expectations and Boundaries in the Doctoral Supervisor\textendash Supervisee Relationship},
  shorttitle = {Mind the Gap},
  author = {{Parker-Jenkins}, Marie},
  year = {2018},
  month = jan,
  journal = {Studies in Higher Education},
  volume = {43},
  number = {1},
  pages = {57--71},
  issn = {0307-5079, 1470-174X},
  doi = {10.1080/03075079.2016.1153622},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/PUT3SBDP/Parker-Jenkins - 2018 - Mind the gap developing the roles, expectations a.pdf}
}

@misc{parnami2022,
  title = {Learning from {{Few Examples}}: {{A Summary}} of {{Approaches}} to {{Few-Shot Learning}}},
  shorttitle = {Learning from {{Few Examples}}},
  author = {Parnami, Archit and Lee, Minwoo},
  year = {2022},
  month = mar,
  number = {arXiv:2203.04291},
  eprint = {2203.04291},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently1 proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/QTCJSTBY/Parnami and Lee - 2022 - Learning from Few Examples A Summary of Approache.pdf}
}

@article{paszke,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  pages = {12},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/37RTHBLG/Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf}
}

@article{pekalska2006,
  title = {Prototype Selection for Dissimilarity-Based Classifiers},
  author = {P{\k{e}}kalska, El{\.z}bieta and Duin, Robert P.W. and Pacl{\'i}k, Pavel},
  year = {2006},
  month = feb,
  journal = {Pattern Recognition},
  volume = {39},
  number = {2},
  pages = {189--208},
  issn = {00313203},
  doi = {10.1016/j.patcog.2005.06.012},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/Z3IVMHM4/PÄ™kalska et al. - 2006 - Prototype selection for dissimilarity-based classi.pdf}
}

@article{peng2020,
  title = {A {{Comprehensive Overview}} and {{Survey}} of {{Recent Advances}} in {{Meta-Learning}}},
  author = {Peng, Huimin},
  year = {2020},
  month = oct,
  journal = {arXiv:2004.11149 [cs, stat]},
  eprint = {2004.11149},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in highly automated AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly introduce meta-learning methodologies in the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/Q34LJSH2/Peng - 2020 - A Comprehensive Overview and Survey of Recent Adva.pdf}
}

@article{perdikaris2017,
  title = {Nonlinear Information Fusion Algorithms for Data-Efficient Multi-Fidelity Modelling},
  author = {Perdikaris, P. and Raissi, M. and Damianou, A. and Lawrence, N. D. and Karniadakis, G. E.},
  year = {2017},
  month = feb,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {473},
  number = {2198},
  pages = {20160751},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2016.0751},
  abstract = {Multi-fidelity modelling enables accurate inference of quantities of interest by synergistically combining realizations of low-cost/low-fidelity models with a small set of high-fidelity observations. This is particularly effective when the low- and high-fidelity models exhibit strong correlations, and can lead to significant computational gains over approaches that solely rely on high-fidelity models. However, in many cases of practical interest, low-fidelity models can only be well correlated to their high-fidelity counterparts for a specific range of input parameters, and potentially return wrong trends and erroneous predictions if probed outside of their validity regime. Here we put forth a probabilistic framework based on Gaussian process regression and nonlinear autoregressive schemes that is capable of learning complex nonlinear and space-dependent cross-correlations between models of variable fidelity, and can effectively safeguard against low-fidelity models that provide wrong trends. This introduces a new class of multi-fidelity information fusion algorithms that provide a fundamental extension to the existing linear autoregressive methodologies, while still maintaining the same algorithmic complexity and overall computational cost. The performance of the proposed methods is tested in several benchmark problems involving both synthetic and real multi-fidelity datasets from computational fluid dynamics simulations.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/2S5L46BR/Perdikaris et al. - 2017 - Nonlinear information fusion algorithms for data-e.pdf}
}

@article{petersen,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  pages = {72},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/FRC5W3VG/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf}
}

@article{pyrialakos2021,
  title = {A Neural Network-Aided {{Bayesian}} Identification Framework for Multiscale Modeling of Nanocomposites},
  author = {Pyrialakos, Stefanos and Kalogeris, Ioannis and Sotiropoulos, Gerasimos and Papadopoulos, Vissarion},
  year = {2021},
  month = oct,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {384},
  pages = {113937},
  issn = {00457825},
  doi = {10.1016/j.cma.2021.113937},
  abstract = {This work proposes a Bayesian framework for determining the mechanical properties of carbon based nanocomposites. In particular, Bayesian parameter inference is applied to learn the parameters that characterize the CNT/polymer interface in the microscale. These parameters are associated with great uncertainties and their characterization is a difficult task, since microscale measurements are costly and hard to obtain. To overcome this, the present study introduces a computational framework for updating the prior beliefs on the values of these parameters, by using measurements on large-scale structures comprised of the composite. In terms of modeling, the CNT/polymer interface is formulated with the cohesive zone model and a bilinear bond-slip constitutive law. Typically, predicting the response of such systems requires multiscale analysis approaches, such as the F E2 method, employed in this work. Despite its accuracy, F E2 is associated with immense computational demands for large-scale problems and, therefore, its application to the Bayesian setting, which requires multiple model evaluations, is prohibitive. To alleviate this cost, a surrogate modeling technique is developed which utilizes artificial neural networks, trained to predict the nonlinear stress\textendash strain relationship of representative volume elements of the microstructure. The data set over which the neural network is trained, is obtained by analyzing a limited number of different RVE configurations using a detailed finite element analysis. The elaborated methodology is first validated through a numerical example from 2D elasticity, which demonstrated its high accuracy and its significant cost reduction capabilities. It is then applied to a more challenging large-scale problem from 3D elasticity. Even though this paper focuses on the characterization of the mechanical properties of composite materials, the proposed numerical procedure is generic and can be straightforwardly applied to other physically analogous phenomena related to nano-composite modeling, such as parameter identification in heat transfer or electrical conduction.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/82INYSZH/Pyrialakos et al. - 2021 - A neural network-aided Bayesian identification fra.pdf}
}

@article{raccuglia2016,
  title = {Machine-Learning-Assisted Materials Discovery Using Failed Experiments},
  author = {Raccuglia, Paul and Elbert, Katherine C. and Adler, Philip D. F. and Falk, Casey and Wenny, Malia B. and Mollo, Aurelio and Zeller, Matthias and Friedler, Sorelle A. and Schrier, Joshua and Norquist, Alexander J.},
  year = {2016},
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7601},
  pages = {73--76},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17439},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/KIPKWBZK/Raccuglia et al. - 2016 - Machine-learning-assisted materials discovery usin.pdf}
}

@article{rad2021,
  title = {On {{Theory-training Neural Networks}} to {{Infer}} the {{Solution}} of {{Highly Coupled Differential Equations}}},
  author = {Rad, M. Torabi and Viardin, A. and Apel, M.},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.04890 [physics]},
  eprint = {2102.04890},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Deep neural networks are transforming fields ranging from computer vision to computational medicine, and we recently extended their application to the field of phase-change heat transfer by introducing theory-trained neural networks (TTNs) for a solidification problem \textbackslash cite\{TTN\}. Here, we present general, in-depth, and empirical insights into theory-training networks for learning the solution of highly coupled differential equations. We analyze the deteriorating effects of the oscillating loss on the ability of a network to satisfy the equations at the training data points, measured by the final training loss, and on the accuracy of the inferred solution. We introduce a theory-training technique that, by leveraging regularization, eliminates those oscillations, decreases the final training loss, and improves the accuracy of the inferred solution, with no additional computational cost. Then, we present guidelines that allow a systematic search for the network that has the optimal training time and inference accuracy for a given set of equations; following these guidelines can reduce the number of tedious training iterations in that search. Finally, a comparison between theory-training and the rival, conventional method of solving differential equations using discretization attests to the advantages of theory-training not being necessarily limited to high-dimensional sets of equations. The comparison also reveals a limitation of the current theory-training framework that may limit its application in domains where extreme accuracies are necessary.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Applied Physics},
  file = {/home/taylanot/Zotero/storage/MRTSQ26H/Rad et al. - 2021 - On Theory-training Neural Networks to Infer the So.pdf}
}

@misc{raghu2020,
  title = {Rapid {{Learning}} or {{Feature Reuse}}? {{Towards Understanding}} the {{Effectiveness}} of {{MAML}}},
  shorttitle = {Rapid {{Learning}} or {{Feature Reuse}}?},
  author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  year = {2020},
  month = feb,
  number = {arXiv:1909.09157},
  eprint = {1909.09157},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains \textendash{} is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/3EVIRNAR/Raghu et al. - 2020 - Rapid Learning or Feature Reuse Towards Understan.pdf}
}

@article{raissi2017,
  title = {Physics {{Informed Deep Learning}} ({{Part I}}): {{Data-driven Solutions}} of {{Nonlinear Partial Differential Equations}}},
  shorttitle = {Physics {{Informed Deep Learning}} ({{Part I}})},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.10561 [cs, math, stat]},
  eprint = {1711.10561},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We introduce physics informed neural networks \textendash{} neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/6RIGN8EM/Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf}
}

@article{raissi2019,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  shorttitle = {Physics-Informed Neural Networks},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
  year = {2019},
  month = feb,
  journal = {Journal of Computational Physics},
  volume = {378},
  pages = {686--707},
  issn = {00219991},
  doi = {10.1016/j.jcp.2018.10.045},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/3RUCHBB9/Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf}
}

@article{rajasegaran2020,
  title = {{{iTAML}}: {{An Incremental Task-Agnostic Meta-learning Approach}}},
  shorttitle = {{{iTAML}}},
  author = {Rajasegaran, Jathushan and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Shah, Mubarak},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.11652 [cs, stat]},
  eprint = {2003.11652},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Humans can continuously learn new knowledge as their experience grows. In contrast, previous learning in deep neural networks can quickly fade out when they are trained on a new task. In this paper, we hypothesize this problem can be avoided by learning a set of generalized parameters, that are neither specific to old nor new tasks. In this pursuit, we introduce a novel meta-learning approach that seeks to maintain an equilibrium between all the encountered tasks. This is ensured by a new meta-update rule which avoids catastrophic forgetting. In comparison to previous metalearning techniques, our approach is task-agnostic. When presented with a continuum of data, our model automatically identifies the task and quickly adapts to it with just a single update. We perform extensive experiments on five datasets in a class-incremental setting, leading to significant improvements over the state of the art methods (e.g., a 21.3\% boost on CIFAR100 with 10 incremental tasks). Specifically, on large-scale datasets that generally prove difficult cases for incremental learning, our approach delivers absolute gains as high as 19.1\% and 7.4\% on ImageNet and MS-Celeb datasets, respectively. Our codes are available at: https://github.com/brjathu/iTAML .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/8KHPENUA/Rajasegaran et al. - 2020 - iTAML An Incremental Task-Agnostic Meta-learning .pdf}
}

@book{rasmussen2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/home/taylanot/Zotero/storage/MZIXZ2PT/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf}
}

@article{ravi2017,
  title = {{{OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING}}},
  author = {Ravi, Sachin and Larochelle, Hugo},
  year = {2017},
  pages = {11},
  abstract = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classifier has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classifiers requires many iterative steps over many examples to perform well. Here, we propose an LSTMbased meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/HDEXQDLQ/Ravi and Larochelle - 2017 - OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING.pdf}
}

@book{reddy2004,
  title = {An {{Introduction}} to {{Nonlinear Finite Element Analysis}}},
  author = {Reddy, J. N.},
  year = {2004},
  month = mar,
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780198525295.001.0001},
  isbn = {978-0-19-852529-5},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/85Y9AZ78/Reddy - 2004 - An Introduction to Nonlinear Finite Element Analys.pdf}
}

@article{reich1995,
  title = {Machine Learning of Material Behaviour Knowledge from Empirical Data},
  author = {Reich, Yoram and Travitzky, Nahum},
  year = {1995},
  month = jan,
  journal = {Materials \& Design},
  volume = {16},
  number = {5},
  pages = {251--259},
  issn = {02613069},
  doi = {10.1016/0261-3069(96)00007-6},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/SKG4SZD2/Reich and Travitzky - 1995 - Machine learning of material behaviour knowledge f.pdf}
}

@article{reich1997,
  title = {Machine {{Learning Techniques}} for {{Civil Engineering Problems}}},
  author = {Reich, Yoram},
  year = {1997},
  month = jul,
  journal = {Computer-Aided Civil and Infrastructure Engineering},
  volume = {12},
  number = {4},
  pages = {295--310},
  issn = {1093-9687, 1467-8667},
  doi = {10.1111/0885-9507.00065},
  abstract = {The growing volume of information databases presents opportunities for advanced data analysis techniques from machine learning (ML) research. Practical applications of ML are very different from theoretical or empirical studies, involving organizational and human aspects and various other constraints. Despite the importance of applied ML, little has been discussed in the general ML literature on this topic. In order to remedy this situation, I studied practical applications of ML and developed a proposal for a seven-step process that can guide practical applications of ML in engineering. The process is illustrated by relevant applications of ML in civil engineering. This illustration shows that the potential of ML has only begun to be explored but also cautions that in order to be successful, the application process must carefully address the issues related to the seven-step process.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/F5KEPI8V/Reich - 1997 - Machine Learning Techniques for Civil Engineering .pdf}
}

@article{reimann2019,
  title = {Modeling {{Macroscopic Material Behavior With Machine Learning Algorithms Trained}} by {{Micromechanical Simulations}}},
  author = {Reimann, Denise and Nidadavolu, Kapil and {\noopsort{hassan}}{ul Hassan}, Hamad and Vajragupta, Napat and Glasmachers, Tobias and Junker, Philipp and Hartmaier, Alexander},
  year = {2019},
  month = aug,
  journal = {Frontiers in Materials},
  volume = {6},
  pages = {181},
  issn = {2296-8016},
  doi = {10.3389/fmats.2019.00181},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/JUGBQXDY/Reimann et al. - 2019 - Modeling Macroscopic Material Behavior With Machin.pdf}
}

@article{renaud2009,
  title = {The {{Yeoh}} Model Applied to the Modeling of Large Deformation Contact/Impact Problems},
  author = {Renaud, Christine and Cros, Jean-Michel and Feng, Zhi-Qiang and Yang, Bintang},
  year = {2009},
  month = may,
  journal = {International Journal of Impact Engineering},
  volume = {36},
  number = {5},
  pages = {659--666},
  issn = {0734743X},
  doi = {10.1016/j.ijimpeng.2008.09.008},
  abstract = {The present paper is devoted to the modeling of finite deformations of hyperelastic bodies described by the Yeoh model under contact/impact conditions. A total Lagrangian formulation is adopted to describe the geometrically nonlinear behavior. A first order algorithm is applied to integrate the equations of motion. For the finite element implementation, an explicit expression of the tangent operator is derived. Two numerical examples are presented to show the applicability of the developed approach.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/FSUDI77U/Renaud et al. - 2009 - The Yeoh model applied to the modeling of large de.pdf}
}

@article{reyes2021,
  title = {Learning {{Unknown Physics}} of Non-{{Newtonian Fluids}}},
  author = {Reyes, Brandon and Howard, Amanda A. and Perdikaris, Paris and Tartakovsky, Alexandre M.},
  year = {2021},
  month = jul,
  journal = {Physical Review Fluids},
  volume = {6},
  number = {7},
  eprint = {2009.01658},
  eprinttype = {arxiv},
  pages = {073301},
  issn = {2469-990X},
  doi = {10.1103/PhysRevFluids.6.073301},
  abstract = {We extend the physics-informed neural network (PINN) method to learn viscosity models of two non-Newtonian systems (polymer melts and suspensions of particles) using only velocity measurements. The PINN-inferred viscosity models agree with the empirical models for shear rates with large absolute values but deviate for shear rates near zero where the analytical models have an unphysical singularity. Once a viscosity model is learned, we use the PINN method to solve the momentum conservation equation for non-Newtonian fluid flow using only the boundary conditions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Physics - Fluid Dynamics},
  file = {/home/taylanot/Zotero/storage/ANDCYRL8/Reyes et al. - 2021 - Learning Unknown Physics of non-Newtonian Fluids.pdf}
}

@article{rice1971,
  title = {Inelastic Constitutive Relations for Solids: {{An}} Internal-Variable Theory and Its Application to Metal Plasticity},
  shorttitle = {Inelastic Constitutive Relations for Solids},
  author = {Rice, J.R.},
  year = {1971},
  month = nov,
  journal = {Journal of the Mechanics and Physics of Solids},
  volume = {19},
  number = {6},
  pages = {433--455},
  issn = {00225096},
  doi = {10.1016/0022-5096(71)90010-X},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/GDEJD9IQ/Rice - 1971 - Inelastic constitutive relations for solids An in.pdf}
}

@article{rocha2020,
  title = {Micromechanics-Based Surrogate Models for the Response of Composites: {{A}} Critical Comparison between a Classical Mesoscale Constitutive Model, Hyper-Reduction and Neural Networks},
  shorttitle = {Micromechanics-Based Surrogate Models for the Response of Composites},
  author = {Rocha, I.B.C.M. and Kerfriden, P. and {\noopsort{meer}}{van der Meer}, F.P.},
  year = {2020},
  month = jul,
  journal = {European Journal of Mechanics - A/Solids},
  volume = {82},
  pages = {103995},
  issn = {09977538},
  doi = {10.1016/j.euromechsol.2020.103995},
  abstract = {Although being a popular approach for the modeling of laminated composites, mesoscale constitutive models often struggle to represent material response for arbitrary load cases. A better alternative in terms of accuracy is to use the FE2 technique to upscale microscopic material behavior without loss of generality, but the associated computational effort can be extreme. It is therefore interesting to explore alternative surrogate modeling stra\- tegies that maintain as much of the fidelity of FE2 as possible while still being computationally efficient. In this work, three surrogate modeling approaches are compared in terms of accuracy, efficiency and calibration effort: the state-of-the-art mesoscopic plasticity model by Vogler et al. (Vogler et al., 2013), regularized feed-forward neural networks and hyper-reduced-order models obtained by combining the Proper Orthogonal Decomposi\- tion (POD) and Empirical Cubature Method (ECM) techniques. Training datasets are obtained from a Repre\- sentative Volume Element (RVE) model of the composite microstructure with a number of randomly-distributed linear-elastic fibers surrounded by a matrix with pressure-dependent plasticity. The approaches are evaluated with a comprehensive set of numerical tests comprising pure stress cases and three different stress combinations relevant in the design of laminated composites. The models are assessed on their ability to accurately reproduce the training cases as well as on how well they are able to predict unseen stress combinations. Gains in execution time are compared by using the trained surrogates in the FE2 model of an interlaminar shear test.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MHL28A9D/Rocha et al. - 2020 - Micromechanics-based surrogate models for the resp.pdf}
}

@article{rocha2021,
  title = {On-the-Fly Construction of Surrogate Constitutive Models for Concurrent Multiscale Mechanical Analysis through Probabilistic Machine Learning},
  author = {Rocha, I.B.C.M. and Kerfriden, P. and {\noopsort{meer}}{van der Meer}, F.P.},
  year = {2021},
  month = jan,
  journal = {Journal of Computational Physics: X},
  volume = {9},
  pages = {100083},
  issn = {25900552},
  doi = {10.1016/j.jcpx.2020.100083},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MKCFCT4Q/Rocha et al. - 2021 - On-the-fly construction of surrogate constitutive .pdf}
}

@article{ronneberger2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  journal = {arXiv:1505.04597 [cs]},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/taylanot/Zotero/storage/V6U6HLAI/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@article{rosenman1989,
  title = {The Place of Expert Systems in Civil Engineering},
  author = {Rosenman, Michael A. and Balachandran, Bala M. and Gero, John S.},
  year = {1989},
  month = jan,
  journal = {Civil Engineering Systems},
  volume = {6},
  number = {1-2},
  pages = {11--20},
  issn = {0263-0257},
  doi = {10.1080/02630258908970538},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/4UJFWHBB/Rosenman et al. - 1989 - The place of expert systems in civil engineering.pdf}
}

@article{rothfuss,
  title = {Meta-{{Learning Reliable Priors}} in the {{Function Space}}},
  author = {Rothfuss, Jonas and Chen, Jinfan and Heyn, Dominique and Krause, Andreas},
  pages = {14},
  abstract = {When data are scarce, meta-learning can improve a learner's accuracy by harnessing previous experience from related learning tasks. However, existing methods have unreliable uncertainty estimates which are often overconfident. Addressing these shortcomings, we introduce a novel meta-learning framework, called F-PACOH, that treats meta-learned priors as stochastic processes and performs meta-level regularization directly in the function space. This allows us to directly steer the probabilistic predictions of the meta-learner towards high epistemic uncertainty in regions of insufficient meta-training data and, thus, obtain well-calibrated uncertainty estimates. Finally, we showcase how our approach can be integrated with sequential decision making, where reliable uncertainty quantification is imperative. In our benchmark study on meta-learning for Bayesian Optimization (BO), F-PACOH significantly outperforms all other meta-learners and standard baselines.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/RU43E9PG/Rothfuss et al. - Meta-Learning Reliable Priors in the Function Spac.pdf}
}

@article{rothfussa,
  title = {{{PACOH}}: {{Bayes-Optimal Meta-Learning}} with {{PAC-Guarantees}}},
  author = {Rothfuss, Jonas and Fortuin, Vincent and Josifoski, Martin and Krause, Andreas},
  pages = {11},
  abstract = {Meta-learning can successfully acquire useful inductive biases from data. Yet, its generalization properties to unseen learning tasks are poorly understood. Particularly if the number of metatraining tasks is small, this raises concerns about overfitting. We provide a theoretical analysis using the PAC-Bayesian framework and derive novel generalization bounds for meta-learning. Using these bounds, we develop a class of PAC-optimal meta-learning algorithms with performance guarantees and a principled meta-level regularization. Unlike previous PAC-Bayesian meta-learners, our method results in a standard stochastic optimization problem which can be solved efficiently and scales well. When instantiating our PACoptimal hyper-posterior (PACOH) with Gaussian processes and Bayesian Neural Networks as base learners, the resulting methods yield state-ofthe-art performance, both in terms of predictive accuracy and the quality of uncertainty estimates. Thanks to their principled treatment of uncertainty, our meta-learners can also be successfully employed for sequential decision problems.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/C5HPVMB4/Rothfuss et al. - PACOH Bayes-Optimal Meta-Learning with PAC-Guaran.pdf}
}

@article{rubanova,
  title = {Latent {{ODEs}} for {{Irregularly-Sampled Time Series}}},
  author = {Rubanova, Yulia and Chen, Ricky T Q and Duvenaud, David},
  pages = {11},
  abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/I3WCJ9VC/Rubanova et al. - Latent ODEs for Irregularly-Sampled Time Series.pdf}
}

@article{ruder2017,
  title = {An {{Overview}} of {{Multi-Task Learning}} in {{Deep Neural Networks}}},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.05098 [cs, stat]},
  eprint = {1706.05098},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/KA3M6JGE/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf}
}

@article{saeb2016,
  title = {Aspects of {{Computational Homogenization}} at {{Finite Deformations}}: {{A Unifying Review From Reuss}}' to {{Voigt}}'s {{Bound}}},
  shorttitle = {Aspects of {{Computational Homogenization}} at {{Finite Deformations}}},
  author = {Saeb, Saba and Steinmann, Paul and Javili, Ali},
  year = {2016},
  month = sep,
  journal = {Applied Mechanics Reviews},
  volume = {68},
  number = {5},
  pages = {050801},
  issn = {0003-6900, 2379-0407},
  doi = {10.1115/1.4034024},
  abstract = {The objective of this contribution is to present a unifying review on strain-driven computational homogenization at finite strains, thereby elaborating on computational aspects of the finite element method. The underlying assumption of computational homogenization is separation of length scales, and hence, computing the material response at the macroscopic scale from averaging the microscopic behavior. In doing so, the energetic equivalence between the two scales, the Hill\textendash Mandel condition, is guaranteed via imposing proper boundary conditions such as linear displacement, periodic displacement and antiperiodic traction, and constant traction boundary conditions. Focus is given on the finite element implementation of these boundary conditions and their influence on the overall response of the material. Computational frameworks for all canonical boundary conditions are briefly formulated in order to demonstrate similarities and differences among the various boundary conditions. Furthermore, we detail on the computational aspects of the classical Reuss' and Voigt's bounds and their extensions to finite strains. A concise and clear formulation for computing the macroscopic tangent necessary for FE2 calculations is presented. The performances of the proposed schemes are illustrated via a series of two- and three-dimensional numerical examples. The numerical examples provide enough details to serve as benchmarks.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/7JP98DV2/Saeb et al. - 2016 - Aspects of Computational Homogenization at Finite .pdf}
}

@article{sahlicostabal2020,
  title = {Physics-{{Informed Neural Networks}} for {{Cardiac Activation Mapping}}},
  author = {Sahli Costabal, Francisco and Yang, Yibo and Perdikaris, Paris and Hurtado, Daniel E. and Kuhl, Ellen},
  year = {2020},
  month = feb,
  journal = {Frontiers in Physics},
  volume = {8},
  pages = {42},
  issn = {2296-424X},
  doi = {10.3389/fphy.2020.00042},
  abstract = {A critical procedure in diagnosing atrial fibrillation is the creation of electro-anatomic activation maps. Current methods generate these mappings from interpolation using a few sparse data points recorded inside the atria; they neither include prior knowledge of the underlying physics nor uncertainty of these recordings. Here we propose a physics-informed neural network for cardiac activation mapping that accounts for the underlying wave propagation dynamics and we quantify the epistemic uncertainty associated with these predictions. These uncertainty estimates not only allow us to quantify the predictive error of the neural network, but also help to reduce it by judiciously selecting new informative measurement locations via active learning. We illustrate the potential of our approach using a synthetic benchmark problem and a personalized electrophysiology model of the left atrium. We show that our new method outperforms linear interpolation and Gaussian process regression for the benchmark problem and linear interpolation at clinical densities for the left atrium. In both cases, the active learning algorithm achieves lower error levels than random allocation. Our findings open the door toward physics-based electro-anatomic mapping with the ultimate goals to reduce procedural time and improve diagnostic predictability for patients affected by atrial fibrillation. Open source code is available at https://github.com/fsahli/EikonalNet.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/CD3N58DC/Sahli Costabal et al. - 2020 - Physics-Informed Neural Networks for Cardiac Activ.pdf}
}

@article{santoro,
  title = {Meta-{{Learning}} with {{Memory-Augmented Neural Networks}}},
  author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  pages = {9},
  abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ``one-shot learning.'' Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory locationbased focusing mechanisms.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/5IMULHDG/Santoro et al. - Meta-Learning with Memory-Augmented Neural Network.pdf}
}

@book{schilders2008,
  title = {Model {{Order Reduction}}: {{Theory}}, {{Research Aspects}} and {{Applications}}},
  shorttitle = {Model {{Order Reduction}}},
  editor = {Schilders, Wilhelmus H. A. and {\noopsort{vorst}}{van der Vorst}, Henk A. and Rommes, Joost and Bock, Hans-Georg and {\noopsort{hoog}}{de Hoog}, Frank and Friedman, Avner and Gupta, Arvind and Neunzert, Helmut and Pulleyblank, William R. and Rusten, Torgeir and Santosa, Fadil and Tornberg, Anna-Karin and Bonilla, Luis L. and Mattheij, Robert and Scherzer, Otmar},
  year = {2008},
  series = {Mathematics in {{Industry}}},
  volume = {13},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78841-6},
  isbn = {978-3-540-78840-9 978-3-540-78841-6},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/C8S76KKZ/Schilders et al. - 2008 - Model Order Reduction Theory, Research Aspects an.pdf}
}

@article{schmidhuber,
  title = {{{SIMPLE PRINCIPLES OF METALEARNING}}},
  author = {Schmidhuber, Jurgen and Zhao, Jieyu and Wiering, Marco and Elvezia, Corso},
  pages = {23},
  abstract = {The goal of metalearning is to generate useful shifts of inductive bias by adapting the current learning strategy in a \textbackslash useful" way. Our learner leads a single life during which actions are continually executed according to the system's internal state and current policy (a modi able, probabilistic algorithm mapping environmental inputs and internal states to outputs and new internal states). An action is considered a learning algorithm if it can modify the policy. E ects of learning processes on later learning processes are measured using reward/time ratios. Occasional backtracking enforces success histories of still valid policy modi cations corresponding to histories of lifelong reward accelerations. The principle allows for plugging in a wide variety of learning algorithms. In particular, it allows for embedding the learner's policy modi cation strategy within the policy itself (self-reference). To demonstrate the principle's feasibility in cases where conventional reinforcement learning fails, we test it in complex, non-Markovian, changing environments (\textbackslash POMDPs"). One of the tasks involves more than 1013 states, two learners that both cooperate and compete, and strongly delayed reinforcement signals (initially separated by more than 300,000 time steps).},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/R72KY4IM/Schmidhuber et al. - SIMPLE PRINCIPLES OF METALEARNING.pdf}
}

@article{schmidt2019,
  title = {Recent Advances and Applications of Machine Learning in Solid-State Materials Science},
  author = {Schmidt, Jonathan and Marques, M{\'a}rio R. G. and Botti, Silvana and Marques, Miguel A. L.},
  year = {2019},
  month = dec,
  journal = {npj Computational Materials},
  volume = {5},
  number = {1},
  pages = {83},
  issn = {2057-3960},
  doi = {10.1038/s41524-019-0221-0},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/LJQSVEWR/Schmidt et al. - 2019 - Recent advances and applications of machine learni.pdf}
}

@incollection{scholkopf2001,
  title = {A {{Generalized Representer Theorem}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
  editor = {Goos, G. and Hartmanis, J. and {\noopsort{leeuwen}}{van Leeuwen}, J. and Helmbold, David and Williamson, Bob},
  year = {2001},
  volume = {2111},
  pages = {416--426},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44581-1_27},
  abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
  isbn = {978-3-540-42343-0 978-3-540-44581-4},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/IUMR4JM7/SchÃ¶lkopf et al. - 2001 - A Generalized Representer Theorem.pdf}
}

@book{scholkopf2002,
  title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  shorttitle = {Learning with Kernels},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander J.},
  year = {2002},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-19475-4},
  langid = {english},
  lccn = {Q325.5 .S32 2002},
  keywords = {Kernel functions,Support vector machines},
  file = {/home/taylanot/Zotero/storage/GW5MX8PM/SchÃ¶lkopf and Smola - 2002 - Learning with kernels support vector machines, re.pdf}
}

@article{schooling1999,
  title = {An Example of the Use of Neural Computing Techniques in Materials Science\textemdash the Modelling of Fatigue Thresholds in {{Ni-base}} Superalloys},
  author = {Schooling, J.M and Brown, M and Reed, P.A.S},
  year = {1999},
  month = feb,
  journal = {Materials Science and Engineering: A},
  volume = {260},
  number = {1-2},
  pages = {222--239},
  issn = {09215093},
  doi = {10.1016/S0921-5093(98)00957-5},
  abstract = {Two adaptive numerical modelling techniques have been applied to prediction of fatigue thresholds in Ni-base superalloys. A Bayesian neural network and a neurofuzzy network have been compared, both of which have the ability to automatically adjust the network's complexity to the current dataset. In both cases, despite inevitable data restrictions, threshold values have been modelled with some degree of success. However, it is argued in this paper that the neurofuzzy modelling approach offers real benefits over the use of a classical neural network as the mathematical complexity of the relationships can be restricted to allow for the paucity of data, and the linguistic fuzzy rules produced allow assessment of the model without extensive interrogation and examination using a hypothetical dataset. The additive neurofuzzy network structure means that redundant inputs can be excluded from the model and simple sub-networks produced which represent global output trends. Both of these aspects are important for final verification and validation of the information extracted from the numerical data. In some situations neurofuzzy networks may require less data to produce a stable solution, and may be easier to verify in the light of existing physical understanding because of the production of transparent linguistic rules. \textcopyright{} 1999 Elsevier Science S.A. All rights reserved.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/XRNAAPDE/Schooling et al. - 1999 - An example of the use of neural computing techniqu.pdf}
}

@book{schroder2014,
  title = {Plasticity and beyond: Microstructures, Crystal-Plasticity and Phase Transitions},
  shorttitle = {Plasticity and Beyond},
  editor = {Schr{\"o}der, J{\"o}rg},
  year = {2014},
  series = {Courses and Lectures},
  number = {Vol. 550},
  publisher = {{Springer}},
  address = {{Wien}},
  isbn = {978-3-7091-1624-1},
  langid = {english},
  lccn = {MLCM 2017/43284 (T)},
  keywords = {Congresses,Microstructure,Plasticity},
  file = {/home/taylanot/Zotero/storage/Q3NRTQPB/SchrÃ¶der - 2014 - Plasticity and beyond microstructures, crystal-pl.pdf}
}

@article{seah2013,
  title = {Combating {{Negative Transfer From Predictive Distribution Differences}}},
  author = {Seah, Chun-Wei and Ong, Yew-Soon and Tsang, Ivor W.},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Cybernetics},
  volume = {43},
  number = {4},
  pages = {1153--1165},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TSMCB.2012.2225102},
  abstract = {Domain adaptation (DA), which leverages labeled data from related source domains, comes in handy when the label information of the target domain is scarce or unavailable. However, as the source data do not come from the same origin as that of the target domain, the predictive distributions of the source and target domains are likely to differ in reality. At the extreme, the predictive distributions of the source domains can differ completely from that of the target domain. In such case, using the learned source classifier to assist in the prediction of target data can result in prediction performance that is poorer than that with the omission of the source data. This phenomenon is established as negative transfer with impact known to be more severe in the multiclass context. To combat negative transfer due to differing predictive distributions across domains, we first introduce the notion of positive transferability for the assessment of synergy between the source and target domains in their prediction models, and we also propose a criterion to measure the positive transferability between sample pairs of different domains in terms of their prediction distributions. With the new measure, a predictive distribution matching (PDM) regularizer and a PDM framework learn the target classifier by favoring source data with large positive transferability while inferring the labels of target unlabeled data. Extensive experiments are conducted to validate the performance efficacy of the proposed PDM framework using several commonly used multidomain benchmark data sets, including Sentiment, Reuters, and Newsgroup, in the context of both binary-class and multiclass domains. Subsequently, the PDM framework is put to work on a real-world scenario pertaining to water cluster molecule identification. The experimental results illustrate the adverse impact of negative transfer on several state-of-the-art DA methods, whereas the proposed framework exhibits excellent and robust predictive performances.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/K43R36ZT/Seah et al. - 2013 - Combating Negative Transfer From Predictive Distri.pdf}
}

@article{setlur2021,
  title = {Two {{Sides}} of {{Meta-Learning Evaluation}}: {{In}} vs. {{Out}} of {{Distribution}}},
  shorttitle = {Two {{Sides}} of {{Meta-Learning Evaluation}}},
  author = {Setlur, Amrith and Li, Oscar and Smith, Virginia},
  year = {2021},
  month = oct,
  journal = {arXiv:2102.11503 [cs]},
  eprint = {2102.11503},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We categorize meta-learning evaluation into two settings: in-distribution [ID], in which the train and test tasks are sampled iid from the same underlying task distribution, and out-of-distribution [OOD], in which they are not. While most metalearning theory and some FSL applications follow the ID setting, we identify that most existing few-shot classification benchmarks instead reflect OOD evaluation, as they use disjoint sets of train (base) and test (novel) classes for task generation. This discrepancy is problematic because\textemdash as we show on numerous benchmarks\textemdash meta-learning methods that perform better on existing OOD datasets may perform significantly worse in the ID setting. In addition, in the OOD setting, even though current FSL benchmarks seem befitting, our study highlights concerns in 1) reliably performing model selection for a given meta-learning method, and 2) consistently comparing the performance of different methods. To address these concerns, we provide suggestions on how to construct FSL benchmarks to allow for ID evaluation as well as more reliable OOD evaluation. Our work\textdagger{} aims to inform the meta-learning community about the importance and distinction of ID vs. OOD evaluation, as well as the subtleties of OOD evaluation with current benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/7FPS8B4U/Setlur et al. - 2021 - Two Sides of Meta-Learning Evaluation In vs. Out .pdf}
}

@article{settles,
  title = {Active {{Learning Literature Survey}}},
  author = {Settles, Burr},
  pages = {47},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/Y4AUDL3U/Settles - Active Learning Literature Survey.pdf}
}

@book{shalev-shwartz2014,
  title = {Understanding {{Machine Learning}}: {{From Theory}} to {{Algorithms}}},
  shorttitle = {Understanding {{Machine Learning}}},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781107298019},
  isbn = {978-1-107-29801-9},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/CFHUB3FD/Shalev-Shwartz and Ben-David - 2014 - Understanding Machine Learning From Theory to Alg.pdf}
}

@article{sharifi-noghabi2020,
  title = {Domain {{Generalization}} via {{Semi-supervised Meta Learning}}},
  author = {{Sharifi-Noghabi}, Hossein and Asghari, Hossein and Mehrasa, Nazanin and Ester, Martin},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.12658 [cs, stat]},
  eprint = {2009.12658},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The goal of domain generalization is to learn from multiple source domains to generalize to unseen target domains under distribution discrepancy. Current state-ofthe-art methods in this area are fully supervised, but for many real-world problems it is hardly possible to obtain enough labeled samples. In this paper, we propose the first method of domain generalization to leverage unlabeled samples, combining of meta learning's episodic training and semi-supervised learning, called DGSML. DGSML employs an entropy-based pseudo-labeling approach to assign labels to unlabeled samples and then utilizes a novel discrepancy loss to ensure that class centroids before and after labeling unlabeled samples are close to each other. To learn a domain-invariant representation, it also utilizes a novel alignment loss to ensure that the distance between pairs of class centroids, computed after adding the unlabeled samples, is preserved across different domains. DGSML is trained by a meta learning approach to mimic the distribution shift between the input source domains and unseen target domains. Experimental results on benchmark datasets indicate that DGSML outperforms state-of-the-art domain generalization and semi-supervised learning methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/I2A8CWQJ/Sharifi-Noghabi et al. - 2020 - Domain Generalization via Semi-supervised Meta Lea.pdf}
}

@article{slack2019,
  title = {Fair {{Meta-Learning}}: {{Learning How}} to {{Learn Fairly}}},
  shorttitle = {Fair {{Meta-Learning}}},
  author = {Slack, Dylan and Friedler, Sorelle and Givental, Emile},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.04336 [cs, stat]},
  eprint = {1911.04336},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Data sets for fairness relevant tasks can lack examples or be biased according to a specific label in a sensitive attribute. We demonstrate the usefulness of weight based meta-learning approaches in such situations. For models that can be trained through gradient descent, we demonstrate that there are some parameter configurations that allow models to be optimized from a few number of gradient steps and with minimal data which are both fair and accurate. To learn such weight sets, we adapt the popular MAML algorithm to Fair-MAML by the inclusion of a fairness regularization term. In practice, Fair-MAML allows practitioners to train fair machine learning models from only a few examples when data from related tasks is available. We empirically exhibit the value of this technique by comparing to relevant baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/AX2K6YRD/Slack et al. - 2019 - Fair Meta-Learning Learning How to Learn Fairly.pdf}
}

@article{smith2018,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  year = {2018},
  month = apr,
  journal = {arXiv:1803.09820 [cs, stat]},
  eprint = {1803.09820},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentum. Files to help replicate the results reported here are available at https://github.com/lnsmith54/hyperParam1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/6ZLAUEHZ/Smith - 2018 - A disciplined approach to neural network hyper-par.pdf}
}

@article{smith2018a,
  title = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  year = {2018},
  month = feb,
  journal = {arXiv:1711.00489 [cs, stat]},
  eprint = {1711.00489},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate and scaling the batch size B {$\propto$} . Finally, one can increase the momentum coefficient m and scale B {$\propto$} 1/(1 - m), although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to 76.1\% validation accuracy in under 30 minutes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/4FWGGLTI/Smith et al. - 2018 - Don't Decay the Learning Rate, Increase the Batch .pdf}
}

@article{smola1998,
  title = {On a {{Kernel-Based Method}} for {{Pattern Recognition}}, {{Regression}}, {{Approximation}}, and {{Operator Inversion}}},
  author = {Smola, A. J. and Sch{\"o}lkopf, B.},
  year = {1998},
  month = sep,
  journal = {Algorithmica},
  volume = {22},
  number = {1-2},
  pages = {211--231},
  issn = {0178-4617},
  doi = {10.1007/PL00013831},
  abstract = {We present a kernel-based framework for pattern recognition, regression estimation, function approximation, and multiple operator inversion. Adopting a regularization-theoretic framework, the above are formulated as constrained optimization problems. Previous approaches such as ridge regression, support vector methods, and regularization networks are included as special cases. We show connections between the cost function and some properties up to now believed to apply to support vector machines only. For appropriately chosen cost functions, the optimal solution of all the problems described above can be found by solving a simple quadratic programming problem.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/KALE8A5I/Smola and SchÃ¶lkopf - 1998 - On a Kernel-Based Method for Pattern Recognition, .pdf}
}

@article{song2020,
  title = {{{ES-MAML}}: {{Simple Hessian-Free Meta Learning}}},
  shorttitle = {{{ES-MAML}}},
  author = {Song, Xingyou and Gao, Wenbo and Yang, Yuxiang and Choromanski, Krzysztof and Pacchiano, Aldo and Tang, Yunhao},
  year = {2020},
  month = jul,
  journal = {arXiv:1910.01215 [cs, math, stat]},
  eprint = {1910.01215},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of non-smooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. We show empirically that ES-MAML is competitive with existing methods and often yields better adaptation with fewer queries.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/JN4SF457/Song et al. - 2020 - ES-MAML Simple Hessian-Free Meta Learning.pdf}
}

@article{stephenson,
  title = {Can We Globally Optimize Cross-Validation Loss? {{Quasiconvexity}} in Ridge Regression},
  author = {Stephenson, William T and Frangella, Zachary and Udell, Madeleine and Broderick, Tamara},
  pages = {13},
  abstract = {Models like LASSO and ridge regression are extensively used in practice due to their interpretability, ease of use, and strong theoretical guarantees. Crossvalidation (CV) is widely used for hyperparameter tuning in these models, but do practical optimization methods minimize the true out-of-sample loss? A recent line of research promises to show that the optimum of the CV loss matches the optimum of the out-of-sample loss (possibly after simple corrections). It remains to show how tractable it is to minimize the CV loss. In the present paper, we show that, in the case of ridge regression, the CV loss may fail to be quasiconvex and thus may have multiple local optima. We can guarantee that the CV loss is quasiconvex in at least one case: when the spectrum of the covariate matrix is nearly flat and the noise in the observed responses is not too high. More generally, we show that quasiconvexity status is independent of many properties of the observed data (response norm, covariate-matrix right singular vectors, and singular-value scaling) and has a complex dependence on the few that remain. We empirically confirm our theory using simulated experiments.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/T66ZCDEM/Stephenson et al. - Can we globally optimize cross-validation loss Qu.pdf}
}

@article{sun2019,
  title = {Meta-{{Transfer Learning}} for {{Few-Shot Learning}}},
  author = {Sun, Qianru and Liu, Yaoyao and Chua, Tat-Seng and Schiele, Bernt},
  year = {2019},
  month = apr,
  journal = {arXiv:1812.02391 [cs]},
  eprint = {1812.02391},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, meta refers to training multiple tasks, and transfer is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using (5-class, 1-shot) and (5-class, 5shot) recognition tasks on two challenging few-shot learning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/taylanot/Zotero/storage/JNGXI3UU/Sun et al. - Meta-Transfer Learning for Few-Shot Learning.pdf;/home/taylanot/Zotero/storage/VD4VZRQK/Sun et al. - 2019 - Meta-Transfer Learning for Few-Shot Learning.pdf}
}

@article{sun2020,
  title = {Test-{{Time Training}} with {{Self-Supervision}} for {{Generalization}} under {{Distribution Shifts}}},
  author = {Sun, Yu and Wang, Xiaolong and Liu, Zhuang and Miller, John and Efros, Alexei A. and Hardt, Moritz},
  year = {2020},
  month = jul,
  journal = {arXiv:1909.13231 [cs, stat]},
  eprint = {1909.13231},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a single unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/2PWHHNRD/Sun et al. - 2020 - Test-Time Training with Self-Supervision for Gener.pdf}
}

@incollection{szabo2009,
  title = {On the {{Discretization Time-Step}} in the {{Finite Element Theta-Method}} of the {{Discrete Heat Equation}}},
  booktitle = {Numerical {{Analysis}} and {{Its Applications}}},
  author = {Szab{\'o}, Tam{\'a}s},
  editor = {Margenov, Svetozar and Vulkov, Lubin G. and Wa{\'s}niewski, Jerzy},
  year = {2009},
  volume = {5434},
  pages = {564--571},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-00464-3_66},
  abstract = {In this paper the numerical solution of the one dimensional heat conduction equation is investigated, by applying Dirichlet boundary condition at the left hand side and Neumann boundary condition was applied at the right hand side. To the discretization in space, we apply the linear finite element method and for the time discretization the wellknown theta-method. The aim of the work is to derive an adequate numerical solution for the homogenous initial condition by this approach. We theoretically analyze the possible choice of the time-discretization step-size and establish the interval where the discrete model is reliable to the original physical phenomenon.},
  isbn = {978-3-642-00463-6 978-3-642-00464-3},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/4YMGVVQB/SzabÃ³ - 2009 - On the Discretization Time-Step in the Finite Elem.pdf}
}

@article{tan2018,
  title = {A {{Survey}} on {{Deep Transfer Learning}}},
  author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.01974 [cs, stat]},
  eprint = {1808.01974},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/DZSICN2X/Tan et al. - 2018 - A Survey on Deep Transfer Learning.pdf}
}

@article{temizer2008,
  title = {On the Computation of the Macroscopic Tangent for Multiscale Volumetric Homogenization Problems},
  author = {Temizer, {\.I}. and Wriggers, P.},
  year = {2008},
  month = dec,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {198},
  number = {3-4},
  pages = {495--510},
  issn = {00457825},
  doi = {10.1016/j.cma.2008.08.018},
  abstract = {In this contribution, methods of computing the macroscopic tangent that is required in multiscale volumetric homogenization problems are investigated. A condensation procedure that employs the tangent information from the microscale finite element analysis is derived within a special framework where deformation controlled boundary conditions in micromechanical testing are enforced via the penalty method. The developed methodology is demonstrated by sample macroscale problems in the context of the multilevel finite element strategy. Due to the high memory allocation costs that the condensation procedure induces, a perturbation procedure is developed based on a minimal number of micromechanical tests. Results from the condensation and perturbation procedures are compared for sample infinitesimal and finite deformation inelasticity problems and the algorithmic consistency of the macroscopic tangent with the evolution of the macroscopic stress is discussed. It is shown that the ability to compute the macroscopic tangent can also be employed to construct a stress controlled micromechanical testing procedure.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/YW44HS44/Temizer and Wriggers - 2008 - On the computation of the macroscopic tangent for .pdf}
}

@book{tenek1998,
  title = {Finite {{Element Analysis}} for {{Composite Structures}}},
  author = {Tenek, Lazarus Teneketzis and Argyris, John},
  editor = {Gladwell, G. M. L.},
  year = {1998},
  series = {Solid {{Mechanics}} and {{Its Applications}}},
  volume = {59},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-015-9044-0},
  isbn = {978-90-481-4975-9 978-94-015-9044-0},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/A6W722IM/Tenek and Argyris - 1998 - Finite Element Analysis for Composite Structures.pdf}
}

@incollection{theodoridis2009,
  title = {Clustering {{Algorithms III}}: {{Schemes Based}} on {{Function Optimization}}},
  shorttitle = {Clustering {{Algorithms III}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
  year = {2009},
  pages = {701--763},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-59749-272-0.50016-5},
  isbn = {978-1-59749-272-0},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/DHA5DA2Q/Theodoridis and Koutroumbas - 2009 - Clustering Algorithms III Schemes Based on Functi.pdf}
}

@incollection{theodoridis2009a,
  title = {Feature {{Generation I}}: {{Data Transformation}} and {{Dimensionality Reduction}}},
  shorttitle = {Feature {{Generation I}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
  year = {2009},
  pages = {323--409},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-59749-272-0.50008-6},
  isbn = {978-1-59749-272-0},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/5FA3BH6H/Theodoridis and Koutroumbas - 2009 - Feature Generation I Data Transformation and Dime.pdf}
}

@book{thrun1998,
  title = {Learning to {{Learn}}},
  editor = {Thrun, Sebastian and Pratt, Lorien},
  year = {1998},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4615-5529-2},
  isbn = {978-1-4613-7527-2 978-1-4615-5529-2},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/WXX36E6X/Thrun and Pratt - 1998 - Learning to Learn.pdf}
}

@article{tibshirani,
  title = {Valerie and {{Patrick Hastie}}},
  author = {Tibshirani, Sami and Friedman, Harry},
  pages = {764},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/VZTIHXPZ/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf}
}

@article{tipireddy2019,
  title = {A Comparative Study of Physics-Informed Neural Network Models for Learning Unknown Dynamics and Constitutive Relations},
  author = {Tipireddy, Ramakrishna and Perdikaris, Paris and Stinis, Panos and Tartakovsky, Alexandre},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.04058 [physics]},
  eprint = {1904.04058},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {We investigate the use of discrete and continuous versions of physics-informed neural network methods for learning unknown dynamics or constitutive relations of a dynamical system . For the case of unknown dynamics, we represent all the dynamics with a deep neural network (DNN). When the dynamics of the system are known up to the specification of constitutive relations (that can depend on the state of the system), we represent these constitutive relations with a DNN. The discrete versions combine classical multistep discretization methods for dynamical systems with neural network based machine learning methods. On the other hand, the continuous versions utilize deep neural networks to minimize the residual function for the continuous governing equations. We use the case of a fedbatch bioreactor system to study the effectiveness of these approaches and discuss conditions for their applicability. Our results indicate that the accuracy of the trained neural network models is much higher for the cases where we only have to learn a constitutive relation instead of the whole dynamics. This finding corroborates the well-known fact from scientific computing that building as much structural information is available into an algorithm can enhance its efficiency and/or accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Physics - Computational Physics},
  file = {/home/taylanot/Zotero/storage/6KAA5JZD/Tipireddy et al. - 2019 - A comparative study of physics-informed neural net.pdf}
}

@book{torquato2002,
  title = {Random {{Heterogeneous Materials}}},
  author = {Torquato, Salvatore},
  editor = {Antman, S. S. and Sirovich, L. and Marsden, J. E. and Wiggins, S.},
  year = {2002},
  series = {Interdisciplinary {{Applied Mathematics}}},
  volume = {16},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-6355-3},
  isbn = {978-1-4757-6357-7 978-1-4757-6355-3},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/XNYIFPRU/Torquato - 2002 - Random Heterogeneous Materials.pdf}
}

@article{turan,
  title = {Surrogate {{Constitutive Models}} with {{Multi-fidelity Gaussian Processes}}s for{{Composite Micromodels}}},
  author = {Turan, {\"O} T},
  pages = {90},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/TA4J2859/Turan - Surrogate Constitutive Models with Multi-ï¬delity G.pdf}
}

@article{vantuijl2019,
  title = {Wavelet Based Reduced Order Models for Microstructural Analyses},
  author = {{\noopsort{tuijl}}{van Tuijl}, Rody A. and Harnish, Cale and Matou{\v s}, Karel and Remmers, Joris J. C. and Geers, Marc G. D.},
  year = {2019},
  month = mar,
  journal = {Computational Mechanics},
  volume = {63},
  number = {3},
  pages = {535--554},
  issn = {0178-7675, 1432-0924},
  doi = {10.1007/s00466-018-1608-3},
  abstract = {This paper proposes a novel method to accurately and efficiently reduce a microstructural mechanical model using a wavelet based discretisation. The model enriches a standard reduced order modelling (ROM) approach with a wavelet representation. Although the ROM approach reduces the dimensionality of the system of equations, the computational complexity of the integration of the weak form remains problematic. Using a sparse wavelet representation of the required integrands, the computational cost of the assembly of the system of equations is reduced significantly. This wavelet-reduced order model (W-ROM) is applied to the mechanical equilibrium of a microstructural volume as used in a computational homogenisation framework. The reduction technique however is not limited to micro-scale models and can also be applied to macroscopic problems to reduce the computational costs of the integration. For the sake of clarity, the W-ROM will be demonstrated using a one-dimensional example, providing full insight in the underlying steps taken.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/NFJ8Y6FP/van Tuijl et al. - 2019 - Wavelet based reduced order models for microstruct.pdf}
}

@article{vera,
  title = {Lagrangian and {{Eulerian Forms}} of {{Finite Plasticity}}},
  author = {Vera, Giorgio Tantuan De},
  pages = {121},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/BNECGPCM/Vera - Lagrangian and Eulerian Forms of Finite Plasticity.pdf}
}

@article{verhelst2021,
  title = {Stretch-{{Based Hyperelastic Material Formulations}} for {{Isogeometric Kirchhoff}}\textendash{{Love Shells}} with {{Application}} to {{Wrinkling}}},
  author = {Verhelst, H.M. and M{\"o}ller, M. and Den Besten, J.H. and Mantzaflaris, A. and Kaminski, M.L.},
  year = {2021},
  month = oct,
  journal = {Computer-Aided Design},
  volume = {139},
  pages = {103075},
  issn = {00104485},
  doi = {10.1016/j.cad.2021.103075},
  abstract = {Modelling nonlinear phenomena in thin rubber shells calls for stretch-based material models, such as the Ogden model which conveniently utilizes eigenvalues of the deformation tensor. Derivation and implementation of such models have been already made in Finite Element Methods. This is, however, still lacking in shell formulations based on Isogeometric Analysis, where higher-order continuity of the spline basis is employed for improved accuracy. This paper fills this gap by presenting formulations of stretch-based material models for isogeometric Kirchhoff-Love shells. We derive general formulations based on explicit treatment in terms of derivatives of the strain energy density functions with respect to principal stretches for (in)compressible material models where determination of eigenvalues as well as the spectral basis transformations is required. Using several numerical benchmarks, we verify our formulations on invariant-based Neo-Hookean and Mooney-Rivlin models and with a stretch-based Ogden model. In addition, the model is applied to simulate collapsing behaviour of a truncated cone and it is used to simulate tension wrinkling of a thin sheet.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/S8T99ZWZ/Verhelst et al. - 2021 - Stretch-Based Hyperelastic Material Formulations f.pdf}
}

@article{viering2021,
  title = {The {{Shape}} of {{Learning Curves}}: A {{Review}}},
  shorttitle = {The {{Shape}} of {{Learning Curves}}},
  author = {Viering, Tom and Loog, Marco},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.10948 [cs]},
  eprint = {2103.10948},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning curves provide insight into the dependence of a learner's generalization performance on the training set size. This important tool can be used for model selection, to predict the effect of more training data, and to reduce the computational complexity of model training and hyperparameter tuning. This review recounts the origins of the term, provides a formal definition of the learning curve, and briefly covers basics such as its estimation. Our main contribution is a comprehensive overview of the literature regarding the shape of learning curves. We discuss empirical and theoretical evidence that supports well-behaved curves that often have the shape of a power law or an exponential. We consider the learning curves of Gaussian processes, the complex shapes they can display, and the factors influencing them. We draw specific attention to examples of learning curves that are ill-behaved, showing worse learning performance with more training data. To wrap up, we point out various open problems that warrant deeper empirical and theoretical investigation. All in all, our review underscores that learning curves are surprisingly diverse and no universal model can be identified.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/JU5698K4/Viering and Loog - 2021 - The Shape of Learning Curves a Review.pdf}
}

@article{vilalta,
  title = {A {{Perspective View}} and {{Survey}} of {{Meta-Learning}}},
  author = {Vilalta, Ricardo and Drissi, Youssef},
  pages = {20},
  abstract = {Different researchers hold different views of what the term meta-learning exactly means. The first part of this paper provides our own perspective view in which the goal is to build self-adaptive learners (i.e. learning algorithms that improve their bias dynamically through experience by accumulating meta-knowledge). The second part provides a survey of meta-learning as reported by the machine-learning literature. We find that, despite different views and research lines, a question remains constant: how can we exploit knowledge about learning (i.e. meta-knowledge) to improve the performance of learning algorithms? Clearly the answer to this question is key to the advancement of the field and continues being the subject of intensive research.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/5NL2PE7G/Vilalta and Drissi - A Perspective View and Survey of Meta-Learning.pdf}
}

@inproceedings{volpi2021,
  title = {Continual {{Adaptation}} of {{Visual Representations}} via {{Domain Randomization}} and {{Meta-learning}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Volpi, Riccardo and Larlus, Diane and Rogez, Gregory},
  year = {2021},
  month = jun,
  pages = {4441--4451},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00442},
  abstract = {Most standard learning approaches lead to fragile models which are prone to drift when sequentially trained on samples of a different nature\textemdash the well-known catastrophic forgetting issue. In particular, when a model consecutively learns from different visual domains, it tends to forget the past domains in favor of the most recent ones. In this context, we show that one way to learn models that are inherently more robust against forgetting is domain randomization\textemdash for vision tasks, randomizing the current domain's distribution with heavy image manipulations. Building on this result, we devise a meta-learning strategy where a regularizer explicitly penalizes any loss associated with transferring the model from the current domain to different ``auxiliary'' meta-domains, while also easing adaptation to them. Such meta-domains are also generated through randomized image manipulations. We empirically demonstrate in a variety of experiments\textemdash spanning from classification to semantic segmentation\textemdash that our approach results in models that are less prone to catastrophic forgetting when transferred to new domains.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/AR6A2P8H/Volpi et al. - 2021 - Continual Adaptation of Visual Representations via.pdf}
}

@article{wang2018,
  title = {Instance-Based {{Deep Transfer Learning}}},
  author = {Wang, Tianyang and Huan, Jun and Zhu, Michelle},
  year = {2018},
  month = nov,
  journal = {arXiv:1809.02776 [cs]},
  eprint = {1809.02776},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep transfer learning recently has acquired significant research interest. It makes use of pre-trained models that are learned from a source domain, and utilizes these models for the tasks in a target domain. Model-based deep transfer learning is probably the most frequently used method. However, very little research work has been devoted to enhancing deep transfer learning by focusing on the influence of data. In this paper, we propose an instance-based approach to improve deep transfer learning in a target domain. Specifically, we choose a pre-trained model from a source domain and apply this model to estimate the influence of training samples in a target domain. Then we optimize the training data of the target domain by removing the training samples that will lower the performance of the pre-trained model. We later either fine-tune the pre-trained model with the optimized training data in the target domain, or build a new model which is initialized partially based on the pre-trained model, and fine-tune it with the optimized training data in the target domain. Using this approach, transfer learning can help deep learning models to capture more useful features. Extensive experiments demonstrate the effectiveness of our approach on boosting the quality of deep learning models for some common computer vision tasks, such as image classification.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/taylanot/Zotero/storage/ZLRLSAYG/Wang et al. - 2018 - Instance-based Deep Transfer Learning.pdf}
}

@article{wang2019,
  title = {Meta-Modeling Game for Deriving Theory-Consistent, Microstructure-Based Traction\textendash Separation Laws via Deep Reinforcement Learning},
  author = {Wang, Kun and Sun, WaiChing},
  year = {2019},
  month = apr,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {346},
  pages = {216--241},
  issn = {00457825},
  doi = {10.1016/j.cma.2018.11.026},
  abstract = {This paper presents a new meta-modeling framework that employs deep reinforcement learning (DRL) to generate mechanical constitutive models for interfaces. The constitutive models are conceptualized as information flow in directed graphs. The process of writing constitutive models is simplified as a sequence of forming graph edges with the goal of maximizing the model score (a function of accuracy, robustness and forward prediction quality). Thus meta-modeling can be formulated as a Markov decision process with well-defined states, actions, rules, objective functions and rewards. By using neural networks to estimate policies and state values, the computer agent is able to efficiently self-improve the constitutive model it generated through self-playing, in the same way AlphaGo Zero (the algorithm that outplayed the world champion in the game of Go) improves its gameplay. Our numerical examples show that this automated meta-modeling framework does not only produces models which outperform existing cohesive models on benchmark traction\textendash separation data, but is also capable of detecting hidden mechanisms among micro-structural features and incorporating them in constitutive models to improve the forward prediction accuracy, both of which are difficult tasks to do manually.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/SCZ9G9GG/Wang and Sun - 2019 - Meta-modeling game for deriving theory-consistent,.pdf}
}

@article{wang2020,
  title = {Generalizing from a {{Few Examples}}: {{A Survey}} on {{Few-Shot Learning}}},
  shorttitle = {Generalizing from a {{Few Examples}}},
  author = {Wang, Yaqing and Yao, Quanming and Kwok, James and Ni, Lionel M.},
  year = {2020},
  month = mar,
  journal = {arXiv:1904.05046 [cs]},
  eprint = {1904.05046},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/6SNMJWIA/Wang et al. - 2020 - Generalizing from a Few Examples A Survey on Few-.pdf}
}

@article{wang2020a,
  title = {Global {{Convergence}} and {{Generalization Bound}} of {{Gradient-Based Meta-Learning}} with {{Deep Neural Nets}}},
  author = {Wang, Haoxiang and Sun, Ruoyu and Li, Bo},
  year = {2020},
  month = nov,
  journal = {arXiv:2006.14606 [cs, stat]},
  eprint = {2006.14606},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gradient-based meta-learning (GBML) with deep neural nets (DNNs) has become a popular approach for few-shot learning. However, due to the non-convexity of DNNs and the bi-level optimization in GBML, the theoretical properties of GBML with DNNs remain largely unknown. In this paper, we first aim to answer the following question: Does GBML with DNNs have global convergence guarantees? We provide a positive answer to this question by proving that GBML with overparameterized DNNs is guaranteed to converge to global optima at a linear rate. The second question we aim to address is: How does GBML achieve fast adaption to new tasks with prior experience on past tasks? To answer it, we theoretically show that GBML is equivalent to a functional gradient descent operation that explicitly propagates experience from the past tasks to new ones, and then we prove a generalization error bound of GBML with over-parameterized DNNs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/LLLAKX55/Wang et al. - 2020 - Global Convergence and Generalization Bound of Gra.pdf}
}

@article{wang2020b,
  title = {When and Why {{PINNs}} Fail to Train: {{A}} Neural Tangent Kernel Perspective},
  shorttitle = {When and Why {{PINNs}} Fail to Train},
  author = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.14527 [cs, math, stat]},
  eprint = {2007.14527},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/ PINNsNTK.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/5QJV47DY/Wang et al. - 2020 - When and why PINNs fail to train A neural tangent.pdf}
}

@article{wang2021,
  title = {Bridging {{Multi-Task Learning}} and {{Meta-Learning}}: {{Towards Efficient Training}} and {{Effective Adaptation}}},
  shorttitle = {Bridging {{Multi-Task Learning}} and {{Meta-Learning}}},
  author = {Wang, Haoxiang and Zhao, Han and Li, Bo},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.09017 [cs, stat]},
  eprint = {2106.09017},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Multi-task learning (MTL) aims to improve the generalization of several related tasks by learning them jointly. As a comparison, in addition to the joint training scheme, modern meta-learning allows unseen tasks with limited labels during the test phase, in the hope of fast adaptation over them. Despite the subtle difference between MTL and meta-learning in the problem formulation, both learning paradigms share the same insight that the shared structure between existing training tasks could lead to better generalization and adaptation. In this paper, we take one important step further to understand the close connection between these two learning paradigms, through both theoretical analysis and empirical investigation. Theoretically, we first demonstrate that MTL shares the same optimization formulation with a class of gradient-based meta-learning (GBML) algorithms. We then prove that for over-parameterized neural networks with sufficient depth, the learned predictive functions of MTL and GBML are close. In particular, this result implies that the predictions given by these two models are similar over the same unseen task. Empirically, we corroborate our theoretical findings by showing that, with proper implementation, MTL is competitive against state-of-the-art GBML algorithms on a set of few-shot image classification benchmarks. Since existing GBML algorithms often involve costly second-order bi-level optimization, our first-order MTL method is an order of magnitude faster on large-scale datasets such as miniImageNet. We believe this work could help bridge the gap between these two learning paradigms, and provide a computationally efficient alternative to GBML that also supports fast task adaptation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/ZW9SMXSD/Wang et al. - 2021 - Bridging Multi-Task Learning and Meta-Learning To.pdf}
}

@article{wang2021a,
  title = {Deep Learning of Free Boundary and {{Stefan}} Problems},
  author = {Wang, Sifan and Perdikaris, Paris},
  year = {2021},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {428},
  eprint = {2006.05311},
  eprinttype = {arxiv},
  pages = {109914},
  issn = {00219991},
  doi = {10.1016/j.jcp.2020.109914},
  abstract = {Free boundary problems appear naturally in numerous areas of mathematics, science and engineering. These problems present a great computational challenge because they necessitate numerical methods that can yield an accurate approximation of free boundaries and complex dynamic interfaces. In this work, we propose a multi-network model based on physics-informed neural networks to tackle a general class of forward and inverse free boundary problems called Stefan problems. Specifically, we approximate the unknown solution as well as any moving boundaries by two deep neural networks. Besides, we formulate a new type of inverse Stefan problems that aim to reconstruct the solution and free boundaries directly from sparse and noisy measurements. We demonstrate the effectiveness of our approach in a series of benchmarks spanning different types of Stefan problems, and illustrate how the proposed framework can accurately recover solutions of partial differential equations with moving boundaries and dynamic interfaces. All code and data accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/DeepStefan.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/NUS35662/Wang and Perdikaris - 2021 - Deep learning of free boundary and Stefan problems.pdf}
}

@article{wang2021b,
  title = {Learning the Solution Operator of Parametric Partial Differential Equations with Physics-Informed {{DeepOnets}}},
  author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.10974 [cs, math, stat]},
  eprint = {2103.10974},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Deep operator networks (DeepONets) are receiving increased attention thanks to their demonstrated capability to approximate nonlinear operators between infinite-dimensional Banach spaces. However, despite their remarkable early promise, they typically require large training data-sets consisting of paired input-output observations which may be expensive to obtain, while their predictions may not be consistent with the underlying physical principles that generated the observed data. In this work, we propose a novel model class coined as physics-informed DeepONets, which introduces an effective regularization mechanism for biasing the outputs of DeepOnet models towards ensuring physical consistency. This is accomplished by leveraging automatic differentiation to impose the underlying physical laws via soft penalty constraints during model training. We demonstrate that this simple, yet remarkably effective extension can not only yield a significant improvement in the predictive accuracy of DeepOnets, but also greatly reduce the need for large training data-sets. To this end, a remarkable observation is that physics-informed DeepONets are capable of solving parametric partial differential equations (PDEs) without any paired input-output observations, except for a set of given initial or boundary conditions. We illustrate the effectiveness of the proposed framework through a series of comprehensive numerical studies across various types of PDEs. Strikingly, a trained physics informed DeepOnet model can predict the solution of O(103) timedependent PDEs in a fraction of a second \textendash{} up to three orders of magnitude faster compared a conventional PDE solver. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/BZM3ZECK/Wang et al. - 2021 - Learning the solution operator of parametric parti.pdf}
}

@article{wang2021c,
  title = {Metamodeling of Constitutive Model Using {{Gaussian}} Process Machine Learning},
  author = {Wang, Jikun and Li, Tianjiao and Cui, Fan and Hui, Chung-Yuen and Yeo, Jingjie and Zehnder, Alan T.},
  year = {2021},
  month = sep,
  journal = {Journal of the Mechanics and Physics of Solids},
  volume = {154},
  pages = {104532},
  issn = {00225096},
  doi = {10.1016/j.jmps.2021.104532},
  abstract = {A method based on Singular Value Decomposition (SVD) and Gaussian process machine learning is proposed to build a metamodel of a constitutive model that models time dependent and nonlinear behavior. To test this method, we apply it to determine the material parameters of a nonlinear viscoelastic (poly(vinylalcohol)) hydrogel (PVA). Using the metamodel, we are able to rapidly generate the stress histories for a large set of data points spanning a wide range of material parameters without solving the constitutive model of the PVA gel explicitly. To determine the material parameters, we compare the stress histories predicted by the metamodel with the observed stress histories from laboratory experiments consisting of uniaxial tension cyclic and relaxation tests. The efficiency of the metamodel allows us to determine the material parameters of the constitutive model governing the time-dependent behavior of the PVA gel in a short time. The proposed method shows that there exist many sets of material parameters that can faithfully reproduce the experimental data. Further, our method reveals important relationships between the material parameters in the constitutive model. Although the focus is on the PVA gel system, the method can be easily transferred to build a metamodel for any material model.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MXI673F3/Wang et al. - 2021 - Metamodeling of constitutive model using Gaussian .pdf}
}

@article{wang2022,
  title = {Mosaic {{Flows}}: {{A Transferable Deep Learning Framework}} for {{Solving PDEs}} on {{Unseen Domains}}},
  shorttitle = {Mosaic {{Flows}}},
  author = {Wang, Hengjie and Planas, Robert and Chandramowlishwaran, Aparna and Bostanabad, Ramin},
  year = {2022},
  month = feb,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {389},
  eprint = {2104.10873},
  eprinttype = {arxiv},
  pages = {114424},
  issn = {00457825},
  doi = {10.1016/j.cma.2021.114424},
  abstract = {Physics-informed neural networks (PINNs) are increasingly employed to replace/augment traditional numerical methods in solving partial differential equations (PDEs). While state-of-the-art PINNs have many attractive features, they approximate a specific realization of a PDE system and hence are problem-specific. That is, the model needs to be re-trained each time the boundary conditions (BCs) and domain shape/size change. This limitation prohibits the application of PINNs to realistic or large-scale engineering problems especially since the costs and efforts associated with their training are considerable.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Performance,I.2.6,Physics - Computational Physics},
  file = {/home/taylanot/Zotero/storage/JHBWJV2J/Wang et al. - 2022 - Mosaic Flows A Transferable Deep Learning Framewo.pdf}
}

@article{webb2018,
  title = {Analyzing Concept Drift and Shift from Sample Data},
  author = {Webb, Geoffrey I. and Lee, Loong Kuan and Goethals, Bart and Petitjean, Fran{\c c}ois},
  year = {2018},
  month = sep,
  journal = {Data Mining and Knowledge Discovery},
  volume = {32},
  number = {5},
  pages = {1179--1199},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-018-0554-1},
  abstract = {Concept drift and shift are major issues that greatly affect the accuracy and reliability of many real-world applications of machine learning. We propose a new data mining task, concept drift mapping\textemdash the description and analysis of instances of concept drift or shift. We argue that concept drift mapping is an essential prerequisite for tackling concept drift and shift. We propose tools for this purpose, arguing for the importance of quantitative descriptions of drift and shift in marginal distributions. We present quantitative concept drift mapping techniques, along with methods for visualizing their results. We illustrate their effectiveness for real-world applications across energy-pricing, vegetation monitoring and airline scheduling.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/DUNPD68E/Webb et al. - 2018 - Analyzing concept drift and shift from sample data.pdf}
}

@article{weiss2016,
  title = {A Survey of Transfer Learning},
  author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
  year = {2016},
  month = dec,
  journal = {Journal of Big Data},
  volume = {3},
  number = {1},
  pages = {9},
  issn = {2196-1115},
  doi = {10.1186/s40537-016-0043-6},
  abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/EZ68WJMC/Weiss et al. - 2016 - A survey of transfer learning.pdf}
}

@article{wiens2014,
  title = {A Study in Transfer Learning: Leveraging Data from Multiple Hospitals to Enhance Hospital-Specific Predictions},
  shorttitle = {A Study in Transfer Learning},
  author = {Wiens, J. and Guttag, J. and Horvitz, E.},
  year = {2014},
  month = jul,
  journal = {Journal of the American Medical Informatics Association},
  volume = {21},
  number = {4},
  pages = {699--706},
  issn = {1067-5027, 1527-974X},
  doi = {10.1136/amiajnl-2013-002162},
  abstract = {Background Data-driven risk stratification models built using data from a single hospital often have a paucity of training data. However, leveraging data from other hospitals can be challenging owing to institutional differences with patients and with data coding and capture. Objective To investigate three approaches to learning hospital-specific predictions about the risk of hospitalassociated infection with Clostridium difficile, and perform a comparative analysis of the value of different ways of using external data to enhance hospital-specific predictions. Materials and methods We evaluated each approach on 132 853 admissions from three hospitals, varying in size and location. The first approach was a single-task approach, in which only training data from the target hospital (ie, the hospital for which the model was intended) were used. The second used only data from the other two hospitals. The third approach jointly incorporated data from all hospitals while seeking a solution in the target space. Results The relative performance of the three different approaches was found to be sensitive to the hospital selected as the target. However, incorporating data from all hospitals consistently had the highest performance. Discussion The results characterize the challenges and opportunities that come with (1) using data or models from collections of hospitals without adapting them to the site at which the model will be used, and (2) using only local data to build models for small institutions or rare events. Conclusions We show how external data from other hospitals can be successfully and efficiently incorporated into hospital-specific models.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/KVX53GG8/Wiens et al. - 2014 - A study in transfer learning leveraging data from.pdf}
}

@article{winata2020,
  title = {Meta-{{Transfer Learning}} for {{Code-Switched Speech Recognition}}},
  author = {Winata, Genta Indra and Cahyawijaya, Samuel and Lin, Zhaojiang and Liu, Zihan and Xu, Peng and Fung, Pascale},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.14228 [cs, eess]},
  eprint = {2004.14228},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {An increasing number of people in the world today speak a mixed-language as a result of being multilingual. However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets. Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data. Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/taylanot/Zotero/storage/E6VY8FTI/Winata et al. - 2020 - Meta-Transfer Learning for Code-Switched Speech Re.pdf}
}

@article{wulfinghoff2018,
  title = {Model Order Reduction of Nonlinear Homogenization Problems Using a {{Hashin}}\textendash{{Shtrikman}} Type Finite Element Method},
  author = {Wulfinghoff, Stephan and Cavaliere, Fabiola and Reese, Stefanie},
  year = {2018},
  month = mar,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {330},
  pages = {149--179},
  issn = {00457825},
  doi = {10.1016/j.cma.2017.10.019},
  abstract = {This work presents a computational nonlinear homogenization approach, the starting point of which is a model order reduction method based on data-clustering. To this end, the micromechanical data from numerical experiments (snapshots) is analyzed in order to identify characteristic microstructural deformation patterns. These describe how the macroscopic strain typically localizes within the microstructure. The outcome of the procedure is a subdivision of the microstructure into a set of clusters of material points. Within each cluster the strain is then approximated as being constant.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/38WM9GD2/Wulfinghoff et al. - 2018 - Model order reduction of nonlinear homogenization .pdf}
}

@article{xu2020,
  title = {{{MetaFun}}: {{Meta-Learning}} with {{Iterative Functional Updates}}},
  shorttitle = {{{MetaFun}}},
  author = {Xu, Jin and Ton, Jean-Francois and Kim, Hyunjik and Kosiorek, Adam R. and Teh, Yee Whye},
  year = {2020},
  month = aug,
  journal = {arXiv:1912.02738 [cs, stat]},
  eprint = {1912.02738},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We develop a functional encoder-decoder approach to supervised meta-learning, where labeled data is encoded into an infinite-dimensional functional representation rather than a finitedimensional one. Furthermore, rather than directly producing the representation, we learn a neural update rule resembling functional gradient descent which iteratively improves the representation. The final representation is used to condition the decoder to make predictions on unlabeled data. Our approach is the first to demonstrates the success of encoder-decoder style meta-learning methods like conditional neural processes on largescale few-shot classification benchmarks such as miniImageNet and tieredImageNet, where it achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/BBSR7QAX/Xu et al. - 2020 - MetaFun Meta-Learning with Iterative Functional U.pdf}
}

@article{xue2020,
  title = {A Data-Driven Computational Scheme for the Nonlinear Mechanical Properties of Cellular Mechanical Metamaterials under Large Deformation},
  author = {Xue, Tianju and Beatson, Alex and Chiaramonte, Maurizio and Roeder, Geoffrey and Ash, Jordan T. and Menguc, Yigit and Adriaenssens, Sigrid and Adams, Ryan P. and Mao, Sheng},
  year = {2020},
  journal = {Soft Matter},
  volume = {16},
  number = {32},
  pages = {7524--7534},
  issn = {1744-683X, 1744-6848},
  doi = {10.1039/D0SM00488J},
  abstract = {A novel computational scheme using neural networks is proposed to efficiently capture the nonlinear mechanics of soft metamaterials under large deformation.           ,              Cellular mechanical metamaterials are a special class of materials whose mechanical properties are primarily determined by their geometry. However, capturing the nonlinear mechanical behavior of these materials, especially those with complex geometries and under large deformation, can be challenging due to inherent computational complexity. In this work, we propose a data-driven multiscale computational scheme as a possible route to resolve this challenge. We use a neural network to approximate the effective strain energy density as a function of cellular geometry and overall deformation. The network is constructed by ``learning'' from the data generated by finite element calculation of a set of representative volume elements at cellular scales. This effective strain energy density is then used to predict the mechanical responses of cellular materials at larger scales. Compared with direct finite element simulation, the proposed scheme can reduce the computational time up to two orders of magnitude. Potentially, this scheme can facilitate new optimization algorithms for designing cellular materials of highly specific mechanical properties.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/7N29JN3B/Xue et al. - 2020 - A data-driven computational scheme for the nonline.pdf}
}

@article{yamada2019,
  title = {Predicting {{Materials Properties}} with {{Little Data Using Shotgun Transfer Learning}}},
  author = {Yamada, Hironao and Liu, Chang and Wu, Stephen and Koyama, Yukinori and Ju, Shenghong and Shiomi, Junichiro and Morikawa, Junko and Yoshida, Ryo},
  year = {2019},
  month = oct,
  journal = {ACS Central Science},
  volume = {5},
  number = {10},
  pages = {1717--1730},
  issn = {2374-7943, 2374-7951},
  doi = {10.1021/acscentsci.9b00804},
  abstract = {There is a growing demand for the use of machine learning (ML) to derive fast-to-evaluate surrogate models of materials properties. In recent years, a broad array of materials property databases have emerged as part of a digital transformation of materials science. However, recent technological advances in ML are not fully exploited because of the insufficient volume and diversity of materials data. An ML framework called ``transfer learning'' has considerable potential to overcome the problem of limited amounts of materials data. Transfer learning relies on the concept that various property types, such as physical, chemical, electronic, thermodynamic, and mechanical properties, are physically interrelated. For a given target property to be predicted from a limited supply of training data, models of related proxy properties are pretrained using sufficient data; these models capture common features relevant to the target task. Repurposing of such machine-acquired features on the target task yields outstanding prediction performance even with exceedingly small data sets, as if highly experienced human experts can make rational inferences even for considerably less experienced tasks. In this study, to facilitate widespread use of transfer learning, we develop a pretrained model library called XenonPy.MDL. In this first release, the library comprises more than 140 000 pretrained models for various properties of small molecules, polymers, and inorganic crystalline materials. Along with these pretrained models, we describe some outstanding successes of transfer learning in different scenarios such as building models with only dozens of materials data, increasing the ability of extrapolative prediction through a strategic model transfer, and so on. Remarkably, transfer learning has autonomously identified rather nontrivial transferability across different properties transcending the different disciplines of materials science; for example, our analysis has revealed underlying bridges between small molecules and polymers and between organic and inorganic chemistry.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/NYKGP6NA/Yamada et al. - 2019 - Predicting Materials Properties with Little Data U.pdf}
}

@article{yang2019,
  title = {Adversarial Uncertainty Quantification in Physics-Informed Neural Networks},
  author = {Yang, Yibo and Perdikaris, Paris},
  year = {2019},
  month = oct,
  journal = {Journal of Computational Physics},
  volume = {394},
  pages = {136--152},
  issn = {00219991},
  doi = {10.1016/j.jcp.2019.05.027},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/XDI2MHB6/Yang and Perdikaris - 2019 - Adversarial uncertainty quantification in physics-.pdf}
}

@book{yang2020,
  title = {Transfer {{Learning}}},
  author = {Yang, Qiang and Zhang, Yu and Dai, Wenyuan and Pan, Sinno Jialin},
  year = {2020},
  month = jan,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781139061773},
  isbn = {978-1-139-06177-3 978-1-107-01690-3},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/YQVIQHT3/Yang et al. - 2020 - Transfer Learning.pdf}
}

@article{yao2019,
  title = {Hierarchically {{Structured Meta-learning}}},
  author = {Yao, Huaxiu and Wei, Ying and Huang, Junzhou and Li, Zhenhui},
  year = {2019},
  month = nov,
  journal = {arXiv:1905.05301 [cs, stat]},
  eprint = {1905.05301},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In order to learn quickly with few samples, metalearning utilizes prior knowledge learned from previous tasks. However, a critical challenge in meta-learning is task uncertainty and heterogeneity, which can not be handled via globally sharing knowledge among tasks. In this paper, based on gradient-based meta-learning, we propose a hierarchically structured meta-learning (HSML) algorithm that explicitly tailors the transferable knowledge to different clusters of tasks. Inspired by the way human beings organize knowledge, we resort to a hierarchical task clustering structure to cluster tasks. As a result, the proposed approach not only addresses the challenge via the knowledge customization to different clusters of tasks, but also preserves knowledge generalization among a cluster of similar tasks. To tackle the changing of task relationship, in addition, we extend the hierarchical structure to a continual learning environment. The experimental results show that our approach can achieve state-of-the-art performance in both toy-regression and few-shot image classification problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/QL76FC4U/Yao et al. - 2019 - Hierarchically Structured Meta-learning.pdf}
}

@article{yao2022,
  title = {Meta-{{Learning}} with {{Fewer Tasks}} through {{Task Interpolation}}},
  author = {Yao, Huaxiu and Zhang, Linjun and Finn, Chelsea},
  year = {2022},
  month = mar,
  journal = {arXiv:2106.02695 [cs]},
  eprint = {2106.02695},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/FYBCKAS5/Yao et al. - 2022 - Meta-Learning with Fewer Tasks through Task Interp.pdf}
}

@article{ye2018,
  title = {Deep Neural Networks for Accurate Predictions of Crystal Stability},
  author = {Ye, Weike and Chen, Chi and Wang, Zhenbin and Chu, Iek-Heng and Ong, Shyue Ping},
  year = {2018},
  month = dec,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {3800},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-06322-x},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/PYFL4SW3/Ye et al. - 2018 - Deep neural networks for accurate predictions of c.pdf}
}

@inproceedings{yim2017,
  title = {A {{Gift}} from {{Knowledge Distillation}}: {{Fast Optimization}}, {{Network Minimization}} and {{Transfer Learning}}},
  shorttitle = {A {{Gift}} from {{Knowledge Distillation}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
  year = {2017},
  month = jul,
  pages = {7130--7138},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.754},
  abstract = {We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN. As the DNN maps from the input space to the output space through many layers sequentially, we define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers. When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the flow between two layers exhibits three important phenomena: (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model; (2) the student DNN outperforms the original DNN; and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/ICQ7S6JK/Yim et al. - 2017 - A Gift from Knowledge Distillation Fast Optimizat.pdf}
}

@article{yin2020,
  title = {Meta-{{Learning}} without {{Memorization}}},
  author = {Yin, Mingzhang and Tucker, George and Zhou, Mingyuan and Levine, Sergey and Finn, Chelsea},
  year = {2020},
  month = apr,
  journal = {arXiv:1912.03820 [cs, stat]},
  eprint = {1912.03820},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradientbased meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/3PPYRZFB/Yin et al. - 2020 - Meta-Learning without Memorization.pdf}
}

@inproceedings{younger2001,
  title = {Meta-Learning with Backpropagation},
  booktitle = {{{IJCNN}}'01. {{International Joint Conference}} on {{Neural Networks}}. {{Proceedings}} ({{Cat}}. {{No}}.{{01CH37222}})},
  author = {Younger, A.S. and Hochreiter, S. and Conwell, P.R.},
  year = {2001},
  volume = {3},
  pages = {2001--2006},
  publisher = {{IEEE}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/IJCNN.2001.938471},
  abstract = {This paper introduces gradient descent methods applied to meta-leaming (leaming how to leam) in Neural Networks. Meta-leaning has been of interest in the machine leaming field for decades because of its appealing applications to intelligent agents, non-stationary time series, autonomous robots, and improved leaming algorithms. Many previous neural network-based approaches toward meta-leaming have been based on evolutionary methods. We show how to use gradient descent for meta-leaming in recurrent neural networks. Based on previous work on Fixed-Weight Leaming Neural Networks, we hypothesize that any recurrent network topology and its corresponding leaming algorithm(s) is a potential meta-leaming system. We tested several recurrent neural network topologies and their corresponding forms of Backpropagation for their ability to meta-leam. One of our systems, based on the Long Short-Term Memory neural network developed a leaming algorithm that could leam any two-dimensional quadratic function (from a set of such functions\vphantom\{\} after only 30 training examples.},
  isbn = {978-0-7803-7044-9},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/KZUW8LQ9/Younger et al. - 2001 - Meta-learning with backpropagation.pdf}
}

@inproceedings{yu2005,
  title = {Learning {{Gaussian}} Processes from Multiple Tasks},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning  - {{ICML}} '05},
  author = {Yu, Kai and Tresp, Volker and Schwaighofer, Anton},
  year = {2005},
  pages = {1012--1019},
  publisher = {{ACM Press}},
  address = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102479},
  abstract = {We consider the problem of multi-task learning, that is, learning multiple related functions. Our approach is based on a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes (GPs). The resulting models can be learned easily via an EM-algorithm. Empirical studies on multi-label text categorization suggest that the presented models allow accurate solutions of these multi-task problems.},
  isbn = {978-1-59593-180-1},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/T39SBU24/Yu et al. - 2005 - Learning Gaussian processes from multiple tasks.pdf}
}

@article{yu2022,
  title = {Gradient-Enhanced Physics-Informed Neural Networks for Forward and Inverse {{PDE}} Problems},
  author = {Yu, Jeremy and Lu, Lu and Meng, Xuhui and Karniadakis, George Em},
  year = {2022},
  month = apr,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {393},
  eprint = {2111.02801},
  eprinttype = {arxiv},
  pages = {114823},
  issn = {00457825},
  doi = {10.1016/j.cma.2022.114823},
  abstract = {Deep learning has been shown to be an effective tool in solving partial differential equations (PDEs) through physics-informed neural networks (PINNs). PINNs embed the PDE residual into the loss function of the neural network, and have been successfully employed to solve diverse forward and inverse PDE problems. However, one disadvantage of the first generation of PINNs is that they usually have limited accuracy even with many training points. Here, we propose a new method, gradient-enhanced physics-informed neural networks (gPINNs), for improving the accuracy and training efficiency of PINNs. gPINNs leverage gradient information of the PDE residual and embed the gradient into the loss function. We tested gPINNs extensively and demonstrated the effectiveness of gPINNs in both forward and inverse PDE problems. Our numerical results show that gPINN performs better than PINN with fewer training points. Furthermore, we combined gPINN with the method of residual-based adaptive refinement (RAR), a method for improving the distribution of training points adaptively during training, to further improve the performance of gPINN, especially in PDEs with solutions that have steep gradients.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics},
  file = {/home/taylanot/Zotero/storage/LIFQUBTU/Yu et al. - 2022 - Gradient-enhanced physics-informed neural networks.pdf}
}

@article{yuan2020,
  title = {Few {{Is Enough}}: {{Task-Augmented Active Meta-Learning}} for {{Brain Cell Classification}}},
  shorttitle = {Few {{Is Enough}}},
  author = {Yuan, Pengyu and Mobiny, Aryan and Jahanipour, Jahandar and Li, Xiaoyang and Cicalese, Pietro Antonio and Roysam, Badrinath and Patel, Vishal and Dragan, Maric and Van Nguyen, Hien},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.05009 [cs]},
  eprint = {2007.05009},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep Neural Networks (or DNNs) must constantly cope with distribution changes in the input data when the task of interest or the data collection protocol changes. Retraining a network from scratch to combat this issue poses a significant cost. Meta-learning aims to deliver an adaptive model that is sensitive to these underlying distribution changes, but requires many tasks during the meta-training process. In this paper, we propose a tAsk-auGmented actIve meta-LEarning (AGILE) method to efficiently adapt DNNs to new tasks by using a small number of training examples. AGILE combines a meta-learning algorithm with a novel task augmentation technique which we use to generate an initial adaptive model. It then uses Bayesian dropout uncertainty estimates to actively select the most difficult samples when updating the model to a new task. This allows AGILE to learn with fewer tasks and a few informative samples, achieving high performance with a limited dataset. We perform our experiments using the brain cell classification task and compare the results to a plain meta-learning model trained from scratch. We show that the proposed task-augmented meta-learning framework can learn to classify new cell types after a single gradient step with a limited number of training samples. We show that active learning with Bayesian uncertainty can further improve the performance when the number of training samples is extremely small. Using only 1\% of the training data and a single update step, we achieved 90\% accuracy on the new cell type classification task, a 50\% points improvement over a state-of-the-art meta-learning algorithm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/taylanot/Zotero/storage/6DI3VKKN/Yuan et al. - 2020 - Few Is Enough Task-Augmented Active Meta-Learning.pdf}
}

@book{yvonnet2019,
  title = {Computational {{Homogenization}} of {{Heterogeneous Materials}} with {{Finite Elements}}},
  author = {Yvonnet, Julien},
  year = {2019},
  series = {Solid {{Mechanics}} and {{Its Applications}}},
  volume = {258},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-18383-7},
  isbn = {978-3-030-18382-0 978-3-030-18383-7},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/TPC7AY4Y/Yvonnet - 2019 - Computational Homogenization of Heterogeneous Mate.pdf}
}

@article{zamir,
  title = {Taskonomy: {{Disentangling Task Transfer Learning}}},
  author = {Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
  pages = {12},
  abstract = {Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, in order to, for instance, seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/LZ92W3S3/Zamir et al. - Taskonomy Disentangling Task Transfer Learning.pdf}
}

@article{zhang2021,
  title = {A {{Survey}} on {{Multi-Task Learning}}},
  author = {Zhang, Yu and Yang, Qiang},
  year = {2021},
  month = mar,
  journal = {arXiv:1707.08114 [cs]},
  eprint = {1707.08114},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/8NDUAXXQ/Zhang and Yang - 2021 - A Survey on Multi-Task Learning.pdf}
}

@article{zhang2021a,
  title = {Hierarchical Deep-Learning Neural Networks: Finite Elements and Beyond},
  shorttitle = {Hierarchical Deep-Learning Neural Networks},
  author = {Zhang, Lei and Cheng, Lin and Li, Hengyang and Gao, Jiaying and Yu, Cheng and Domel, Reno and Yang, Yang and Tang, Shaoqiang and Liu, Wing Kam},
  year = {2021},
  month = jan,
  journal = {Computational Mechanics},
  volume = {67},
  number = {1},
  pages = {207--230},
  issn = {0178-7675, 1432-0924},
  doi = {10.1007/s00466-020-01928-9},
  abstract = {The hierarchical deep-learning neural network (HiDeNN) is systematically developed through the construction of structured deep neural networks (DNNs) in a hierarchical manner, and a special case of HiDeNN for representing Finite Element Method (or HiDeNN-FEM in short) is established. In HiDeNN-FEM, weights and biases are functions of the nodal positions, hence the training process in HiDeNN-FEM includes the optimization of the nodal coordinates. This is the spirit of r-adaptivity, and it increases both the local and global accuracy of the interpolants. By fixing the number of hidden layers and increasing the number of neurons by training the DNNs, rh-adaptivity can be achieved, which leads to further improvement of the accuracy for the solutions. The generalization of rational functions is achieved by the development of three fundamental building blocks of constructing deep hierarchical neural networks. The three building blocks are linear functions, multiplication, and inversion. With these building blocks, the class of deep learning interpolation functions are demonstrated for interpolation theories such as Lagrange polynomials, NURBS, isogeometric, reproducing kernel particle method, and others. In HiDeNNFEM, enrichment functions through the multiplication of neurons is equivalent to the enrichment in standard finite element methods, that is, generalized, extended, and partition of unity finite element methods. Numerical examples performed by HiDeNN-FEM exhibit reduced approximation error compared with the standard FEM. Finally, an outlook for the generalized HiDeNN to high-order continuity for multiple dimensions and topology optimizations are illustrated through the hierarchy of the proposed DNNs.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/2EYL428M/Zhang et al. - 2021 - Hierarchical deep-learning neural networks finite.pdf}
}

@article{zhao2014,
  title = {Online {{Transfer Learning}}},
  author = {Zhao, Peilin and Hoi, Steven C.H. and Wang, Jialei and Li, Bin},
  year = {2014},
  month = nov,
  journal = {Artificial Intelligence},
  volume = {216},
  pages = {76--102},
  issn = {00043702},
  doi = {10.1016/j.artint.2014.06.003},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/2DM9J2NW/Zhao et al. - 2014 - Online Transfer Learning.pdf}
}

@article{zhao2021,
  title = {Active- and Transfer-Learning Applied to Microscale-Macroscale Coupling to Simulate Viscoelastic Flows},
  author = {Zhao, Lifei and Li, Zhen and Wang, Zhicheng and Caswell, Bruce and Ouyang, Jie and Karniadakis, George Em},
  year = {2021},
  month = feb,
  journal = {Journal of Computational Physics},
  volume = {427},
  eprint = {2005.04382},
  eprinttype = {arxiv},
  pages = {110069},
  issn = {00219991},
  doi = {10.1016/j.jcp.2020.110069},
  abstract = {Active- and transfer-learning are applied to microscale dynamics of polymer flows for the multiscale discovery of effective constitutive approximations required in viscoelastic flow simulation. The result is macroscopic rheology directly connected to a microstructural model. Micro and macroscale simulations are adaptively coupled by means of Gaussian process regression (GPR) to run the expensive microscale computations only as necessary. This multiscale method is demonstrated with flows of a polymer solution as a model system. At the microscale level dissipative particle dynamics (DPD) is employed to model the fluid as a suspension of beadspring micro-structures subjected to steady shear flow. The results yield the non-Newtonian viscosity and the first normal stress difference at strain rates as training data used in a GPR model. DPD parameters are calibrated with respect to experimental data for a real polymer solution. Compliance with these data requires adjustment of the DPD model's cutoff radius, which then becomes a function of the second invariant of the strain rate tensor. The FENE-P model is chosen for the macroscale description using the spectral element method (SEM) to simulate channel flow and flow past a circular cylinder. The DPD results at the lowest possible shear strain rate yield an estimate of the zero-shear rate viscosity, which allows the initiation of the macroscale flow by SEM as a Newtonian fluid. The resulting strain-rate field is surveyed to determine additional shear strain rate sampling points for the DPD system. This new information allows an initial fitting of parameters of the constitutive equation followed by new SEM simulations at the macroscale. Guided by active-learning GPR to select new sampling points, this process continues until convergence is achieved.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Physics - Computational Physics,Physics - Fluid Dynamics},
  file = {/home/taylanot/Zotero/storage/MSDP7U95/Zhao et al. - 2021 - Active- and transfer-learning applied to microscal.pdf}
}

@article{zhou,
  title = {Task {{Similarity Aware Meta Learning}}: {{Theory-inspired Improvement}} on {{MAML}}},
  author = {Zhou, Pan and Zou, Yingtian and Yuan, Xiao-Tong and Feng, Jiashi and Xiong, Caiming and Hoi, Steven},
  pages = {11},
  abstract = {Few-shot learning ability is heavily desired for machine intelligence. By meta-learning a model initialization from training tasks with fast adaptation ability to new tasks, model-agnostic meta-learning (MAML) has achieved remarkable success in a number of few-shot learning applications. However, theoretical understandings on the learning ability of MAML remain absent yet, hindering developing new and more advanced meta learning methods in a principled way. In this work, we solve this problem by theoretically justifying the fast adaptation capability of MAML when applied to new tasks. Specifically, we prove that the learnt meta-initialization can benefit the fast adaptation to new tasks with only a few steps of gradient descent. This result explicitly reveals the benefits of the unique designs in MAML. Then we propose a theory-inspired task similarity aware MAML which clusters tasks into multiple groups according to the estimated optimal model parameters and learns group-specific initializations. The proposed method improves upon MAML by speeding up the adaptation and giving stronger few-shot learning ability. Experimental results on the few-shot classification tasks testify its advantages.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/2MTLZATI/Zhou et al. - Task Similarity Aware Meta Learning Theory-inspir.pdf}
}

@article{zhou2018,
  title = {Deep {{Meta-Learning}}: {{Learning}} to {{Learn}} in the {{Concept Space}}},
  shorttitle = {Deep {{Meta-Learning}}},
  author = {Zhou, Fengwei and Wu, Bin and Li, Zhenguo},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.03596 [cs]},
  eprint = {1802.03596},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Few-shot learning remains challenging for metalearning that learns a learning algorithm (metalearner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep metalearning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs fewshot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53\% and 56.53\% to 58.18\% and 63.47\%, improves MAML from 49.28\% and 50.45\% to 56.65\% and 64.63\%, and improves Meta-SGD from 53.83\% and 53.34\% to 61.62\% and 66.95\%, respectively.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/taylanot/Zotero/storage/8BTPIUKF/Zhou et al. - 2018 - Deep Meta-Learning Learning to Learn in the Conce.pdf}
}

@article{zhu2018,
  title = {Bayesian Deep Convolutional Encoder\textendash Decoder Networks for Surrogate Modeling and Uncertainty Quantification},
  author = {Zhu, Yinhao and Zabaras, Nicholas},
  year = {2018},
  month = aug,
  journal = {Journal of Computational Physics},
  volume = {366},
  pages = {415--447},
  issn = {00219991},
  doi = {10.1016/j.jcp.2018.04.018},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/SMMYWJ39/Zhu and Zabaras - 2018 - Bayesian deep convolutional encoderâ€“decoder networ.pdf}
}

@article{zhu2019,
  title = {Physics-Constrained Deep Learning for High-Dimensional Surrogate Modeling and Uncertainty Quantification without Labeled Data},
  author = {Zhu, Yinhao and Zabaras, Nicholas and Koutsourelakis, Phaedon-Stelios and Perdikaris, Paris},
  year = {2019},
  month = oct,
  journal = {Journal of Computational Physics},
  volume = {394},
  pages = {56--81},
  issn = {00219991},
  doi = {10.1016/j.jcp.2019.05.024},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/BTZUBSZF/Zhu et al. - 2019 - Physics-constrained deep learning for high-dimensi.pdf}
}

@article{zhu2020,
  title = {Latent Variable Regression for Supervised Modeling and Monitoring},
  author = {Zhu, Qinqin},
  year = {2020},
  month = may,
  journal = {IEEE/CAA Journal of Automatica Sinica},
  volume = {7},
  number = {3},
  pages = {800--811},
  issn = {2329-9266, 2329-9274},
  doi = {10.1109/JAS.2020.1003153},
  abstract = {A latent variable regression algorithm with a regularization term (rLVR) is proposed in this paper to extract latent relations between process data X and quality data Y. In rLVR, the prediction error between X and Y is minimized, which is proved to be equivalent to maximizing the projection of quality variables in the latent space. The geometric properties and model relations of rLVR are analyzed, and the geometric and theoretical relations among rLVR, partial least squares, and canonical correlation analysis are also presented. The rLVR-based monitoring framework is developed to monitor process-relevant and quality-relevant variations simultaneously. The prediction and monitoring effectiveness of rLVR algorithm is demonstrated through both numerical simulations and the Tennessee Eastman (TE) process.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/X456AD8Q/Zhu - 2020 - Latent variable regression for supervised modeling.pdf}
}

@article{zhuang2020,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  year = {2020},
  month = jun,
  journal = {arXiv:1911.02685 [cs, stat]},
  eprint = {1911.02685},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/taylanot/Zotero/storage/34KKTCQZ/Zhuang et al. - 2020 - A Comprehensive Survey on Transfer Learning.pdf}
}

@article{zhuang2021,
  title = {Deep Autoencoder Based Energy Method for the Bending, Vibration, and Buckling Analysis of {{Kirchhoff}} Plates with Transfer Learning},
  author = {Zhuang, Xiaoying and Guo, Hongwei and Alajlan, Naif and Zhu, Hehua and Rabczuk, Timon},
  year = {2021},
  month = may,
  journal = {European Journal of Mechanics - A/Solids},
  volume = {87},
  pages = {104225},
  issn = {09977538},
  doi = {10.1016/j.euromechsol.2021.104225},
  abstract = {In this paper, we present a deep autoencoder based energy method (DAEM) for the bending, vibration and buckling analysis of Kirchhoff plates. The DAEM exploits the higher order continuity of the DAEM and integrates a deep autoencoder and the minimum total potential principle in one framework yielding an unsupervised feature learning method. The DAEM is a specific type of feedforward deep neural network (DNN) and can also serve as function approximator. With robust feature extraction capacity, the DAEM can more efficiently identify patterns behind the whole energy system, such as the field variables, natural frequency and critical buckling load factor studied in this paper. The objective function is to minimize the total potential energy. The DAEM performs unsupervised learning based on generated collocation points inside the physical domain so that the total potential energy is minimized at all points. For the vibration and buckling analysis, the loss function is constructed based on Rayleigh's principle and the fundamental frequency and the critical buckling load is extracted. A scaled hyperbolic tangent activation function for the underlying mechanical model is presented which meets the continuity requirement and alleviates the gradient vanishing/explosive problems under bending. The DAEM is implemented using Pytorch and the LBFGS optimizer. To further improve the computational efficiency and enhance the generality of this machine learning method, we employ transfer learning. A comprehensive study of the DAEM configuration is performed for several numerical examples with various geometries, load conditions, and boundary conditions.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/JZTR8RCW/Zhuang et al. - 2021 - Deep autoencoder based energy method for the bendi.pdf}
}

@article{zisis2015,
  title = {Evaluation of Material Properties of Incompressible Hyperelastic Materials Based on Instrumented Indentation of an Equal-Biaxial Prestretched Substrate},
  author = {Zisis, Th. and Zafiropoulou, V.I. and Giannakopoulos, A.E.},
  year = {2015},
  month = jul,
  journal = {International Journal of Solids and Structures},
  volume = {64--65},
  pages = {132--144},
  issn = {00207683},
  doi = {10.1016/j.ijsolstr.2015.03.019},
  abstract = {It is well known that instrumented indentation tests are useful tools in probing mechanical properties of materials such as metals and ceramics. Instrumented indentation of hyperelastic materials such as rubbers, bio-materials, tissues etc has not been examined in depth, especially the inverse problem of material characterization from instrumented indentation response. The difficulty of the inverse problem for such materials is that the unknown property is a function, the elastic energy density function. There are several such functions and each function is often characterized by more than one parameter. If the maximum indentation depth is low, we have shown that instrumented indentation of initially unstretched hyperelastic materials can only resolve a combination of the material parameters. If the maximum indentation depth is high, the indentation can provide independent material properties, however not in a unique way. Moreover, high indentation loads could lead to surface puncturing and so blur the test results. In this work, we show that we can use spherical indentation of a substrate at different but known prestretch levels to obtain the involved material properties of the energy density function. The present methodology can also incorporate a limit energy failure criterion and instrumented indentation can incorporate this behavior which we may call indentation strength.},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/DL5YMKWJ/Zisis et al. - 2015 - Evaluation of material properties of incompressibl.pdf}
}

@article{zotero-768,
  title = {Hints from {{Probability}} and {{Statistics}}},
  pages = {12},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/33J8Y4VP/Hints from Probability and Statistics.pdf}
}

@article{zotero-769,
  title = {Linear {{Algebra Basics}}},
  pages = {3},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/QI8C934G/Linear Algebra Basics.pdf}
}

@article{zotero-770,
  title = {Cost {{Function Optimization}}},
  pages = {16},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/KKB95PQS/Cost Function Optimization.pdf}
}

@article{zotero-771,
  title = {Basic {{Definitions}} from {{Linear Systems Theory}}},
  pages = {3},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/S7ZSS3CE/Basic Definitions from Linear Systems Theory.pdf}
}

@article{zotero-772,
  title = {Classifiers {{Based}} on {{Bayes Decision Theory}}},
  pages = {77},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/MRLSD4HU/Classifiers Based on Bayes Decision Theory.pdf}
}

@article{zotero-773,
  title = {Linear {{Classifiers}}},
  pages = {60},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/EM86G7MY/Linear Classifiers.pdf}
}

@article{zotero-774,
  title = {Nonlinear {{Classifiers}}},
  pages = {110},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/6BWNYMI9/Nonlinear Classifiers.pdf}
}

@article{zotero-775,
  title = {Feature {{Selection}}},
  pages = {62},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/4498XZEH/Feature Selection.pdf}
}

@article{zotero-777,
  title = {Feature {{Generation II}}},
  pages = {69},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/2SFYV4NY/Feature Generation II.pdf}
}

@article{zotero-778,
  title = {Template {{Matching}}},
  pages = {39},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/GDXANPJZ/Template Matching.pdf}
}

@article{zotero-779,
  title = {Context-{{Dependent Classification}}},
  pages = {45},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/3YEMYB9B/Context-Dependent Classification.pdf}
}

@article{zotero-780,
  title = {Supervised {{Learning}}: {{The Epilogue}}},
  pages = {28},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/KMQ7GRTM/Supervised Learning The Epilogue.pdf}
}

@article{zotero-781,
  title = {Clustering: {{Basic Concepts}}},
  pages = {31},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/DK3A5NDS/Clustering Basic Concepts.pdf}
}

@article{zotero-782,
  title = {Clustering {{Algorithms I}}: {{Sequential Algorithms}}},
  pages = {26},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/KCCQTFS8/Clustering Algorithms I Sequential Algorithms.pdf}
}

@article{zotero-783,
  title = {Clustering {{Algorithms II}}: {{Hierarchical Algorithms}}},
  pages = {48},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/NYJAHJQZ/Clustering Algorithms II Hierarchical Algorithms.pdf}
}

@article{zotero-785,
  title = {Cluster {{Validity}}},
  pages = {51},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/6XD3VMMV/Cluster Validity.pdf}
}

@incollection{zwart2016,
  title = {Methodological {{Classification}} of {{Innovative Engineering Projects}}},
  booktitle = {Philosophy of {{Technology}} after the {{Empirical Turn}}},
  author = {Zwart, Sjoerd D. and {\noopsort{vries}}{de Vries}, Marc J.},
  editor = {Franssen, Maarten and Vermaas, Pieter E. and Kroes, Peter and Meijers, Anthonie W.M.},
  year = {2016},
  volume = {23},
  pages = {219--248},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-33717-3_13},
  abstract = {In this chapter we report on and discuss our empirical classification of innovative engineering projects. Basic innovative engineering projects are characterized by their overall goal and accompanying method. On the basis of this goal and method, we classify engineering projects as all falling in one of the following categories: (1) Descriptive knowledge as prevalent in the descriptive sciences; (2) Design of artefacts and processes; (3) Engineering Means-end knowledge; (4) Modeling (simulation serious gaming included); (5) Engineering optimization; and (6) Engineering mathematics. These categories are illustrated with examples drawn from our educational experiences. Formally our classification system is a partition: the categories are mutually exclusive and collectively exhaustive. Regarding its empirical power, we claim intra-departmental completeness for the projects that we have studied at the Departments of Mechanics and Applied Physics of Delft University of Technology; we hypothesize intra-academic completeness within Universities of Technology; and we hope for and encourage investigating extraacademic completeness regarding engineering in industry. Besides having significant consequences for the methodology of the engineering sciences, our categorization provides a new way to study empirically the relation between science and technology.},
  isbn = {978-3-319-33716-6 978-3-319-33717-3},
  langid = {english},
  file = {/home/taylanot/Zotero/storage/3N39VKXV/Zwart and de Vries - 2016 - Methodological Classification of Innovative Engine.pdf}
}

@preamble{ "\providecommand{\noopsort}[1]{} " }

