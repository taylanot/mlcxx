\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{thrun1998}
\citation{finn2017}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {title}{Contribution Title\unskip {}}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{First Author \and Second Author \and Third Author}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1.1}{}}
\citation{flennerhag2019,nichol2018,rajasegaran2020,collins2020,guiroy2019}
\citation{finn2017}
\citation{finn2017}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model-Agnostic Meta-Learning (MAML)}{2}{section.1.2}\protected@file@percent }
\newlabel{sec:maml}{{2}{2}{Model-Agnostic Meta-Learning (MAML)}{section.1.2}{}}
\citation{flennerhag2019,collins2020}
\citation{antoniou2019}
\citation{nichol2018}
\citation{grant2018}
\citation{finn2019,rajasegaran2020}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces MAML\cite  {finn2017} Algorithm\relax }}{3}{algocf.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:MAML}{{1}{3}{Model-Agnostic Meta-Learning (MAML)}{algocf.1}{}}
\newlabel{fig:lin_maml}{{1a}{3}{Linear Problem\relax }{figure.caption.3}{}}
\newlabel{sub@fig:lin_maml}{{a}{3}{Linear Problem\relax }{figure.caption.3}{}}
\newlabel{fig:nonlin_maml}{{1b}{3}{Nonlinear Problem\relax }{figure.caption.3}{}}
\newlabel{sub@fig:nonlin_maml}{{b}{3}{Nonlinear Problem\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualizing the MAML intermediate model. 100 sample tasks drawn from $p_\mathcal  {T}$ for both linear ($m=0$ and $c=1$) and nonlinear ($c_1=2$ and $c_2=2$) problems shown transparent and intermediate model trained (obtained from MAML algorithm) for 1D cases of the experimentation given with solid dark orange line. The linear and the nonlinear problems are provided in Section \ref  {sec:methods}.\relax }}{3}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{3}{section.1.3}\protected@file@percent }
\newlabel{sec:rw}{{3}{3}{Related Work}{section.1.3}{}}
\citation{li2017}
\citation{behl2019}
\citation{lake2019}
\citation{finn2017}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setting}{4}{section.1.4}\protected@file@percent }
\newlabel{sec:methods}{{4}{4}{Experimental Setting}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Learning Problems}{4}{subsection.1.4.1}\protected@file@percent }
\newlabel{eq:linearreg}{{1}{4}{Learning Problems}{equation.1.4.1}{}}
\newlabel{eq:nonlinearreg}{{2}{5}{Learning Problems}{equation.1.4.2}{}}
\newlabel{eq:ee}{{3}{5}{Learning Problems}{equation.1.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Models}{5}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Estimator}{5}{section*.5}\protected@file@percent }
\newlabel{fig:lintasks}{{2a}{6}{$\lab = \trans {\inp }\scaletask $\relax }{figure.caption.4}{}}
\newlabel{sub@fig:lintasks}{{a}{6}{$\lab = \trans {\inp }\scaletask $\relax }{figure.caption.4}{}}
\newlabel{fig:nonlintasks}{{2b}{6}{$\lab = \trans {\sine (\inp +\phasetask )}\scaletask $\relax }{figure.caption.4}{}}
\newlabel{sub@fig:nonlintasks}{{b}{6}{$\lab = \trans {\sine (\inp +\phasetask )}\scaletask $\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 100 sample tasks drawn from $p_\mathcal  {T}$ for both linear ($m=0$ and $c=1$) and nonlinear ($c_1=1$ and $c_2=1$) problems.\relax }}{6}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ridge Estimator}{6}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Kernel Ridge Estimator}{6}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient Descent Estimator}{6}{section*.8}\protected@file@percent }
\citation{finn2017}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results and Discussion}{7}{section.1.5}\protected@file@percent }
\newlabel{sec:resdis}{{5}{7}{Results and Discussion}{section.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Linear Problem}{7}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Training Samples $N$:}{8}{section*.10}\protected@file@percent }
\newlabel{fig:linear-N-D-1}{{3a}{8}{$D=1$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:linear-N-D-1}{{a}{8}{$D=1$\relax }{figure.caption.11}{}}
\newlabel{fig:linear-N-D-10}{{3b}{8}{$D=10$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:linear-N-D-10}{{b}{8}{$D=10$\relax }{figure.caption.11}{}}
\newlabel{fig:linear-N-D-50}{{3c}{8}{$D=50$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:linear-N-D-50}{{c}{8}{$D=50$\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The expected performance for changing the number of training samples for various dimensional problems.\relax }}{8}{figure.caption.11}\protected@file@percent }
\newlabel{fig:linear-N}{{3}{8}{The expected performance for changing the number of training samples for various dimensional problems.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Dimensionality $D$:}{8}{section*.12}\protected@file@percent }
\newlabel{fig:linear-D-N-1}{{4a}{9}{$N=1$\relax }{figure.caption.13}{}}
\newlabel{sub@fig:linear-D-N-1}{{a}{9}{$N=1$\relax }{figure.caption.13}{}}
\newlabel{fig:linear-D-N-10}{{4b}{9}{$N=10$\relax }{figure.caption.13}{}}
\newlabel{sub@fig:linear-D-N-10}{{b}{9}{$N=10$\relax }{figure.caption.13}{}}
\newlabel{fig:linear-D-N-50}{{4c}{9}{$N=50$\relax }{figure.caption.13}{}}
\newlabel{sub@fig:linear-D-N-50}{{c}{9}{$N=50$\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The expected performance for changing the number of dimensions for the various number of training points.\relax }}{9}{figure.caption.13}\protected@file@percent }
\newlabel{fig:linear-D}{{4}{9}{The expected performance for changing the number of dimensions for the various number of training points.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Task Variance $c$:}{9}{section*.14}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Mean expected performance for the range $c:[0,1]$ range with various gradient steps for the MAML and GD models with $\eta =0.334$. Note that only $D=1$, $N=10$ case (see Figure \ref  {fig:linear-c-N-10-D-1}) is presented.\relax }}{9}{table.caption.15}\protected@file@percent }
\newlabel{tab:zoom}{{1}{9}{Mean expected performance for the range $c:[0,1]$ range with various gradient steps for the MAML and GD models with $\lr =0.334$. Note that only $D=1$, $N=10$ case (see Figure \ref {fig:linear-c-N-10-D-1}) is presented.\relax }{table.caption.15}{}}
\newlabel{fig:linear-c-N-1-D-1}{{5a}{10}{$D=1$, $N=1$\relax }{figure.caption.16}{}}
\newlabel{sub@fig:linear-c-N-1-D-1}{{a}{10}{$D=1$, $N=1$\relax }{figure.caption.16}{}}
\newlabel{fig:linear-c-N-10-D-1}{{5b}{10}{$D=1$, $N=10$\relax }{figure.caption.16}{}}
\newlabel{sub@fig:linear-c-N-10-D-1}{{b}{10}{$D=1$, $N=10$\relax }{figure.caption.16}{}}
\newlabel{fig:linear-c-N-50-D-1}{{5c}{10}{$D=1$, $N=50$\relax }{figure.caption.16}{}}
\newlabel{sub@fig:linear-c-N-50-D-1}{{c}{10}{$D=1$, $N=50$\relax }{figure.caption.16}{}}
\newlabel{fig:linear-c-N-10-D-10}{{5d}{10}{$D=10$, $N=10$\relax }{figure.caption.16}{}}
\newlabel{sub@fig:linear-c-N-10-D-10}{{d}{10}{$D=10$, $N=10$\relax }{figure.caption.16}{}}
\newlabel{fig:linear-c-N-10-D-50}{{5e}{10}{$D=50$, $N=10$\relax }{figure.caption.16}{}}
\newlabel{sub@fig:linear-c-N-10-D-50}{{e}{10}{$D=50$, $N=10$\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The expected performance for changing the number of training samples for various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref  {fig:linear-c-N-1-D-1}, \ref  {fig:linear-c-N-10-D-1}, \ref  {fig:linear-c-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref  {fig:linear-c-N-10-D-1}, \ref  {fig:linear-c-N-10-D-10}, \ref  {fig:linear-c-N-10-D-50}.\relax }}{10}{figure.caption.16}\protected@file@percent }
\newlabel{fig:linear-c}{{5}{10}{The expected performance for changing the number of training samples for various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref {fig:linear-c-N-1-D-1}, \ref {fig:linear-c-N-10-D-1}, \ref {fig:linear-c-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref {fig:linear-c-N-10-D-1}, \ref {fig:linear-c-N-10-D-10}, \ref {fig:linear-c-N-10-D-50}.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Number of Gradient Steps $n_{{iter}}$:}{10}{section*.17}\protected@file@percent }
\newlabel{fig:linear-n_iter-N-1-D-1}{{6a}{11}{$D=1$, $N=1$\relax }{figure.caption.18}{}}
\newlabel{sub@fig:linear-n_iter-N-1-D-1}{{a}{11}{$D=1$, $N=1$\relax }{figure.caption.18}{}}
\newlabel{fig:linear-n_iter-N-10-D-1}{{6b}{11}{$D=1$, $N=10$\relax }{figure.caption.18}{}}
\newlabel{sub@fig:linear-n_iter-N-10-D-1}{{b}{11}{$D=1$, $N=10$\relax }{figure.caption.18}{}}
\newlabel{fig:linear-n_iter-N-50-D-1}{{6c}{11}{$D=1$, $N=50$\relax }{figure.caption.18}{}}
\newlabel{sub@fig:linear-n_iter-N-50-D-1}{{c}{11}{$D=1$, $N=50$\relax }{figure.caption.18}{}}
\newlabel{fig:linear-n_iter-N-10-D-10}{{6d}{11}{$D=10$, $N=10$\relax }{figure.caption.18}{}}
\newlabel{sub@fig:linear-n_iter-N-10-D-10}{{d}{11}{$D=10$, $N=10$\relax }{figure.caption.18}{}}
\newlabel{fig:linear-n_iter-N-10-D-50}{{6e}{11}{$D=50$, $N=10$\relax }{figure.caption.18}{}}
\newlabel{sub@fig:linear-n_iter-N-10-D-50}{{e}{11}{$D=50$, $N=10$\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The expected performance for changing the number of gradient steps $n_{{iter}}$ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref  {fig:linear-n_iter-N-1-D-1}, \ref  {fig:linear-n_iter-N-10-D-1}, \ref  {fig:linear-n_iter-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref  {fig:linear-n_iter-N-10-D-1}, \ref  {fig:linear-n_iter-N-10-D-10}, \ref  {fig:linear-n_iter-N-10-D-50}.\relax }}{11}{figure.caption.18}\protected@file@percent }
\newlabel{fig:linear-n_iter}{{6}{11}{The expected performance for changing the number of gradient steps $\iter $ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref {fig:linear-n_iter-N-1-D-1}, \ref {fig:linear-n_iter-N-10-D-1}, \ref {fig:linear-n_iter-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref {fig:linear-n_iter-N-10-D-1}, \ref {fig:linear-n_iter-N-10-D-10}, \ref {fig:linear-n_iter-N-10-D-50}.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Task Mean $m$:}{11}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Nonlinear Problem}{11}{section*.21}\protected@file@percent }
\newlabel{fig:linear-m-N-1-D-1}{{7a}{12}{$D=1$, $N=1$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:linear-m-N-1-D-1}{{a}{12}{$D=1$, $N=1$\relax }{figure.caption.20}{}}
\newlabel{fig:linear-m-N-10-D-1}{{7b}{12}{$D=1$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:linear-m-N-10-D-1}{{b}{12}{$D=1$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{fig:linear-m-N-50-D-1}{{7c}{12}{$D=1$, $N=50$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:linear-m-N-50-D-1}{{c}{12}{$D=1$, $N=50$\relax }{figure.caption.20}{}}
\newlabel{fig:linear-m-N-10-D-10}{{7d}{12}{$D=10$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:linear-m-N-10-D-10}{{d}{12}{$D=10$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{fig:linear-m-N-10-D-50}{{7e}{12}{$D=50$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:linear-m-N-10-D-50}{{e}{12}{$D=50$, $N=10$\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The expected performance for changing task mean $m$ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref  {fig:linear-m-N-1-D-1}, \ref  {fig:linear-m-N-10-D-1}, \ref  {fig:linear-m-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref  {fig:linear-m-N-10-D-1}, \ref  {fig:linear-m-N-10-D-10}, \ref  {fig:linear-m-N-10-D-50}.\relax }}{12}{figure.caption.20}\protected@file@percent }
\newlabel{fig:linear-m}{{7}{12}{The expected performance for changing task mean $m$ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref {fig:linear-m-N-1-D-1}, \ref {fig:linear-m-N-10-D-1}, \ref {fig:linear-m-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref {fig:linear-m-N-10-D-1}, \ref {fig:linear-m-N-10-D-10}, \ref {fig:linear-m-N-10-D-50}.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Training Samples $N$:}{12}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Number of Gradient Steps $n_{{iter}}$:}{12}{section*.24}\protected@file@percent }
\newlabel{fig:nonlinear-N-D-1}{{8a}{13}{$D=1$\relax }{figure.caption.23}{}}
\newlabel{sub@fig:nonlinear-N-D-1}{{a}{13}{$D=1$\relax }{figure.caption.23}{}}
\newlabel{fig:nonlinear-N-D-10}{{8b}{13}{$D=10$\relax }{figure.caption.23}{}}
\newlabel{sub@fig:nonlinear-N-D-10}{{b}{13}{$D=10$\relax }{figure.caption.23}{}}
\newlabel{fig:nonlinear-N-D-50}{{8c}{13}{$D=50$\relax }{figure.caption.23}{}}
\newlabel{sub@fig:nonlinear-N-D-50}{{c}{13}{$D=50$\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The expected performance for changing the number of training samples for various dimensional problems.\relax }}{13}{figure.caption.23}\protected@file@percent }
\newlabel{ref:nonlinear-N}{{8}{13}{The expected performance for changing the number of training samples for various dimensional problems.\relax }{figure.caption.23}{}}
\newlabel{fig:nonlinear-n_iter-N-1-D-1}{{9a}{13}{$D=1$, $N=1$\relax }{figure.caption.25}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-1-D-1}{{a}{13}{$D=1$, $N=1$\relax }{figure.caption.25}{}}
\newlabel{fig:nonlinear-n_iter-N-10-D-1}{{9b}{13}{$D=1$, $N=10$\relax }{figure.caption.25}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-10-D-1}{{b}{13}{$D=1$, $N=10$\relax }{figure.caption.25}{}}
\newlabel{fig:nonlinear-n_iter-N-50-D-1}{{9c}{13}{$D=1$, $N=50$\relax }{figure.caption.25}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-50-D-1}{{c}{13}{$D=1$, $N=50$\relax }{figure.caption.25}{}}
\newlabel{fig:nonlinear-n_iter-N-10-D-10}{{9d}{13}{$D=10$, $N=10$\relax }{figure.caption.25}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-10-D-10}{{d}{13}{$D=10$, $N=10$\relax }{figure.caption.25}{}}
\newlabel{fig:nonlinear-n_iter-N-10-D-50}{{9e}{13}{$D=50$, $N=10$\relax }{figure.caption.25}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-10-D-50}{{e}{13}{$D=50$, $N=10$\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The expected performance for changing number of gradient steps $n_{{iter}}$ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref  {fig:nonlinear-n_iter-N-1-D-1}, \ref  {fig:nonlinear-n_iter-N-10-D-1}, \ref  {fig:nonlinear-n_iter-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref  {fig:nonlinear-n_iter-N-10-D-1}, \ref  {fig:nonlinear-n_iter-N-10-D-10}, \ref  {fig:nonlinear-n_iter-N-10-D-50}.\relax }}{13}{figure.caption.25}\protected@file@percent }
\newlabel{fig:nonlinear-n_iter}{{9}{13}{The expected performance for changing number of gradient steps $\iter $ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref {fig:nonlinear-n_iter-N-1-D-1}, \ref {fig:nonlinear-n_iter-N-10-D-1}, \ref {fig:nonlinear-n_iter-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref {fig:nonlinear-n_iter-N-10-D-1}, \ref {fig:nonlinear-n_iter-N-10-D-10}, \ref {fig:nonlinear-n_iter-N-10-D-50}.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Phase Task Variance $c_2$:}{14}{section*.26}\protected@file@percent }
\newlabel{fig:nonlinear-c2-N-1-D-1}{{10a}{14}{$D=1$, $N=1$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:nonlinear-c2-N-1-D-1}{{a}{14}{$D=1$, $N=1$\relax }{figure.caption.27}{}}
\newlabel{fig:nonlinear-c2-N-10-D-1}{{10b}{14}{$D=1$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:nonlinear-c2-N-10-D-1}{{b}{14}{$D=1$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{fig:nonlinear-c2-N-50-D-1}{{10c}{14}{$D=1$, $N=50$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:nonlinear-c2-N-50-D-1}{{c}{14}{$D=1$, $N=50$\relax }{figure.caption.27}{}}
\newlabel{fig:nonlinear-c2-N-10-D-10}{{10d}{14}{$D=10$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:nonlinear-c2-N-10-D-10}{{d}{14}{$D=10$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{fig:nonlinear-c2-N-10-D-50}{{10e}{14}{$D=50$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:nonlinear-c2-N-10-D-50}{{e}{14}{$D=50$, $N=10$\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The expected performance for changing the number of training samples for various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref  {fig:nonlinear-c2-N-1-D-1}, \ref  {fig:nonlinear-c2-N-10-D-1}, \ref  {fig:nonlinear-c2-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref  {fig:nonlinear-c2-N-10-D-1}, \ref  {fig:nonlinear-c2-N-10-D-10}, \ref  {fig:nonlinear-c2-N-10-D-50}.\relax }}{14}{figure.caption.27}\protected@file@percent }
\newlabel{fig:nonlinear-c2}{{10}{14}{The expected performance for changing the number of training samples for various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref {fig:nonlinear-c2-N-1-D-1}, \ref {fig:nonlinear-c2-N-10-D-1}, \ref {fig:nonlinear-c2-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref {fig:nonlinear-c2-N-10-D-1}, \ref {fig:nonlinear-c2-N-10-D-10}, \ref {fig:nonlinear-c2-N-10-D-50}.\relax }{figure.caption.27}{}}
\citation{fallah2021}
\citation{nakkiran2020}
\citation{behl2019,li2017b}
\citation{raghu2020}
\citation{denevi2018}
\citation{guiroy2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Discussion}{15}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{15}{section.1.6}\protected@file@percent }
\newlabel{sec:conc}{{6}{15}{Conclusion}{section.1.6}{}}
\bibstyle{splncs04}
\bibdata{/home/taylanot/Dropbox/archive_bib/mylibrary.bib}
\bibcite{antoniou2019}{1}
\bibcite{behl2019}{2}
\bibcite{collins2020}{3}
\bibcite{denevi2018}{4}
\bibcite{fallah2021}{5}
\bibcite{finn2017}{6}
\bibcite{finn2019}{7}
\bibcite{flennerhag2019}{8}
\bibcite{grant2018}{9}
\bibcite{guiroy2019}{10}
\bibcite{lake2019}{11}
\bibcite{li2017}{12}
\bibcite{li2017b}{13}
\bibcite{nakkiran2020}{14}
\bibcite{nichol2018}{15}
\bibcite{raghu2020}{16}
\bibcite{rajasegaran2020}{17}
\bibcite{thrun1998}{18}
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix}{16}{section.1.7}\protected@file@percent }
\newlabel{fig:linear-std_y-N-1-D-1}{{11a}{17}{$D=1$, $N=1$\relax }{figure.caption.29}{}}
\newlabel{sub@fig:linear-std_y-N-1-D-1}{{a}{17}{$D=1$, $N=1$\relax }{figure.caption.29}{}}
\newlabel{fig:linear-std_y-N-10-D-1}{{11b}{17}{$D=1$, $N=10$\relax }{figure.caption.29}{}}
\newlabel{sub@fig:linear-std_y-N-10-D-1}{{b}{17}{$D=1$, $N=10$\relax }{figure.caption.29}{}}
\newlabel{fig:linear-std_y-N-50-D-1}{{11c}{17}{$D=1$, $N=50$\relax }{figure.caption.29}{}}
\newlabel{sub@fig:linear-std_y-N-50-D-1}{{c}{17}{$D=1$, $N=50$\relax }{figure.caption.29}{}}
\newlabel{fig:linear-std_y-N-10-D-10}{{11d}{17}{$D=10$, $N=10$\relax }{figure.caption.29}{}}
\newlabel{sub@fig:linear-std_y-N-10-D-10}{{d}{17}{$D=10$, $N=10$\relax }{figure.caption.29}{}}
\newlabel{fig:linear-std_y-N-10-D-50}{{11e}{17}{$D=50$, $N=10$\relax }{figure.caption.29}{}}
\newlabel{sub@fig:linear-std_y-N-10-D-50}{{e}{17}{$D=50$, $N=10$\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces [\textbf  {Linear Problem}] The Expected Error for changing noise standard deviation $\sigma $ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref  {fig:linear-std_y-N-1-D-1}, \ref  {fig:linear-std_y-N-10-D-1}, \ref  {fig:linear-std_y-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref  {fig:linear-std_y-N-10-D-1}, \ref  {fig:linear-std_y-N-10-D-10}, \ref  {fig:linear-std_y-N-10-D-50}.\relax }}{17}{figure.caption.29}\protected@file@percent }
\newlabel{fig:linear-std_y}{{11}{17}{[\textbf {Linear Problem}] The Expected Error for changing noise standard deviation $\sigma $ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref {fig:linear-std_y-N-1-D-1}, \ref {fig:linear-std_y-N-10-D-1}, \ref {fig:linear-std_y-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref {fig:linear-std_y-N-10-D-1}, \ref {fig:linear-std_y-N-10-D-10}, \ref {fig:linear-std_y-N-10-D-50}.\relax }{figure.caption.29}{}}
\newlabel{fig:linear-b-N-1-D-1}{{12a}{17}{$D=1$, $N=1$\relax }{figure.caption.30}{}}
\newlabel{sub@fig:linear-b-N-1-D-1}{{a}{17}{$D=1$, $N=1$\relax }{figure.caption.30}{}}
\newlabel{fig:linear-b-N-10-D-1}{{12b}{17}{$D=1$, $N=10$\relax }{figure.caption.30}{}}
\newlabel{sub@fig:linear-b-N-10-D-1}{{b}{17}{$D=1$, $N=10$\relax }{figure.caption.30}{}}
\newlabel{fig:linear-b-N-50-D-1}{{12c}{17}{$D=1$, $N=50$\relax }{figure.caption.30}{}}
\newlabel{sub@fig:linear-b-N-50-D-1}{{c}{17}{$D=1$, $N=50$\relax }{figure.caption.30}{}}
\newlabel{fig:linear-b-N-10-D-10}{{12d}{17}{$D=10$, $N=10$\relax }{figure.caption.30}{}}
\newlabel{sub@fig:linear-b-N-10-D-10}{{d}{17}{$D=10$, $N=10$\relax }{figure.caption.30}{}}
\newlabel{fig:linear-b-N-10-D-50}{{12e}{17}{$D=50$, $N=10$\relax }{figure.caption.30}{}}
\newlabel{sub@fig:linear-b-N-10-D-50}{{e}{17}{$D=50$, $N=10$\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces [\textbf  {Linear Problem}] The Expected Error for changing input variance $k$ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref  {fig:linear-b-N-1-D-1}, \ref  {fig:linear-b-N-10-D-1}, \ref  {fig:linear-b-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref  {fig:linear-b-N-10-D-1}, \ref  {fig:linear-b-N-10-D-10}, \ref  {fig:linear-b-N-10-D-50}.\relax }}{17}{figure.caption.30}\protected@file@percent }
\newlabel{fig:linear-b}{{12}{17}{[\textbf {Linear Problem}] The Expected Error for changing input variance $k$ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref {fig:linear-b-N-1-D-1}, \ref {fig:linear-b-N-10-D-1}, \ref {fig:linear-b-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref {fig:linear-b-N-10-D-1}, \ref {fig:linear-b-N-10-D-10}, \ref {fig:linear-b-N-10-D-50}.\relax }{figure.caption.30}{}}
\newlabel{fig:nonlinear-std_y-N-1-D-1}{{13a}{18}{$D=1$, $N=1$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:nonlinear-std_y-N-1-D-1}{{a}{18}{$D=1$, $N=1$\relax }{figure.caption.31}{}}
\newlabel{fig:nonlinear-std_y-N-10-D-1}{{13b}{18}{$D=1$, $N=10$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:nonlinear-std_y-N-10-D-1}{{b}{18}{$D=1$, $N=10$\relax }{figure.caption.31}{}}
\newlabel{fig:nonlinear-std_y-N-50-D-1}{{13c}{18}{$D=1$, $N=50$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:nonlinear-std_y-N-50-D-1}{{c}{18}{$D=1$, $N=50$\relax }{figure.caption.31}{}}
\newlabel{fig:nonlinear-std_y-N-10-D-10}{{13d}{18}{$D=10$, $N=10$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:nonlinear-std_y-N-10-D-10}{{d}{18}{$D=10$, $N=10$\relax }{figure.caption.31}{}}
\newlabel{fig:nonlinear-std_y-N-10-D-50}{{13e}{18}{$D=50$, $N=10$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:nonlinear-std_y-N-10-D-50}{{e}{18}{$D=50$, $N=10$\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces [\textbf  {Nonlinear Problem}] The Expected Error for changing noise standard deviation $\sigma $ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref  {fig:nonlinear-std_y-N-1-D-1}, \ref  {fig:nonlinear-std_y-N-10-D-1}, \ref  {fig:nonlinear-std_y-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref  {fig:nonlinear-std_y-N-10-D-1}, \ref  {fig:nonlinear-std_y-N-10-D-10}, \ref  {fig:nonlinear-std_y-N-10-D-50}.\relax }}{18}{figure.caption.31}\protected@file@percent }
\newlabel{fig:nonlinear-std_y}{{13}{18}{[\textbf {Nonlinear Problem}] The Expected Error for changing noise standard deviation $\sigma $ with various training samples and various dimensional problems. For the given parameter the effect of increasing number of training samples can be seen by looking at Figures \ref {fig:nonlinear-std_y-N-1-D-1}, \ref {fig:nonlinear-std_y-N-10-D-1}, \ref {fig:nonlinear-std_y-N-50-D-1} and the effect of increasing dimensionality can be seen by looking at Figures \ref {fig:nonlinear-std_y-N-10-D-1}, \ref {fig:nonlinear-std_y-N-10-D-10}, \ref {fig:nonlinear-std_y-N-10-D-50}.\relax }{figure.caption.31}{}}
