\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{thrun1998}
\citation{thrun1998}
\citation{finn2017}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {title}{When MAML Learns Quickly, Does It Generalize Well?}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\authcount {4}}
\@writefile{toc}{\contentsline {author}{O. Taylan Turan\and David M.J. Tax\and Miguel A. Bessa\and Marco Loog}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1.1}{}}
\citation{flennerhag2019,nichol2018,rajasegaran2020,collins2020,guiroy2019}
\citation{finn2017}
\citation{finn2017}
\citation{flennerhag2019,collins2020}
\citation{antoniou2019}
\citation{nichol2018}
\citation{grant2018}
\citation{finn2019,rajasegaran2020}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model-Agnostic Meta-Learning (MAML)}{2}{section.1.2}\protected@file@percent }
\newlabel{sec:maml}{{2}{2}{Model-Agnostic Meta-Learning (MAML)}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{2}{section.1.3}\protected@file@percent }
\newlabel{sec:rw}{{3}{2}{Related Work}{section.1.3}{}}
\citation{li2017}
\citation{behl2019}
\citation{lake2019}
\citation{finn2017}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces MAML\cite  {finn2017} Algorithm\relax }}{3}{algocf.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:MAML}{{1}{3}{Model-Agnostic Meta-Learning (MAML)}{algocf.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setting}{3}{section.1.4}\protected@file@percent }
\newlabel{sec:methods}{{4}{3}{Experimental Setting}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Learning Problems}{4}{subsection.1.4.1}\protected@file@percent }
\newlabel{eq:linearreg}{{1}{4}{Learning Problems}{equation.1.4.1}{}}
\newlabel{eq:nonlinearreg}{{2}{4}{Learning Problems}{equation.1.4.2}{}}
\newlabel{eq:ee}{{3}{4}{Learning Problems}{equation.1.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Models}{4}{subsection.1.4.2}\protected@file@percent }
\citation{finn2017}
\newlabel{fig:lintasks}{{1a}{5}{$\lab = \trans {\inp }\scaletask $\relax }{figure.caption.3}{}}
\newlabel{sub@fig:lintasks}{{a}{5}{$\lab = \trans {\inp }\scaletask $\relax }{figure.caption.3}{}}
\newlabel{fig:nonlintasks}{{1b}{5}{$\lab = \trans {\sine (\inp +\phasetask )}\scaletask $\relax }{figure.caption.3}{}}
\newlabel{sub@fig:nonlintasks}{{b}{5}{$\lab = \trans {\sine (\inp +\phasetask )}\scaletask $\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 100 sample tasks drawn from $p_\mathcal  {T}$ for both linear ($m=0$ and $c=1$) and nonlinear ($c_1=1$ and $c_2=1$) problems with low opacity and the intermediate models for MAML trained with respective $p_\mathcal  {T}$.\relax }}{5}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Estimator}{5}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ridge Estimator}{5}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Kernel Ridge Estimator}{5}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient Descent}{5}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results and Discussion}{6}{section.1.5}\protected@file@percent }
\newlabel{sec:resdis}{{5}{6}{Results and Discussion}{section.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Linear Problem}{6}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Number of Gradient Steps $n_{{iter}}$:}{6}{section*.9}\protected@file@percent }
\newlabel{fig:linear-n_iter-N-1-D-1}{{2a}{7}{$D=1$, $N=1$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:linear-n_iter-N-1-D-1}{{a}{7}{$D=1$, $N=1$\relax }{figure.caption.10}{}}
\newlabel{fig:linear-n_iter-N-10-D-1}{{2b}{7}{$D=1$, $N=10$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:linear-n_iter-N-10-D-1}{{b}{7}{$D=1$, $N=10$\relax }{figure.caption.10}{}}
\newlabel{fig:linear-n_iter-N-50-D-1}{{2c}{7}{$D=1$, $N=50$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:linear-n_iter-N-50-D-1}{{c}{7}{$D=1$, $N=50$\relax }{figure.caption.10}{}}
\newlabel{fig:linear-n_iter-N-10-D-10}{{2d}{7}{$D=10$, $N=10$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:linear-n_iter-N-10-D-10}{{d}{7}{$D=10$, $N=10$\relax }{figure.caption.10}{}}
\newlabel{fig:linear-n_iter-N-10-D-50}{{2e}{7}{$D=50$, $N=10$\relax }{figure.caption.10}{}}
\newlabel{sub@fig:linear-n_iter-N-10-D-50}{{e}{7}{$D=50$, $N=10$\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The expected error for the increasing number of gradient steps $n_{{iter}}$ used for adaptation when changing the number of training samples for various problems of different dimensions.\relax }}{7}{figure.caption.10}\protected@file@percent }
\newlabel{fig:linear-n_iter}{{2}{7}{The expected error for the increasing number of gradient steps $\iter $ used for adaptation when changing the number of training samples for various problems of different dimensions.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of the Number of Training Samples $N$:}{7}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Task Variance $c$:}{7}{section*.13}\protected@file@percent }
\newlabel{fig:linear-N-D-1}{{3a}{8}{$D=1$\relax }{figure.caption.12}{}}
\newlabel{sub@fig:linear-N-D-1}{{a}{8}{$D=1$\relax }{figure.caption.12}{}}
\newlabel{fig:linear-N-D-10}{{3b}{8}{$D=10$\relax }{figure.caption.12}{}}
\newlabel{sub@fig:linear-N-D-10}{{b}{8}{$D=10$\relax }{figure.caption.12}{}}
\newlabel{fig:linear-N-D-50}{{3c}{8}{$D=50$\relax }{figure.caption.12}{}}
\newlabel{sub@fig:linear-N-D-50}{{c}{8}{$D=50$\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The expected error for the increasing number of training samples and problem dimensionality.\relax }}{8}{figure.caption.12}\protected@file@percent }
\newlabel{fig:linear-N}{{3}{8}{The expected error for the increasing number of training samples and problem dimensionality.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Mean expected error for the range $c:[0,1]$ range with various gradient steps for the MAML and GD models with $\eta =0.3234$. Note that only $D=1$, $N=10$ case (see Figure \ref  {fig:linear-c-N-10-D-1}) is presented.\relax }}{8}{table.caption.14}\protected@file@percent }
\newlabel{tab:zoom}{{1}{8}{Mean expected error for the range $c:[0,1]$ range with various gradient steps for the MAML and GD models with $\lr =0.3234$. Note that only $D=1$, $N=10$ case (see Figure \ref {fig:linear-c-N-10-D-1}) is presented.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Task Mean $m$:}{8}{section*.16}\protected@file@percent }
\newlabel{fig:linear-c-N-1-D-1}{{4a}{9}{$D=1$, $N=1$\relax }{figure.caption.15}{}}
\newlabel{sub@fig:linear-c-N-1-D-1}{{a}{9}{$D=1$, $N=1$\relax }{figure.caption.15}{}}
\newlabel{fig:linear-c-N-10-D-1}{{4b}{9}{$D=1$, $N=10$\relax }{figure.caption.15}{}}
\newlabel{sub@fig:linear-c-N-10-D-1}{{b}{9}{$D=1$, $N=10$\relax }{figure.caption.15}{}}
\newlabel{fig:linear-c-N-50-D-1}{{4c}{9}{$D=1$, $N=50$\relax }{figure.caption.15}{}}
\newlabel{sub@fig:linear-c-N-50-D-1}{{c}{9}{$D=1$, $N=50$\relax }{figure.caption.15}{}}
\newlabel{fig:linear-c-N-10-D-10}{{4d}{9}{$D=10$, $N=10$\relax }{figure.caption.15}{}}
\newlabel{sub@fig:linear-c-N-10-D-10}{{d}{9}{$D=10$, $N=10$\relax }{figure.caption.15}{}}
\newlabel{fig:linear-c-N-10-D-50}{{4e}{9}{$D=50$, $N=10$\relax }{figure.caption.15}{}}
\newlabel{sub@fig:linear-c-N-10-D-50}{{e}{9}{$D=50$, $N=10$\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The expected error for increasing task variance $c$ when changing the number of training samples for various problems of different dimensions.\relax }}{9}{figure.caption.15}\protected@file@percent }
\newlabel{fig:linear-c}{{4}{9}{The expected error for increasing task variance $c$ when changing the number of training samples for various problems of different dimensions.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Nonlinear Problem}{9}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Number of Gradient Steps $n_{{iter}}$:}{9}{section*.19}\protected@file@percent }
\newlabel{fig:linear-m-N-1-D-1}{{5a}{10}{$D=1$, $N=1$\relax }{figure.caption.17}{}}
\newlabel{sub@fig:linear-m-N-1-D-1}{{a}{10}{$D=1$, $N=1$\relax }{figure.caption.17}{}}
\newlabel{fig:linear-m-N-10-D-1}{{5b}{10}{$D=1$, $N=10$\relax }{figure.caption.17}{}}
\newlabel{sub@fig:linear-m-N-10-D-1}{{b}{10}{$D=1$, $N=10$\relax }{figure.caption.17}{}}
\newlabel{fig:linear-m-N-50-D-1}{{5c}{10}{$D=1$, $N=50$\relax }{figure.caption.17}{}}
\newlabel{sub@fig:linear-m-N-50-D-1}{{c}{10}{$D=1$, $N=50$\relax }{figure.caption.17}{}}
\newlabel{fig:linear-m-N-10-D-10}{{5d}{10}{$D=10$, $N=10$\relax }{figure.caption.17}{}}
\newlabel{sub@fig:linear-m-N-10-D-10}{{d}{10}{$D=10$, $N=10$\relax }{figure.caption.17}{}}
\newlabel{fig:linear-m-N-10-D-50}{{5e}{10}{$D=50$, $N=10$\relax }{figure.caption.17}{}}
\newlabel{sub@fig:linear-m-N-10-D-50}{{e}{10}{$D=50$, $N=10$\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The expected error for increasing task mean $m$ when changing the number of training samples for various problems of different dimensions.\relax }}{10}{figure.caption.17}\protected@file@percent }
\newlabel{fig:linear-m}{{5}{10}{The expected error for increasing task mean $m$ when changing the number of training samples for various problems of different dimensions.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Number of Training Samples $N$:}{10}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Phase Task Variance $c_2$:}{10}{section*.23}\protected@file@percent }
\citation{fallah2021}
\citation{nakkiran2020}
\newlabel{fig:nonlinear-n_iter-N-1-D-1}{{6a}{11}{$D=1$, $N=1$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-1-D-1}{{a}{11}{$D=1$, $N=1$\relax }{figure.caption.20}{}}
\newlabel{fig:nonlinear-n_iter-N-10-D-1}{{6b}{11}{$D=1$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-10-D-1}{{b}{11}{$D=1$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{fig:nonlinear-n_iter-N-50-D-1}{{6c}{11}{$D=1$, $N=50$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-50-D-1}{{c}{11}{$D=1$, $N=50$\relax }{figure.caption.20}{}}
\newlabel{fig:nonlinear-n_iter-N-10-D-10}{{6d}{11}{$D=10$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-10-D-10}{{d}{11}{$D=10$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{fig:nonlinear-n_iter-N-10-D-50}{{6e}{11}{$D=50$, $N=10$\relax }{figure.caption.20}{}}
\newlabel{sub@fig:nonlinear-n_iter-N-10-D-50}{{e}{11}{$D=50$, $N=10$\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The expected error for the increasing number of gradient steps $n_{{iter}}$ used for adaptation when changing the number of training samples for various problems of different dimensions.\relax }}{11}{figure.caption.20}\protected@file@percent }
\newlabel{fig:nonlinear-n_iter}{{6}{11}{The expected error for the increasing number of gradient steps $\iter $ used for adaptation when changing the number of training samples for various problems of different dimensions.\relax }{figure.caption.20}{}}
\newlabel{fig:nonlinear-N-D-1}{{7a}{11}{$D=1$\relax }{figure.caption.22}{}}
\newlabel{sub@fig:nonlinear-N-D-1}{{a}{11}{$D=1$\relax }{figure.caption.22}{}}
\newlabel{fig:nonlinear-N-D-10}{{7b}{11}{$D=10$\relax }{figure.caption.22}{}}
\newlabel{sub@fig:nonlinear-N-D-10}{{b}{11}{$D=10$\relax }{figure.caption.22}{}}
\newlabel{fig:nonlinear-N-D-50}{{7c}{11}{$D=50$\relax }{figure.caption.22}{}}
\newlabel{sub@fig:nonlinear-N-D-50}{{c}{11}{$D=50$\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The expected error for the increasing number of training samples and problem dimensionality.\relax }}{11}{figure.caption.22}\protected@file@percent }
\newlabel{fig:nonlinear-N}{{7}{11}{The expected error for the increasing number of training samples and problem dimensionality.\relax }{figure.caption.22}{}}
\citation{behl2019,li2017b}
\citation{raghu2020}
\citation{denevi2018}
\citation{guiroy2019}
\newlabel{fig:nonlinear-c2-N-1-D-1}{{8a}{12}{$D=1$, $N=1$\relax }{figure.caption.24}{}}
\newlabel{sub@fig:nonlinear-c2-N-1-D-1}{{a}{12}{$D=1$, $N=1$\relax }{figure.caption.24}{}}
\newlabel{fig:nonlinear-c2-N-10-D-1}{{8b}{12}{$D=1$, $N=10$\relax }{figure.caption.24}{}}
\newlabel{sub@fig:nonlinear-c2-N-10-D-1}{{b}{12}{$D=1$, $N=10$\relax }{figure.caption.24}{}}
\newlabel{fig:nonlinear-c2-N-50-D-1}{{8c}{12}{$D=1$, $N=50$\relax }{figure.caption.24}{}}
\newlabel{sub@fig:nonlinear-c2-N-50-D-1}{{c}{12}{$D=1$, $N=50$\relax }{figure.caption.24}{}}
\newlabel{fig:nonlinear-c2-N-10-D-10}{{8d}{12}{$D=10$, $N=10$\relax }{figure.caption.24}{}}
\newlabel{sub@fig:nonlinear-c2-N-10-D-10}{{d}{12}{$D=10$, $N=10$\relax }{figure.caption.24}{}}
\newlabel{fig:nonlinear-c2-N-10-D-50}{{8e}{12}{$D=50$, $N=10$\relax }{figure.caption.24}{}}
\newlabel{sub@fig:nonlinear-c2-N-10-D-50}{{e}{12}{$D=50$, $N=10$\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The expected error for increasing task variance for phase $c_2$ used for adaptation when changing the number of training samples for various problems of different dimensions.\relax }}{12}{figure.caption.24}\protected@file@percent }
\newlabel{fig:nonlinear-c2}{{8}{12}{The expected error for increasing task variance for phase $c_2$ used for adaptation when changing the number of training samples for various problems of different dimensions.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Discussion}{12}{subsection.1.5.1}\protected@file@percent }
\bibstyle{splncs04}
\bibdata{/home/taylanot/Dropbox/archive_bib/mylibrary.bib}
\bibcite{antoniou2019}{1}
\bibcite{behl2019}{2}
\bibcite{collins2020}{3}
\bibcite{denevi2018}{4}
\bibcite{fallah2021}{5}
\bibcite{finn2017}{6}
\bibcite{finn2019}{7}
\bibcite{flennerhag2019}{8}
\bibcite{grant2018}{9}
\bibcite{guiroy2019}{10}
\bibcite{lake2019}{11}
\bibcite{li2017}{12}
\bibcite{li2017b}{13}
\bibcite{nakkiran2020}{14}
\bibcite{nichol2018}{15}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{13}{section.1.6}\protected@file@percent }
\newlabel{sec:conc}{{6}{13}{Conclusion}{section.1.6}{}}
\bibcite{raghu2020}{16}
\bibcite{rajasegaran2020}{17}
\bibcite{thrun1998}{18}
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix}{14}{section.1.7}\protected@file@percent }
\newlabel{fig:linear-std_y-N-1-D-1}{{9a}{14}{$D=1$, $N=1$\relax }{figure.caption.26}{}}
\newlabel{sub@fig:linear-std_y-N-1-D-1}{{a}{14}{$D=1$, $N=1$\relax }{figure.caption.26}{}}
\newlabel{fig:linear-std_y-N-10-D-1}{{9b}{14}{$D=1$, $N=10$\relax }{figure.caption.26}{}}
\newlabel{sub@fig:linear-std_y-N-10-D-1}{{b}{14}{$D=1$, $N=10$\relax }{figure.caption.26}{}}
\newlabel{fig:linear-std_y-N-50-D-1}{{9c}{14}{$D=1$, $N=50$\relax }{figure.caption.26}{}}
\newlabel{sub@fig:linear-std_y-N-50-D-1}{{c}{14}{$D=1$, $N=50$\relax }{figure.caption.26}{}}
\newlabel{fig:linear-std_y-N-10-D-10}{{9d}{14}{$D=10$, $N=10$\relax }{figure.caption.26}{}}
\newlabel{sub@fig:linear-std_y-N-10-D-10}{{d}{14}{$D=10$, $N=10$\relax }{figure.caption.26}{}}
\newlabel{fig:linear-std_y-N-10-D-50}{{9e}{14}{$D=50$, $N=10$\relax }{figure.caption.26}{}}
\newlabel{sub@fig:linear-std_y-N-10-D-50}{{e}{14}{$D=50$, $N=10$\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces [\textbf  {Linear Problem}]: The expected error for increasing noise standard deviation $\sigma $ when changing the number of training samples for various problems of different dimensions.\relax }}{14}{figure.caption.26}\protected@file@percent }
\newlabel{fig:linear-std_y}{{9}{14}{[\textbf {Linear Problem}]: The expected error for increasing noise standard deviation $\sigma $ when changing the number of training samples for various problems of different dimensions.\relax }{figure.caption.26}{}}
\newlabel{fig:linear-b-N-1-D-1}{{10a}{15}{$D=1$, $N=1$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:linear-b-N-1-D-1}{{a}{15}{$D=1$, $N=1$\relax }{figure.caption.27}{}}
\newlabel{fig:linear-b-N-10-D-1}{{10b}{15}{$D=1$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:linear-b-N-10-D-1}{{b}{15}{$D=1$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{fig:linear-b-N-50-D-1}{{10c}{15}{$D=1$, $N=50$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:linear-b-N-50-D-1}{{c}{15}{$D=1$, $N=50$\relax }{figure.caption.27}{}}
\newlabel{fig:linear-b-N-10-D-10}{{10d}{15}{$D=10$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:linear-b-N-10-D-10}{{d}{15}{$D=10$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{fig:linear-b-N-10-D-50}{{10e}{15}{$D=50$, $N=10$\relax }{figure.caption.27}{}}
\newlabel{sub@fig:linear-b-N-10-D-50}{{e}{15}{$D=50$, $N=10$\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces [\textbf  {Linear Problem}]: The expected error for increasing input variance $k$ when changing the number of training samples for various problems of different dimensions.\relax }}{15}{figure.caption.27}\protected@file@percent }
\newlabel{fig:linear-b}{{10}{15}{[\textbf {Linear Problem}]: The expected error for increasing input variance $k$ when changing the number of training samples for various problems of different dimensions.\relax }{figure.caption.27}{}}
\newlabel{fig:nonlinear-std_y-N-1-D-1}{{11a}{15}{$D=1$, $N=1$\relax }{figure.caption.28}{}}
\newlabel{sub@fig:nonlinear-std_y-N-1-D-1}{{a}{15}{$D=1$, $N=1$\relax }{figure.caption.28}{}}
\newlabel{fig:nonlinear-std_y-N-10-D-1}{{11b}{15}{$D=1$, $N=10$\relax }{figure.caption.28}{}}
\newlabel{sub@fig:nonlinear-std_y-N-10-D-1}{{b}{15}{$D=1$, $N=10$\relax }{figure.caption.28}{}}
\newlabel{fig:nonlinear-std_y-N-50-D-1}{{11c}{15}{$D=1$, $N=50$\relax }{figure.caption.28}{}}
\newlabel{sub@fig:nonlinear-std_y-N-50-D-1}{{c}{15}{$D=1$, $N=50$\relax }{figure.caption.28}{}}
\newlabel{fig:nonlinear-std_y-N-10-D-10}{{11d}{15}{$D=10$, $N=10$\relax }{figure.caption.28}{}}
\newlabel{sub@fig:nonlinear-std_y-N-10-D-10}{{d}{15}{$D=10$, $N=10$\relax }{figure.caption.28}{}}
\newlabel{fig:nonlinear-std_y-N-10-D-50}{{11e}{15}{$D=50$, $N=10$\relax }{figure.caption.28}{}}
\newlabel{sub@fig:nonlinear-std_y-N-10-D-50}{{e}{15}{$D=50$, $N=10$\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces [\textbf  {Nonlinear Problem}]: The expected error for increasing noise standard deviation $\sigma $ when changing the number of training samples for various problems of different dimensions.\relax }}{15}{figure.caption.28}\protected@file@percent }
\newlabel{fig:nonlinear-std_y}{{11}{15}{[\textbf {Nonlinear Problem}]: The expected error for increasing noise standard deviation $\sigma $ when changing the number of training samples for various problems of different dimensions.\relax }{figure.caption.28}{}}
