% Negative sides of its
Although its minor superiority with regards to expected performance, single-task learners are able to compete with MAML. In all of our experiments single best learning rate and the regularization parameter are selected for the whole expected performance curves individually. We show that even in the case of general regularization, and when enough data is present a single task learner can outperform on expectation of a meta learner when the tasks observed start to deviate from each other. This indicates that the regularization-based meta-learners similar to the ones presented in \cite{denevi2018}, but also suitable for the nonlinear problem settings can be competitive and robust enough for much wider task variance. Moreover, regularization-based methods similar to the ones presented in \cite{guiroy2019} for MAML can prove useful to understand and study MAML.

The use of quick adaptation under a supervised learning setting where this constraint is not imposed by the problem is investigated. It is found that a regularization effect exists induced by quick adaptation which contributes to the generalization performance of MAML even in the convex problem setting. However, more in-depth expected performance should be done for the exact causes of Omniglot dataset and other widely used supervised few-shot learning benchmarks. We believe that understanding the effects of all the contributing assumptions is key to correct use cases of meta-learning methods.


