The use of quick adaptation constraint under a supervised learning setting where this constraint is not imposed by the problem is investigated. It is found that there exists a regularization effect induced by quick adaptation which contributes to the generalization performance of MAML even in the convex problem setting. However, more in-depth expected performance should be done for the exact causes of Omniglot dataset and other widely used supervised few-shot learning benchmarks. We believe that understanding the effects of all the contributing assumptions is key to correct use cases of meta-learning methods.

