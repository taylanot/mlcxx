% Meta Learning Definitions
Learning to learn, also referred to as meta-learning, treats the training of a machine learning model as a learning problem in itself. In this setting, there exist multiple learning problems and they are treated together. A machine learning model is "learning to learn" if the performance on each task improves with training experience obtained from the other tasks \cite{thrun1998}. A major use of meta-learning is to tackle few-shot learning problems, where there is little data available from the learning task that is of prime interest, whereas there is an abundance of data from other, yet similar tasks. 

% What Makes MAML Different
Early works on the learning-to-learn paradigm relied upon two different models working together, where one model tries to improve performance on the specific task and the other tries to improve performance over the observed tasks together \cite{thrun1998}. MAML (Model-Agnostic Meta-Learning) \cite{finn2017} provides an algorithm that circumvents the need for multiple models. This method tackles meta-learning by providing a model initialization (which is always required for models that are optimized with Stochastic Gradient Descent) that facilitates quick adaptation and good generalization.

% Where to use MAML?
Since it is model and problem independent, MAML finds a wide application area in the context of few-shot meta-learning. Specifically, for supervised and reinforcement learning problems under that paradigm. Moreover, MAML also aims to improve a specific task performance quickly (with a few gradient steps). This is an additional important aspect of meta-learning. 

% What is the problem?

The quick adaptation feature can prove useful in certain settings, for instance, in robotics applications, where the reaction/adaptation time of the agents in dynamic environments imposes time limitations. However, this limitation is not present in supervised learning problems, where MAML or its variants are utilized as a baseline (\eg \cite{flennerhag2019,nichol2018,rajasegaran2020,collins2020,guiroy2019}). Most supervised problems are benchmarked with an image classification problem, where $N$-way $K$-shot classification problem ($N$ different classes with $K$ labeled training data) is tackled, where memory or time limitations do not constitute a major issue.

% What is our paper about?
The main aim of this paper is to investigate MAML in settings where quick adaptation is not needed, and where most of the applications and variants of this method are benchmarked. This will be achieved by looking at the expected performance of MAML under two synthetic regression scenarios, and comparing its performance to conventional base learners (\eg Linear Regression, Ridge Regression, Kernel Ridge Regression, etc.). By doing this we aim to investigate the effect of the limited adaptation step, and whether or not there is a benefit to this limitation. The code for all experiments is available at \href{https://github.com/taylanot/EE_MAML.git}{github.com/taylanot/EE$\_$MAML.git}. 
