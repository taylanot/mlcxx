% MAML and its Extensions (deficiencies and improvements)
%MAML inquired about multiple developments that followed the same approach of finding a warm initialization for all the tasks coming from a certain task distribution. In the follow-up works, the deficiencies of MAML are tried to be tackled. \cite{flennerhag2019,collins2020} try to tackle the task similarity needed for MAML, in other words, make the meta-learner more task family robust. The sensitivity of MAML to architectural details is tried to be circumvented in \cite{antoniou2019}. And, the need for the second order term needed by MAML is questioned and shown that first order approximations can give as good results as MAML in \cite{nichol2018}. Deeper changes to the MAML method are presented in \cite{grant2018} Moreover, an extension for the continual learning where tasks are introduced sequentially is proposed in \cite{finn2019,rajasegaran2020}. 

MAML received multiple developments that followed the same approach of finding a warm initialization for all the tasks coming from a certain task distribution. Subsequent works highlight some of its limitations. Authors in \cite{flennerhag2019,collins2020} improved on MAML's need for task similarity making the meta-learning more task family robust. The sensitivity of MAML to architectural details is noted and circumvented in \cite{antoniou2019}. The need for a second order term in MAML is questioned and shown that first order approximations can give equally good results in \cite{nichol2018}. Deeper changes to the MAML method are presented in \cite{grant2018} to convert it to a method of probabilistic inference. Moreover, a continual learning extension where tasks are introduced sequentially is proposed in \cite{finn2019,rajasegaran2020}. 

% Emphasis on Gradient steps 
Besides most of the above-mentioned developments, some of the attention went to improving the quick adaptation stage from the learned initialization. In \cite{li2017} not just the initialization, but also the update direction and the learning rate are optimized. Moreover, in \cite{behl2019} the learning rate for the adaptation is learned with hypergradient descent. Both methods aim to limit the adaptation steps needed. 

% Few-shot Supervised Learning Benchmarks
%It can be seen from the derivative work of MAML that the extra aim introduced with MAML to meta-learning, which is a quick adaptation, is the common denominator in most of the work that can be found. There seems to be a case for quick adaptation in dynamic problems, in other problems, this seems not to be the case. All of the above-mentioned articles use the Omniglot dataset \cite{lake2019} as a supervised classification benchmark with the MAML as the baseline. Moreover, some of the work use the sinusoidal regression task as shown in the original MAML paper \cite{finn2017}. Both of these settings where MAML and its variants are tested do not have the time or the memory restrictions as a few-shot learning problem. However, quick adaptation methods are still being benchmarked with these methods. Although there is quite a lot of effort into improving MAML it is still not clear the effect of adaptation step limitation in a problem that does not necessarily suffer from the quick adaptation constraint.


Therefore, MAML's developments also focus on quick adaptation. However, the above-mentioned articles use the Omniglot dataset \cite{lake2019} as a supervised classification benchmark with MAML as the baseline and some of the works recreate the sinusoidal regression task as shown in the original MAML paper \cite{finn2017}. Both of these settings where MAML and its variants are tested do not have the time or memory restrictions as few-shot learning problems. Yet, quick adaptation methods are still being benchmarked with these datasets. %Although there is quite a lot of effort into improving MAML it is still not clear the effect of adaptation step limitation in a problem that does not necessarily suffer from the quick adaptation constraint.
