% MAML Aim -> Intermediate Model

MAML aims to obtain an intermediate model $\model({\param}_{\text{meta}})$ that can generalize well after adaptation with gradient descent to a dataset $\dataset$ observed from a new and unseen task $\task$ drawn from $\prob_\task$ where the number of training points $N$ and the number of iterations $\iter$ is limited.  

% How to obtain the intermediate 
In order to obtain $\param_{\text{meta}}$ first, a batch of tasks $\{\task_i\}_{i=1}^{M}$ from $\prob_{\task}$ is observed with each having a corresponding dataset $\{\dataset_i\}_{i=1}^{M}$. Then, the future gradients concerning each task are observed and gradient descent is utilized to get possible parameters $\param^\prime$ for each task and a gradient descent iteration is made by collecting all the possible parameters from an observed batch of tasks for the real parameter update. The general procedure for supervised learning problems is given in Algorithm \ref{alg:MAML}. Authors of \cite{finn2017} indicate that by using this procedure the model $\model({\param}_{\text{meta}})$ requires few gradient updates from a specific task, achieving good generalization performance. Examples of the intermediate model $\model({\param}_{\text{meta}})$ prediction with the observed tasks on the background can be seen in Figures \ref{fig:lintasks} and \ref{fig:nonlintasks}.

\begin{algorithm}
  \caption{MAML\cite{finn2017} Algorithm}\label{alg:MAML}
  \KwData{$\prob_{\task}$, $\alpha$, $\beta$}
  \KwResult{Intermediate Model $\model(\param_{meta})$}
  initialize $\param$ randomly; \\
  \While{not done}
  {
    sample a batch of tasks $\task_i$ from $\prob_{\task}$\\
    \ForAll{$\task_i$}
    {
      Obtain future gradients: $\grad{\param}\loss_{\task_i}(\model(\param))$ wrt. $\dataset_i$ \\
      Possible future parameters: $\param_i^\prime = \param_i -\alpha\grad{\param}\loss_{\task_i}(\model(\param))$
    }
    Update: $\param \leftarrow \param- \beta\grad{\param}\sum\loss_{\task_i}(\model(\param_i^\prime))$
  }
\end{algorithm}

