This section is dedicated to the expected performance results of a meta-learning model after adaptation and conventional base learners (\eg Linear, Ridge, and Kernel Ridge Regression models), to see their performance differences under certain scenarios induced by the experimental assumptions (\eg task variance, input variance, noise, and dimensionality). 

\subsubsection{Linear Problem}
The linear problem introduced in Section \ref{sec:methods} is characterized by the dimensionality $D$, number of training samples $N$, number of gradient steps $\iter$, the task variance $c$ and task mean $m$, and the variance of the input samples $k$. For the sake of brevity, only some of the parameters are discussed in this section. Unless the parameter in the configuration is under investigation, the default values are utilized. And, the default values for the experimentation $\sigma=1$, $m=0$, $k=1$, $c=1$, $\iter=1$.
Moreover, the number of tasks drawn ($N_{\task}$), and dataset draws ($N_{\dataset})$  for approximating the expected error given in Equation \ref{eq:ee} are taken to be $100$ each. Finally, the test set size is taken as 1000.


\paragraph{Effect of Number of Gradient Steps $\iter$:} It can be observed from Figure \ref{fig:linear-n_iter-N-1-D-1} that for a low number of training samples the gradient steps taken have little to no influence. But as the number of training samples increases for a given problem dimensionality the effect $\iter$ on the expected error gets much more prominent. It is evident that MAML decreases the number of gradient steps needed for convergence. Moreover, for $D=N$ case it even improves generalization after convergence too (see Figures \ref{fig:linear-n_iter-N-10-D-10} and \ref{fig:linear-n_iter-N-1-D-1}). Overall, it can be observed that the increasing $\iter$ converges towards the Ridge model variants with the exception of the $D=1$ and $N=1$ cases.
 
\begin{figure}[h!]
  \centering
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/n_iter/n_iter-1-1-x-0.tikz}
      \caption{$D=1$, $N=1$}
      \label{fig:linear-n_iter-N-1-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/n_iter/n_iter-1-10-x-0.tikz}
      \caption{$D=1$, $N=10$}
      \label{fig:linear-n_iter-N-10-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/n_iter/n_iter-1-50-x-0.tikz}
      \caption{$D=1$, $N=50$}
      \label{fig:linear-n_iter-N-50-D-1}
    \end{subfigure}

    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/n_iter/n_iter-10-10-x-0.tikz}
      \caption{$D=10$, $N=10$}
      \label{fig:linear-n_iter-N-10-D-10}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/n_iter/n_iter-50-10-x-0.tikz}
      \caption{$D=50$, $N=10$}
      \label{fig:linear-n_iter-N-10-D-50}
    \end{subfigure}  

  \caption{The expected error for the increasing number of gradient steps $\iter$ used for adaptation when changing the number of training samples for various problems of different dimensions.}
  \label{fig:linear-n_iter}
\end{figure}


\paragraph{Effect of the Number of Training Samples $N$:} Results of this experiment can be found in Figure \ref{fig:linear-N} for increasing problem dimensionality. It can be seen that the Linear model suffers from singularities. For instance, in Figure \ref{fig:linear-N-D-10} when the number of samples equals dimensionality $N=D$. However, it is able to have comparable error over all the selected problem dimensionalities. For the increasing training samples case, the Ridge model performs much better as all the cases converge towards the Bayes error. However, MAML is unable to converge towards the Bayes error. This can be attributed to the regularizing effect of the limited gradient steps ($\iter$) allowed for the models. Overall, the improvement that the additional task-related information brings to the gradient-based models is not visible, as the random GD model is orders of magnitude higher in expected error. It is evident that although task information provides gain over random initialization, the expected performance is hindered for the gradient-based models.

\begin{figure}[!h]
  \centering
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/N/Ntrn-1-1-x-0.tikz}
      \caption{$D=1$}
      \label{fig:linear-N-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/N/Ntrn-10-1-x-0.tikz}
      \caption{$D=10$}
      \label{fig:linear-N-D-10}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/N/Ntrn-50-1-x-0.tikz}
      \caption{$D=50$}
      \label{fig:linear-N-D-50}
    \end{subfigure}
  \caption{The expected error for the increasing number of training samples and problem dimensionality.}\label{fig:linear-N}
\end{figure}


\paragraph{Effect of Task Variance $c$:} The results of increasing task variance for various problem dimensions and various numbers of training samples can be found in Figure \ref{fig:linear-c}. The most obvious observation is that for all the models that utilize gradient descent, expected error increases, whereas the Linear model and the Ridge model are only affected by this phenomenon for problem dimensionality $D\geq N$.

In light of this observation, another important result is that for $N\geq D$ and for small task variance the gradient descent variants (except the randomly initialized model) have lower expected error than the Ridge model. However, this performance diminishes with increasing problem dimensionality and the increasing number of training points. It is interesting to see a better performance from just one gradient step. 

That is why an additional mini-experimentation is done for the GD and MAML models to investigate if there is an improvement with the additional gradient steps. The results of this experimentation are given in Table \ref{tab:zoom}. It is observed from the table that there exists a point at which the gradient steps are hurting the expected performance one would get in this range after the second gradient step. Then, it can be conjectured that the number of gradient steps has a regularizing effect on the task distributions with small variance. Despite, the surprising results of MAML-like algorithms, the Ridge model is much more stable and performs better than gradient-based methods for $N\geq D$.

\begin{table}
  \centering
  \caption{Mean expected error for the range $c:[0,1]$ range with various gradient steps for the MAML and GD models with $\lr=0.3234$. Note that only $D=1$, $N=10$ case (see Figure \ref{fig:linear-c-N-10-D-1}) is presented.}\label{tab:zoom}
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|}
    \cline{2-11}
     & \multicolumn{10}{|c|}{$\iter$}\\
    \cline{2-11}
     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
    \hline
    \multicolumn{1}{|c|}{MAML} & 1.2132 & \textbf{1.1938} & 1.2067 & 1.2171 & 1.2318 & 1.2476 & 1.2773 & 1.3330 & 1.4622 & 1.7556  \\
    \hline
    \end{tabular}
\end{table}

\begin{figure}[!h]
  \centering
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/c/c-1-1-x-0.tikz}
      \caption{$D=1$, $N=1$}
      \label{fig:linear-c-N-1-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/c/c-1-10-x-0.tikz}
      \caption{$D=1$, $N=10$}
      \label{fig:linear-c-N-10-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/c/c-1-50-x-0.tikz}
      \caption{$D=1$, $N=50$}
      \label{fig:linear-c-N-50-D-1}
    \end{subfigure}

    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/c/c-10-10-x-0.tikz}
      \caption{$D=10$, $N=10$}
      \label{fig:linear-c-N-10-D-10}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/c/c-50-10-x-0.tikz}
      \caption{$D=50$, $N=10$}
      \label{fig:linear-c-N-10-D-50}
    \end{subfigure}  

  \caption{The expected error for increasing task variance $c$ when changing the number of training samples for various problems of different dimensions.}
  \label{fig:linear-c}
\end{figure}


\paragraph{Effect of Task Mean $m$:} The results can be seen in Figure \ref{fig:linear-m}. The most important observation from this experimentation is that the Ridge model has an increasing expected error for the cases of $N\leq D$ cases (see Figures \ref{fig:linear-m-N-1-D-1}, \ref{fig:linear-m-N-10-D-10} and \ref{fig:linear-m-N-10-D-50}) and mostly the best $\lambda$ from the trials is found to be the lowest value, which makes the Ridge model behave similar to the Linear model. Furthermore, other models which have prior task information do not seem to be affected by the task mean shifting in the task space, as expected. Again, the superiority of including information from the task space is evident as the conventional regularization cannot deal with the changing task distribution mean for $N\leq D$.

\begin{figure}[!h]
  \centering
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/m/m-1-1-x-0.tikz}
      \caption{$D=1$, $N=1$}
      \label{fig:linear-m-N-1-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/m/m-1-10-x-0.tikz}
      \caption{$D=1$, $N=10$}
      \label{fig:linear-m-N-10-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/m/m-1-50-x-0.tikz}
      \caption{$D=1$, $N=50$}
      \label{fig:linear-m-N-50-D-1}
    \end{subfigure}

    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/m/m-10-10-x-0.tikz}
      \caption{$D=10$, $N=10$}
      \label{fig:linear-m-N-10-D-10}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/linres/m/m-50-10-x-0.tikz}
      \caption{$D=50$, $N=10$}
      \label{fig:linear-m-N-10-D-50}
    \end{subfigure}  

  \caption{The expected error for increasing task mean $m$ when changing the number of training samples for various problems of different dimensions.}
  \label{fig:linear-m}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Nonlinear Problem}

The nonlinear problem introduced in Section \ref{sec:methods} has parameters controlling the dimensionality $D$, number of training samples $N$, number of gradient steps $\iter$, the task variances and means $m_1$ and $m_2$, $c_1$ and $c_2$, and the variance of the input samples $k$. Note that only some of the parameters are discussed in this section. Unless the parameter in the configuration is under investigation, the default values are utilized. And, the default values are given as $\sigma=1$, $m_1=1$, $m_2=0$, $c_1=2$, $c_2=2$, $k=1$, $\iter=10$. Moreover, the number of tasks drawn ($N_{\task}$), and dataset draws ($N_{\dataset})$  for approximating the expected error given in Equation \ref{eq:ee} are taken to be $50$ each. Finally, the test set size is taken as 1000.

\paragraph{Effect of Number of Gradient Steps $\iter$:} It can be seen from Figure \ref{fig:nonlinear-n_iter-N-1-D-1} that a single realization of the Kernel Ridge model can have a lower expected error for an extreme value of $1$ training sample for the 1-dimensional problem. However, as the number of training samples increases for a given problem dimensionality MAML model starts showing a lower expected error (see Figure \ref{fig:nonlinear-n_iter-N-10-D-1}). Moreover, the further increase in training samples to $50$ lowers the difference in expected error for given models. Another interesting observation is, that for $N\leq D$ Kernel Ridge model can achieve a lower expected error, and for all the other cases one might find a better MAML model given that sufficient gradient steps are allowed. Moreover, it can be observed that the difference between random GD and MAML is low in terms of expected error for most of the presented problems. Finally, in cases where MAML outperforms Kernel Ridge the number of gradient steps required to get a lower expected error is low.

\begin{figure}[!htb]
  \centering
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/n_iter/n_iter-1-1-x-0.tikz}
      \caption{$D=1$, $N=1$}
      \label{fig:nonlinear-n_iter-N-1-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/n_iter/n_iter-1-10-x-0.tikz}
      \caption{$D=1$, $N=10$}
      \label{fig:nonlinear-n_iter-N-10-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/n_iter/n_iter-1-50-x-0.tikz}
      \caption{$D=1$, $N=50$}
      \label{fig:nonlinear-n_iter-N-50-D-1}
    \end{subfigure}

    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/n_iter/n_iter-10-10-x-0.tikz}
      \caption{$D=10$, $N=10$}
      \label{fig:nonlinear-n_iter-N-10-D-10}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/n_iter/n_iter-50-10-x-0.tikz}
      \caption{$D=50$, $N=10$}
      \label{fig:nonlinear-n_iter-N-10-D-50}
    \end{subfigure}  
  \caption{The expected error for the increasing number of gradient steps $\iter$ used for adaptation when changing the number of training samples for various problems of different dimensions.}
  \label{fig:nonlinear-n_iter}
\end{figure}


\paragraph{Effect of Number of Training Samples $N$:} By looking at Figure \ref{fig:nonlinear-N} it can be seen that for all the given dimensionalities there exists a training sample amount where the expected error of the Kernel Ridge model is higher than MAML. Another observation is that the randomly initialized model average performance is stable over all number of training samples. 

\begin{figure}[!h]
  \centering
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/N/Ntrn-1-1-x-0.tikz}
      \caption{$D=1$}
      \label{fig:nonlinear-N-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/N/Ntrn-10-1-x-0.tikz}
      \caption{$D=10$}
      \label{fig:nonlinear-N-D-10}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/N/Ntrn-50-1-x-0.tikz}
      \caption{$D=50$}
      \label{fig:nonlinear-N-D-50}
    \end{subfigure}
  \caption{The expected error for the increasing number of training samples and problem dimensionality.}\label{fig:nonlinear-N}
\end{figure}


\paragraph{Effect of Phase Task Variance $c_2$:} Remembering that the task variance effect for the linear problem had some interesting properties where even a single gradient step resulted in a lower expected error. One might wonder if that is the case for the nonlinear problem as well. As can be seen in Figure \ref{fig:nonlinear-c2-N-10-D-1} a similar effect is observed for the nonlinear problem. However, for this case, the decreased expected error of MAML is only better than a randomly initialized model. This indicates that a better performance can be achieved with a regularization-based conventional learner. The only clear advantage of utilizing MAML is seen in the case where there is only $1$ training sample. However, even the randomly initialized model performs better than the Kernel Ridge model for that case and what MAML adds is just a slight improvement. 

\begin{figure}[!h]
  \centering
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/c_phase/c_phase-1-1-x-0.tikz}
      \caption{$D=1$, $N=1$}
      \label{fig:nonlinear-c2-N-1-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/c_phase/c_phase-1-10-x-0.tikz}
      \caption{$D=1$, $N=10$}
      \label{fig:nonlinear-c2-N-10-D-1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/c_phase/c_phase-1-50-x-0.tikz}
      \caption{$D=1$, $N=50$}
      \label{fig:nonlinear-c2-N-50-D-1}
    \end{subfigure}

    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/c_phase/c_phase-10-10-x-0.tikz}
      \caption{$D=10$, $N=10$}
      \label{fig:nonlinear-c2-N-10-D-10}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
      \centering
      \includetikz{\textwidth}{Figures_v2/nonlinres/c_phase/c_phase-50-10-x-0.tikz}
      \caption{$D=50$, $N=10$}
      \label{fig:nonlinear-c2-N-10-D-50}
    \end{subfigure}  


  \caption{The expected error for increasing task variance for phase $c_2$ used for adaptation when changing the number of training samples for various problems of different dimensions.}
  \label{fig:nonlinear-c2}
\end{figure}

Additional experimentation results for $\sigma$ for linear and nonlinear problems can be found in the Appendix, it is observed that increasing noise has similar behavior with single task learning models. Moreover, the effect of input variance is investigated and found that the gradient descent based methods perform poorer for the linear problem. 


\subsection{Discussion}

Upon our investigation, it is found empirically that meta-information about the task space can help the generalization performance in linear and nonlinear problem settings even with limited gradient steps. Increased generalization performance of MAML compared to single task learning models on expectation when the tasks that are in consideration are close to each other is observed, where the same observation is made theoretically in \cite{fallah2021}. This observation suggests that there is a regularizing effect of limiting the gradient steps used for adaptation. We conjecture that after the meta-learning stage intermediate model parameters $\param$ are closer to the test set optimum compared to the proximity of train and test set optimums. This type of behavior is investigated in \cite{nakkiran2020} as well, where the large learning rate in the training phase acts as a regularizer due to the discrepancy between train and test loss landscapes. 

This limitation of adaptation steps is noted in \cite{behl2019,li2017b} that tries to improve the MAML adaptation step so that the adaptation is limited to fewer gradient steps, preferably one. Our findings suggest that the expected performance of these methods should be investigated as well since some of the generalization power of MAML might be coming from the regularization induced by not optimizing the training loss perfectly. This hypothesis is supported by the findings of \cite{raghu2020} which concludes that the performance gain of MAML is about feature reuse instead of rapid learning.

