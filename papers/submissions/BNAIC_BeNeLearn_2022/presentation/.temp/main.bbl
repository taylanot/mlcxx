% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
\preamble{%
\providecommand{\noopsort}[1]{}
}

  \entry{finn2017}{article}{}
    \name{author}{3}{}{%
      {{hash=FC}{%
         family={Finn},
         familyi={F\bibinitperiod},
         given={Chelsea},
         giveni={C\bibinitperiod},
      }}%
      {{hash=AP}{%
         family={Abbeel},
         familyi={A\bibinitperiod},
         given={Pieter},
         giveni={P\bibinitperiod},
      }}%
      {{hash=LS}{%
         family={Levine},
         familyi={L\bibinitperiod},
         given={Sergey},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Artificial Intelligence,Computer Science -
  Computer Vision and Pattern Recognition,Computer Science - Machine
  Learning,Computer Science - Neural and Evolutionary Computing}
    \strng{namehash}{FCAPLS1}
    \strng{fullhash}{FCAPLS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2017}
    \field{labeldatesource}{}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    We propose an algorithm for meta-learning that is model-agnostic, in the
  sense that it is compatible with any model trained with gradient descent and
  applicable to a variety of different learning problems, including
  classification, regression, and reinforcement learning. The goal of
  meta-learning is to train a model on a variety of learning tasks, such that
  it can solve new learning tasks using only a small number of training
  samples. In our approach, the parameters of the model are explicitly trained
  such that a small number of gradient steps with a small amount of training
  data from a new task will produce good generalization performance on that
  task. In effect, our method trains the model to be easy to fine-tune. We
  demonstrate that this approach leads to state-of-the-art performance on two
  fewshot image classification benchmarks, produces good results on few-shot
  regression, and accelerates fine-tuning for policy gradient reinforcement
  learning with neural network policies.%
    }
    \verb{eprint}
    \verb 1703.03400
    \endverb
    \field{title}{Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of
  {{Deep Networks}}}
    \field{langid}{english}
    \verb{file}
    \verb /home/taylanot/Zotero/storage/4NHI76JH/Finn et al. - 2017 - Model-Agn
    \verb ostic Meta-Learning for Fast Adaptation o.pdf
    \endverb
    \field{journaltitle}{arXiv:1703.03400 [cs]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs}
    \field{month}{07}
    \field{year}{2017}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry
\enddatalist
\endinput
