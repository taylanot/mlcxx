% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{Finn2017}{article}{}
    \name{author}{3}{}{%
      {{hash=FC}{%
         family={Finn},
         familyi={F\bibinitperiod},
         given={Chelsea},
         giveni={C\bibinitperiod},
      }}%
      {{hash=AP}{%
         family={Abbeel},
         familyi={A\bibinitperiod},
         given={Pieter},
         giveni={P\bibinitperiod},
      }}%
      {{hash=LS}{%
         family={Levine},
         familyi={L\bibinitperiod},
         given={Sergey},
         giveni={S\bibinitperiod},
      }}%
    }
    \strng{namehash}{FCAPLS1}
    \strng{fullhash}{FCAPLS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2017}
    \field{labeldatesource}{}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    We propose an algorithm for mcta-lcaming that is model-agnostic, in the
  sense that it is compatible with any model trained with gradient descent and
  applicable to a variety of different learning problems, including
  classification, regression, and reinforcement learning. The goal of
  meta-learning is to train a model on a variety of learning tasks, such that
  it can solve new learning tasks using only a small number of training
  samples. In our approach, the parameters of the model are explicitly trained
  such that a small number of gradient steps with a small amount of training
  data from a new task will produce good generalization performance on that
  task. In effect, our method trains the model to be easy to fine-tune. We
  demonstrate that this approach leads to state-of-the-art performance on two
  few-shot image classification benchmarks, produces good results on few-shot
  regression, and accelerates fine-tuning for policy gradient reinforcement
  learning with neural network policies.%
    }
    \verb{eprint}
    \verb 1703.03400
    \endverb
    \field{isbn}{9781510855144}
    \field{pages}{1856\bibrangedash 1868}
    \field{title}{{Model-agnostic meta-learning for fast adaptation of deep
  networks}}
    \field{volume}{3}
    \field{journaltitle}{34th International Conference on Machine Learning,
  ICML 2017}
    \field{eprinttype}{arXiv}
    \field{year}{2017}
  \endentry
\enddatalist
\endinput
